{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MGe7v4Bz_m24",
        "outputId": "4a2a37be-066f-4253-fec8-cdf6d45dc551"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: brevitas in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: dependencies==2.0.1 in /usr/local/lib/python3.11/dist-packages (from brevitas) (2.0.1)\n",
            "Requirement already satisfied: numpy<=1.26.4 in /usr/local/lib/python3.11/dist-packages (from brevitas) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from brevitas) (24.2)\n",
            "Requirement already satisfied: setuptools<70.0 in /usr/local/lib/python3.11/dist-packages (from brevitas) (69.5.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from brevitas) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from brevitas) (2.6.0+cu124)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from brevitas) (4.12.2)\n",
            "Requirement already satisfied: unfoldNd in /usr/local/lib/python3.11/dist-packages (from brevitas) (0.2.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->brevitas) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9.1->brevitas) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# TODO:\n",
        "# Try something along the lines of: pip install numpy==1.23.0\n",
        "\n",
        "# Install brevitas first to get the earlier version of numpy\n",
        "!pip install brevitas\n",
        "import brevitas.nn as qn\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import numpy as np\n",
        "\n",
        "# For lower dimensionality mapping\n",
        "# !pip install umap-learn\n",
        "# import umap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkh_uMiL_ous"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "IMAGE_DEPTH = 64\n",
        "PROJECT_PATH = '/content/drive/MyDrive/FinalYearProject/PYNQ'\n",
        "\n",
        "learning_rate = 0.0005\n",
        "batch_size = 1\n",
        "train_batch_size = 64\n",
        "epochs = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DE2jjhD0_wSd"
      },
      "source": [
        "# **Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22HGoOvHEG6m"
      },
      "outputs": [],
      "source": [
        "# Class Enumeration\n",
        "NORMAL_LABEL = 0\n",
        "DoS_LABEL = 1\n",
        "Fuzzy_LABEL = 2\n",
        "Gear_LABEL = 3\n",
        "RPM_LABEL = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2McQes1t_quL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from math import floor\n",
        "\n",
        "\n",
        "class CAN_12Bit_Dataset(Dataset):\n",
        "    def __init__(self, path : str, label_path : str, cls : int):\n",
        "        self.all_data = None\n",
        "        self.label_data = None\n",
        "        self.cls = cls\n",
        "        self.load(path, label_path)\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.all_data.shape[0] - IMAGE_DEPTH\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        if idx > (self.all_data.shape[0] - IMAGE_DEPTH):\n",
        "            raise IndexError(\"list index out of range\")\n",
        "\n",
        "        image = self.all_data[idx : idx + IMAGE_DEPTH].transpose(0, 1) # Remove this when ready!\n",
        "        labels = self.label_data[idx : idx + IMAGE_DEPTH]\n",
        "\n",
        "        return (image, labels, self.cls)\n",
        "\n",
        "\n",
        "    def load(self, path, label_path):\n",
        "        self.all_data = torch.load(path, map_location=torch.device(device), weights_only=True).to(device)\n",
        "        self.label_data = torch.load(label_path, map_location=torch.device(device), weights_only=True).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cRt5_B1vEfwd"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Get all Datasets/Dataloaders\n",
        "DATA_PATH = PROJECT_PATH + '/Datasets/11_Bit'\n",
        "LABEL_PATH = PROJECT_PATH + '/Datasets/Labels'\n",
        "\n",
        "Train_Normal_DS = CAN_12Bit_Dataset(DATA_PATH + '/Train_Normal_data.torch', LABEL_PATH + '/Train_Normal_labels.torch', NORMAL_LABEL)\n",
        "Train_DoS_DS = CAN_12Bit_Dataset(DATA_PATH + '/DoS_data.torch', LABEL_PATH + '/Train_DoS_labels.torch', DoS_LABEL)\n",
        "Train_Fuzzy_DS = CAN_12Bit_Dataset(DATA_PATH + '/Fuzzy_data.torch', LABEL_PATH + '/Train_Fuzzy_labels.torch', Fuzzy_LABEL)\n",
        "Train_Gear_DS = CAN_12Bit_Dataset(DATA_PATH + '/Gear_data.torch', LABEL_PATH + '/Train_Gear_labels.torch', Gear_LABEL)\n",
        "Train_RPM_DS = CAN_12Bit_Dataset(DATA_PATH + '/RPM_data.torch', LABEL_PATH + '/Train_RPM_labels.torch', RPM_LABEL)\n",
        "\n",
        "Test_Normal_DS = CAN_12Bit_Dataset(DATA_PATH + '/Test_Normal_data.torch', LABEL_PATH + '/Test_Normal_labels.torch', NORMAL_LABEL)\n",
        "\n",
        "\n",
        "Train_Normal_DL = DataLoader(Train_Normal_DS, batch_size=train_batch_size, shuffle=True)\n",
        "Train_DoS_DL = DataLoader(Train_DoS_DS, batch_size=batch_size, shuffle=True)\n",
        "Train_Fuzzy_DL = DataLoader(Train_Fuzzy_DS, batch_size=batch_size, shuffle=True)\n",
        "Train_Gear_DL = DataLoader(Train_Gear_DS, batch_size=batch_size, shuffle=True)\n",
        "Train_RPM_DL = DataLoader(Train_RPM_DS, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "Test_Normal_DL = DataLoader(Test_Normal_DS, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ector0qOoer"
      },
      "source": [
        "# **Convert Dataset to Numpy Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCeg0hSLOt0X"
      },
      "outputs": [],
      "source": [
        "def ConvertToNumpyDS_NHWC(torch_dataset, num_batches):\n",
        "    data = np.zeros((num_batches, 64, 1, 11))\n",
        "    labels = np.zeros((num_batches, 64))\n",
        "\n",
        "\n",
        "\n",
        "    it = iter(torch_dataset)\n",
        "    for i in range(num_batches):\n",
        "        batch = next(it)\n",
        "        data[i] = batch[0].cpu().numpy().reshape(64, 1, 11)\n",
        "        labels[i] = batch[1].cpu().numpy()\n",
        "\n",
        "    return data, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxeRsCcfPg5l",
        "outputId": "a0906eff-94fb-42bd-d14d-252094db602d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1000, 64, 1, 11)\n",
            "(1000, 64)\n"
          ]
        }
      ],
      "source": [
        "data, labels = ConvertToNumpyDS_NHWC(Train_Normal_DS, 1000)\n",
        "print(data.shape)\n",
        "print(labels.shape)\n",
        "np.save(PROJECT_PATH + '/Datasets/11_Bit/Numpy/Train_Normal_data_NHWC.npy', data)\n",
        "np.save(PROJECT_PATH + '/Datasets/11_Bit/Numpy/Train_Normal_labels_NHWC.npy', labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHMzB9BGR--G",
        "outputId": "eaa8f98b-7a08-4d41-a05c-2a206e0fa297"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1000, 64, 1, 11)\n",
            "(1000, 64)\n"
          ]
        }
      ],
      "source": [
        "data, label = ConvertToNumpyDS_NHWC(Test_Normal_DS, 1000)\n",
        "print(data.shape)\n",
        "print(labels.shape)\n",
        "np.save(PROJECT_PATH + '/Datasets/11_Bit/Numpy/Test_Normal_data_NHWC.npy', data)\n",
        "np.save(PROJECT_PATH + '/Datasets/11_Bit/Numpy/Test_Normal_labels_NHWC.npy', labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHTFlvyBRWYr",
        "outputId": "68985b59-44b9-4678-fed2-073c8a6a2fcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1000, 64, 1, 11)\n",
            "(1000, 64)\n"
          ]
        }
      ],
      "source": [
        "data, labels = ConvertToNumpyDS_NHWC(Train_DoS_DS, 1000)\n",
        "print(data.shape)\n",
        "print(labels.shape)\n",
        "np.save(PROJECT_PATH + '/Datasets/11_Bit/Numpy/Train_DoS_data_NHWC.npy', data)\n",
        "np.save(PROJECT_PATH + '/Datasets/11_Bit/Numpy/Train_DoS_labels_NHWC.npy', labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOy9lhxjRdu-",
        "outputId": "b400cfa5-40ed-483e-8673-e79d12244d86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1000, 64, 1, 11)\n",
            "(1000, 64)\n"
          ]
        }
      ],
      "source": [
        "data, labels = ConvertToNumpyDS_NHWC(Train_Fuzzy_DS, 1000)\n",
        "print(data.shape)\n",
        "print(labels.shape)\n",
        "np.save(PROJECT_PATH + '/Datasets/11_Bit/Numpy/Train_Fuzzy_data_NHWC.npy', data)\n",
        "np.save(PROJECT_PATH + '/Datasets/11_Bit/Numpy/Train_Fuzzy_labels_NHWC.npy', labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEZhRlPvRnAj",
        "outputId": "0cc9f406-85b0-4a77-e12e-6dab982232f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1000, 64, 1, 11)\n",
            "(1000, 64)\n"
          ]
        }
      ],
      "source": [
        "data, labels = ConvertToNumpyDS_NHWC(Train_Gear_DS, 1000)\n",
        "print(data.shape)\n",
        "print(labels.shape)\n",
        "np.save(PROJECT_PATH + '/Datasets/11_Bit/Numpy/Train_Gear_data_NHWC.npy', data)\n",
        "np.save(PROJECT_PATH + '/Datasets/11_Bit/Numpy/Train_Gear_labels_NHWC.npy', labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKXkQwk6RwaV",
        "outputId": "eb48c0f8-5034-4fa8-dda2-af8325b5cdac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1000, 64, 1, 11)\n",
            "(1000, 64)\n"
          ]
        }
      ],
      "source": [
        "data, labels = ConvertToNumpyDS_NHWC(Train_RPM_DS, 1000)\n",
        "print(data.shape)\n",
        "print(labels.shape)\n",
        "np.save(PROJECT_PATH + '/Datasets/11_Bit/Numpy/Train_RPM_data_NHWC.npy', data)\n",
        "np.save(PROJECT_PATH + '/Datasets/11_Bit/Numpy/Train_RPM_labels_NHWC.npy', labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnx3IVEXHHra"
      },
      "source": [
        "# **Model Definitions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUkw5T6oJtOj"
      },
      "outputs": [],
      "source": [
        "# Choose Quantisation Level\n",
        "from brevitas.quant import Int8Bias\n",
        "bit_width = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vk1Hxo_wJ8Ec"
      },
      "outputs": [],
      "source": [
        "# Quantiser\n",
        "\n",
        "# Base Binariser\n",
        "from brevitas.inject import ExtendedInjector\n",
        "from brevitas.core.quant import BinaryQuant\n",
        "from brevitas.core.scaling import ParameterScaling\n",
        "# from brevitas.inject.enum import ScalingImplType\n",
        "from brevitas.core.scaling import ScalingImplType\n",
        "\n",
        "\n",
        "class MySignedBinaryQuantizer(ExtendedInjector):\n",
        "    tensor_quant = BinaryQuant\n",
        "    scaling_impl=ParameterScaling\n",
        "    scaling_init=1.0\n",
        "    # scaling_impl = ScalingImplType.CONST\n",
        "    # scale=1.0\n",
        "    signed = True\n",
        "\n",
        "\n",
        "\"\"\" Weight, Bias and Activation Binary Quantisers \"\"\"\n",
        "from brevitas.proxy import WeightQuantProxyFromInjector\n",
        "\n",
        "class MySignedBinaryWeightQuantizer(MySignedBinaryQuantizer):\n",
        "    proxy_class = WeightQuantProxyFromInjector\n",
        "\n",
        "\n",
        "from brevitas.proxy import BiasQuantProxyFromInjector\n",
        "\n",
        "class MySignedBinaryBiasQuantizer(MySignedBinaryQuantizer):\n",
        "    proxy_class = BiasQuantProxyFromInjector\n",
        "\n",
        "\n",
        "\n",
        "from brevitas.proxy import ActQuantProxyFromInjector\n",
        "\n",
        "class MySignedBinaryActQuantizer(MySignedBinaryQuantizer):\n",
        "    proxy_class = ActQuantProxyFromInjector\n",
        "\n",
        "# Need this to make quantised ReLUs:\n",
        "# binary_relu = QuantIdentity(act_quant=MySignedBinaryActQuantizer, return_quant_tensor=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmU_T8TjXLvo"
      },
      "outputs": [],
      "source": [
        "from brevitas.quant import Int8ActPerTensorFloat, Int8WeightPerTensorFloat, SignedBinaryWeightPerTensorConst, SignedBinaryActPerTensorConst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRH6Tj1YHHYg"
      },
      "outputs": [],
      "source": [
        "use_bias = False\n",
        "\n",
        "\n",
        "# AutoEncoder Encoder side definition\n",
        "class AE_EncoderNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        # self.quant_input = qn.QuantIdentity(bit_width=bit_width, return_quant_tensor=True)\n",
        "\n",
        "        self.input_layer = nn.Sequential(\n",
        "            # qn.QuantIdentity(act_quant=SignedBinaryActPerTensorConst, return_quant_tensor=True),\n",
        "            # qn.QuantLinear(12, 32, bias=use_bias, weight_quant=Int8WeightPerTensorFloat, bias_quant=Int8Bias),\n",
        "            qn.QuantIdentity(act_quant=SignedBinaryActPerTensorConst, return_quant_tensor=True),\n",
        "        )\n",
        "\n",
        "        self.l1 = nn.Sequential(\n",
        "            qn.QuantConv2d(1, 128, 3, stride=1, padding=1, bias=use_bias, weight_quant=SignedBinaryWeightPerTensorConst, bias_quant=Int8Bias),\n",
        "            nn.MaxPool2d(2),\n",
        "            qn.QuantIdentity(act_quant=SignedBinaryActPerTensorConst, return_quant_tensor=True),\n",
        "        )\n",
        "        self.l2 = nn.Sequential(\n",
        "            qn.QuantConv2d(128, 64, 3, stride=1, padding=1, bias=use_bias, weight_quant=SignedBinaryWeightPerTensorConst, bias_quant=Int8Bias),\n",
        "            nn.MaxPool2d(2),\n",
        "            qn.QuantIdentity(act_quant=SignedBinaryActPerTensorConst, return_quant_tensor=True),\n",
        "        )\n",
        "        self.l3 = nn.Sequential(\n",
        "            qn.QuantConv2d(16, 32, 2, stride=2, bias=use_bias, weight_quant=SignedBinaryWeightPerTensorConst, bias_quant=Int8Bias),\n",
        "            qn.QuantIdentity(act_quant=SignedBinaryActPerTensorConst, return_quant_tensor=True),\n",
        "        )\n",
        "\n",
        "\n",
        "        # self.init_weights()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.input_layer(x)\n",
        "        out = self.l1(out)\n",
        "        out = self.l2(out)\n",
        "        # out = self.l3(out)\n",
        "        return out\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\" Do He Initialisation to prevent relu dead weights \"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "                torch.nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "\n",
        "\n",
        "# AutoEncoder Decoder side definition\n",
        "class AE_DecoderNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "\n",
        "        self.l1 = nn.Sequential(\n",
        "            qn.QuantConvTranspose2d(64, 128, 3, stride=2, padding=1, output_padding=1, bias=use_bias, weight_quant=SignedBinaryWeightPerTensorConst, bias_quant=Int8Bias),\n",
        "            qn.QuantIdentity(act_quant=SignedBinaryActPerTensorConst, return_quant_tensor=True),\n",
        "        )\n",
        "        self.l2 = nn.Sequential(\n",
        "            qn.QuantConvTranspose2d(128, 1, 3, stride=2, padding=1, output_padding=1, bias=use_bias, weight_quant=SignedBinaryWeightPerTensorConst, bias_quant=Int8Bias),\n",
        "            qn.QuantIdentity(act_quant=SignedBinaryActPerTensorConst, return_quant_tensor=True),\n",
        "        )\n",
        "        self.l3 = nn.Sequential(\n",
        "            qn.QuantConv2d(1, 1, 3, stride=1, padding=1, output_padding=1, bias=use_bias, weight_quant=SignedBinaryWeightPerTensorConst, bias_quant=Int8Bias),\n",
        "            # qn.QuantSigmoid(act_quant=SignedBinaryActPerTensorConst, return_quant_tensor=False), #False to return normal torch tensor\n",
        "            # qn.QuantIdentity(act_quant=SignedBinaryActPerTensorConst, return_quant_tensor=True),\n",
        "        )\n",
        "\n",
        "        # self.init_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.l1(x)\n",
        "        out = self.l2(out)\n",
        "        out = self.l3(out)\n",
        "        return out\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\" Do He Initialisation to prevent relu dead weights \"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "                torch.nn.init.kaiming_normal_(m.weight, nonlinearity='relu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_FJTMDgRiye"
      },
      "outputs": [],
      "source": [
        "DEBUG = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5K-yfkUvPLOJ"
      },
      "outputs": [],
      "source": [
        "# AutoEncoder Encoder side definition\n",
        "class AE_EncoderNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.input_layer = nn.Sequential(\n",
        "            qn.QuantIdentity(act_quant=SignedBinaryActPerTensorConst, return_quant_tensor=True),\n",
        "\n",
        "            # Uncomment this:\n",
        "            qn.QuantLinear(12, 32,\n",
        "                           weight_quant=SignedBinaryWeightPerTensorConst,\n",
        "                           weight_bit_width=1,\n",
        "                           output_quant=SignedBinaryActPerTensorConst,\n",
        "                           return_quant_tensor=True,\n",
        "                           bias=False),\n",
        "        )\n",
        "\n",
        "        self.l1 = nn.Sequential(\n",
        "            qn.QuantConv2d(1, 64, 4,\n",
        "                           stride=2, padding=1,\n",
        "                           weight_quant=SignedBinaryWeightPerTensorConst,\n",
        "                           output_quant=SignedBinaryActPerTensorConst,\n",
        "                           return_quant_tensor=True,\n",
        "                           bias=False),\n",
        "            # nn.MaxPool2d(2),\n",
        "            # qn.QuantIdentity(act_quant=SignedBinaryActPerTensorConst, return_quant_tensor=True),\n",
        "        )\n",
        "        self.l2 = nn.Sequential(\n",
        "            qn.QuantConv2d(64, 128, 4,\n",
        "                           stride=2, padding=1,\n",
        "                           weight_quant=SignedBinaryWeightPerTensorConst,\n",
        "                           output_quant=SignedBinaryActPerTensorConst,\n",
        "                           return_quant_tensor=True,\n",
        "                           bias=False),\n",
        "            # nn.MaxPool2d(2),\n",
        "            # qn.QuantIdentity(act_quant=SignedBinaryActPerTensorConst, return_quant_tensor=True),\n",
        "        )\n",
        "        self.l3 = nn.Sequential(\n",
        "            qn.QuantConv2d(128, 256, 4,\n",
        "                           stride=2, padding=1,\n",
        "                           weight_quant=SignedBinaryWeightPerTensorConst,\n",
        "                           output_quant=SignedBinaryActPerTensorConst,\n",
        "                           return_quant_tensor=True,\n",
        "                           bias=False),\n",
        "            # nn.MaxPool2d(2),\n",
        "            # qn.QuantIdentity(act_quant=SignedBinaryActPerTensorConst, return_quant_tensor=True),\n",
        "        )\n",
        "        self.l4 = nn.Sequential(\n",
        "            qn.QuantConv2d(256, 512, 4,\n",
        "                           stride=2, padding=1,\n",
        "                           weight_quant=SignedBinaryWeightPerTensorConst,\n",
        "                           output_quant=SignedBinaryActPerTensorConst,\n",
        "                           return_quant_tensor=True,\n",
        "                           bias=False),\n",
        "            # qn.QuantIdentity(act_quant=SignedBinaryActPerTensorConst, return_quant_tensor=True),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        if DEBUG:\n",
        "            print(x.shape)\n",
        "        out = self.input_layer(x)\n",
        "        if DEBUG:\n",
        "            print(out.shape)\n",
        "        out = self.l1(out)\n",
        "        if DEBUG:\n",
        "            print(out.shape)\n",
        "        out = self.l2(out)\n",
        "        if DEBUG:\n",
        "            print(out.shape)\n",
        "        out = self.l3(out)\n",
        "        if DEBUG:\n",
        "            print(out.shape)\n",
        "        out = self.l4(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "# AutoEncoder Decoder side definition\n",
        "class AE_DecoderNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "\n",
        "        self.l1 = nn.Sequential(\n",
        "            qn.QuantConvTranspose2d(512, 256, 3,\n",
        "                                    stride=1, padding=1, output_padding=0,\n",
        "                                    weight_quant=SignedBinaryWeightPerTensorConst,\n",
        "                                    output_quant=SignedBinaryActPerTensorConst,\n",
        "                                    return_quant_tensor=True,\n",
        "                                    bias=False),\n",
        "            qn.QuantUpsample(scale_factor=2, mode='nearest', return_quant_tensor=False),\n",
        "            # qn.QuantIdentity(act_quant=SignedBinaryActPerTensorConst, return_quant_tensor=True),\n",
        "        )\n",
        "\n",
        "        self.l2 = nn.Sequential(\n",
        "            qn.QuantConvTranspose2d(256, 128, 3,\n",
        "                                    stride=1, padding=1, output_padding=0,\n",
        "                                    weight_quant=SignedBinaryWeightPerTensorConst,\n",
        "                                    output_quant=SignedBinaryActPerTensorConst,\n",
        "                                    return_quant_tensor=True,\n",
        "                                    bias=False),\n",
        "            qn.QuantUpsample(scale_factor=2, mode='nearest', return_quant_tensor=False),\n",
        "            # qn.QuantIdentity(act_quant=SignedBinaryActPerTensorConst, return_quant_tensor=True),\n",
        "        )\n",
        "        self.l3 = nn.Sequential(\n",
        "            qn.QuantConvTranspose2d(128, 64, 3,\n",
        "                                    stride=1, padding=1, output_padding=0,\n",
        "                                    weight_quant=SignedBinaryWeightPerTensorConst,\n",
        "                                    output_quant=SignedBinaryActPerTensorConst,\n",
        "                                    return_quant_tensor=True,\n",
        "                                    bias=False),\n",
        "            qn.QuantUpsample(scale_factor=2, mode='nearest', return_quant_tensor=False),\n",
        "            # qn.QuantIdentity(act_quant=SignedBinaryActPerTensorConst, return_quant_tensor=True),\n",
        "        )\n",
        "        self.l4 = nn.Sequential(\n",
        "            qn.QuantConvTranspose2d(64, 1, 3,\n",
        "                                    stride=1, padding=1, output_padding=0,\n",
        "                                    weight_quant=SignedBinaryWeightPerTensorConst,\n",
        "                                    output_quant=SignedBinaryActPerTensorConst,\n",
        "                                    return_quant_tensor=True,\n",
        "                                    bias=False),\n",
        "            qn.QuantUpsample(scale_factor=2, mode='nearest', return_quant_tensor=False),\n",
        "            # qn.QuantIdentity(act_quant=SignedBinaryActPerTensorConst, return_quant_tensor=True),\n",
        "        )\n",
        "        self.l5 = nn.Sequential(\n",
        "            # qn.QuantConv2d(1, 1, 3, stride=1, padding=1, output_padding=0, bias=use_bias, weight_quant=SignedBinaryWeightPerTensorConst, bias_quant=Int8Bias),\n",
        "            # qn.QuantIdentity(act_quant=SignedBinaryActPerTensorConst, return_quant_tensor=True),\n",
        "            qn.QuantLinear(32, 12,\n",
        "                           weight_quant=SignedBinaryWeightPerTensorConst,\n",
        "                           output_quant=SignedBinaryActPerTensorConst,\n",
        "                           return_quant_tensor=False,\n",
        "                           bias=False),\n",
        "            # qn.QuantSigmoid(act_quant=SignedBinaryActPerTensorConst, return_quant_tensor=False), #False to return normal torch tensor\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if DEBUG:\n",
        "            print(x.shape)\n",
        "        out = self.l1(x)\n",
        "        if DEBUG:\n",
        "            print(out.shape)\n",
        "        out = self.l2(out)\n",
        "        if DEBUG:\n",
        "            print(out.shape)\n",
        "        out = self.l3(out)\n",
        "        if DEBUG:\n",
        "            print(out.shape)\n",
        "        out = self.l4(out)\n",
        "        if DEBUG:\n",
        "            print(out.shape)\n",
        "        out = self.l5(out)\n",
        "        if DEBUG:\n",
        "            print(out.shape)\n",
        "            raise Exception(\"Stop\")\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RYZCQTE4JN8"
      },
      "outputs": [],
      "source": [
        "ShapeVec = [1,16,32] # 1, 16, 32 was pretty good\n",
        "\n",
        "# TODO: Try starting with a large amount of filters and decreasing...\n",
        "# Real-Time paper uses 128, 64 filters\n",
        "\n",
        "ShapeVec = [1,128,12] # ...\n",
        "\n",
        "# AutoEncoder Encoder side definition\n",
        "class AE_EncoderNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        layersVec = [qn.QuantConv2d(ShapeVec[i], ShapeVec[i+1], 2, stride=2,\n",
        "                                    weight_quant=SignedBinaryWeightPerTensorConst,\n",
        "                                    output_quant=SignedBinaryActPerTensorConst,\n",
        "                                    return_quant_tensor=True,\n",
        "                                    bias=False)\n",
        "                     for i in range(len(ShapeVec[:-1])) ]\n",
        "\n",
        "        self.layers = nn.Sequential(*layersVec)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layers(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "# AutoEncoder Decoder side definition\n",
        "class AE_DecoderNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        layersVec = [qn.QuantConvTranspose2d(ShapeVec[::-1][i], ShapeVec[::-1][i+1], 2, stride=2,\n",
        "                                    weight_quant=SignedBinaryWeightPerTensorConst,\n",
        "                                    output_quant=SignedBinaryActPerTensorConst,\n",
        "                                    return_quant_tensor=True,\n",
        "                                    bias=False)\n",
        "                     for i in range(len(ShapeVec[:-1])) ]\n",
        "\n",
        "        self.layers = nn.Sequential(*layersVec)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layers(x)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDgCSxcIJ8m9"
      },
      "outputs": [],
      "source": [
        "class AE_AutoEncoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.encoder = encoder.to(device)\n",
        "        self.decoder = decoder.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.encoder(x)\n",
        "        out = self.decoder(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyLBKFtNK0B3"
      },
      "outputs": [],
      "source": [
        "AutoEncoder = AE_AutoEncoder(AE_EncoderNN(), AE_DecoderNN()).to(device)\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(AutoEncoder.parameters(), lr=0.0001) # lr=0.000001 = OLD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajZYWALM_RaB"
      },
      "source": [
        "# **More Complex AutoEncoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dhrwsKd6RRw"
      },
      "outputs": [],
      "source": [
        "from brevitas.quant import Int8ActPerTensorFloat as ActQuant\n",
        "from brevitas.quant import Int8WeightPerTensorFloat as WeightQuant\n",
        "from brevitas.quant import Int8Bias\n",
        "\n",
        "\n",
        "use_bias = True\n",
        "\n",
        "class RealTime_AutoEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.down1 = nn.Sequential(\n",
        "            qn.QuantConv2d(1, 128, 3,\n",
        "                            stride=1, padding=1,\n",
        "                            input_quant=ActQuant,\n",
        "                            weight_quant=WeightQuant,\n",
        "                            bias_quant=Int8Bias,\n",
        "                            output_quant=ActQuant,\n",
        "                            return_quant_tensor=True,\n",
        "                            bias=use_bias),\n",
        "            qn.QuantReLU(act_quant=ActQuant, return_quant_tensor=True),\n",
        "        )\n",
        "        self.down2 = nn.Sequential(\n",
        "            qn.QuantConv2d(128, 64, 3,\n",
        "                           stride=1, padding=1,\n",
        "                           input_quant=ActQuant,\n",
        "                           weight_quant=WeightQuant,\n",
        "                           bias_quant=Int8Bias,\n",
        "                           output_quant=ActQuant,\n",
        "                           return_quant_tensor=True,\n",
        "                           bias=use_bias),\n",
        "            qn.QuantReLU(act_quant=ActQuant, return_quant_tensor=True),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.up1 = nn.Sequential(\n",
        "            qn.QuantConvTranspose2d(64, 128, 3,\n",
        "                                    stride=2, padding=1,\n",
        "                                    output_padding=1,\n",
        "                                    input_quant=ActQuant,\n",
        "                                    weight_quant=WeightQuant,\n",
        "                                    bias_quant=Int8Bias,\n",
        "                                    output_quant=ActQuant,\n",
        "                                    return_quant_tensor=True,\n",
        "                                    bias=use_bias),\n",
        "            qn.QuantReLU(act_quant=ActQuant, return_quant_tensor=True),\n",
        "        )\n",
        "        self.up2 = nn.Sequential(\n",
        "            qn.QuantConvTranspose2d(128, 1, 3,\n",
        "                                    stride=2, padding=1,\n",
        "                                    output_padding=1,\n",
        "                                    input_quant=ActQuant,\n",
        "                                    weight_quant=WeightQuant,\n",
        "                                    bias_quant=Int8Bias,\n",
        "                                    output_quant=ActQuant,\n",
        "                                    return_quant_tensor=True,\n",
        "                                    bias=use_bias),\n",
        "            qn.QuantReLU(act_quant=ActQuant, return_quant_tensor=True),\n",
        "        )\n",
        "        self.output_layer = nn.Sequential(\n",
        "            qn.QuantConv2d(1, 1, 3,\n",
        "                            stride=1, padding=1,\n",
        "                            input_quant=ActQuant,\n",
        "                            weight_quant=WeightQuant,\n",
        "                            bias_quant=Int8Bias,\n",
        "                            output_quant=ActQuant,\n",
        "                            return_quant_tensor=True,\n",
        "                            bias=use_bias),\n",
        "            qn.QuantSigmoid(act_quant=ActQuant, return_quant_tensor=True),\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.down1(x)\n",
        "        out = nn.MaxPool2d(2, stride=2)(out)\n",
        "        out = self.down2(out)\n",
        "        out = nn.MaxPool2d(2, stride=2)(out)\n",
        "\n",
        "        out = self.up1(out)\n",
        "        out = self.up2(out)\n",
        "        out = self.output_layer(out)\n",
        "\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bED8WcB28Ha4"
      },
      "outputs": [],
      "source": [
        "RT_AutoEncoder = RealTime_AutoEncoder().to(device)\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(RT_AutoEncoder.parameters(), lr=0.001)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "valkchciIhBl"
      },
      "outputs": [],
      "source": [
        "from brevitas.quant import Int8ActPerTensorFloat as ActQuant\n",
        "from brevitas.quant import Int8WeightPerTensorFloat as WeightQuant\n",
        "# from brevitas.quant import SignedBinaryActPerTensorConst as ActQuant\n",
        "# from brevitas.quant import SignedBinaryWeightPerTensorConst as WeightQuant\n",
        "from brevitas.quant import Int8Bias\n",
        "\n",
        "\n",
        "use_bias = False\n",
        "DEBUG = False\n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.encode = nn.Sequential(\n",
        "            # This is a TCN - Temporal Convolutional network!\n",
        "            # nn.ConstantPad1d((2,0), 0), # leftside padding means causal convolution\n",
        "            qn.QuantConv1d(11, 32, 3,\n",
        "                           stride=1, padding=1,\n",
        "                           input_quant=ActQuant,\n",
        "                           weight_quant=WeightQuant,\n",
        "                           bias_quant=Int8Bias,\n",
        "                           output_quant=ActQuant,\n",
        "                           return_quant_tensor=True,\n",
        "                           bias=use_bias),\n",
        "            # qn.QuantReLU(act_quant=ActQuant, return_quant_tensor=True),\n",
        "            nn.MaxPool1d(2, stride=2),\n",
        "            # nn.ConstantPad1d((2,0), 0),\n",
        "            qn.QuantConv1d(32, 64, 3,\n",
        "                           stride=1, padding=1,\n",
        "                           input_quant=ActQuant,\n",
        "                           weight_quant=WeightQuant,\n",
        "                           bias_quant=Int8Bias,\n",
        "                           output_quant=ActQuant,\n",
        "                           return_quant_tensor=True,\n",
        "                           bias=use_bias),\n",
        "            nn.MaxPool1d(2, stride=2),\n",
        "            # qn.QuantReLU(act_quant=ActQuant, return_quant_tensor=True),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.decode = nn.Sequential(\n",
        "            # nn.ConstantPad1d((2,0), 0),\n",
        "            # Linear layer used as a FINN compatible upsample\n",
        "            qn.QuantLinear(16, 64,\n",
        "                           input_quant=ActQuant,\n",
        "                           weight_quant=WeightQuant,\n",
        "                           bias_quant=Int8Bias,\n",
        "                           output_quant=ActQuant,\n",
        "                           return_quant_tensor=True,\n",
        "                           bias=use_bias),\n",
        "            qn.QuantConv1d(64, 11, 3,\n",
        "                           stride=1, padding=1,\n",
        "                           input_quant=ActQuant,\n",
        "                           weight_quant=WeightQuant,\n",
        "                           bias_quant=Int8Bias,\n",
        "                           output_quant=ActQuant,\n",
        "                           return_quant_tensor=True,\n",
        "                           bias=use_bias),\n",
        "            # qn.QuantReLU(act_quant=ActQuant, return_quant_tensor=True),\n",
        "        )\n",
        "        # self.up2_1d = nn.Sequential(\n",
        "        #     # nn.ConstantPad1d((2,0), 0),\n",
        "        #     qn.QuantLinear(32, 64,\n",
        "        #                    input_quant=ActQuant,\n",
        "        #                    weight_quant=WeightQuant,\n",
        "        #                    bias_quant=Int8Bias,\n",
        "        #                    output_quant=ActQuant,\n",
        "        #                    return_quant_tensor=True,\n",
        "        #                    bias=use_bias),\n",
        "        #     qn.QuantConv1d(32, 11, 3,\n",
        "        #                    stride=1, padding=1,\n",
        "        #                    input_quant=ActQuant,\n",
        "        #                    weight_quant=WeightQuant,\n",
        "        #                    bias_quant=Int8Bias,\n",
        "        #                    output_quant=ActQuant,\n",
        "        #                    return_quant_tensor=True,\n",
        "        #                    bias=use_bias),\n",
        "        # )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1d convolutions\n",
        "        # print(x.shape)\n",
        "        out = self.encode(x)\n",
        "        # print(out.shape)\n",
        "\n",
        "        # out = nn.Upsample(scale_factor=2, mode='nearest')(out)\n",
        "        out = self.decode(out)\n",
        "        # print(out.shape)\n",
        "        # out = nn.Upsample(scale_factor=2, mode='nearest')(out)\n",
        "        # out = self.up2_1d(out)\n",
        "        # print(out.shape)\n",
        "        # raise Exception(\"Stop\")\n",
        "\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2RDaPO4_hjg"
      },
      "outputs": [],
      "source": [
        "AutoEncoder = AutoEncoder().to(device)\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(AutoEncoder.parameters(), lr=0.001)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpTR9QTcx5g-"
      },
      "outputs": [],
      "source": [
        "# Load Model\n",
        "AutoEncoder.load_state_dict(torch.load('/content/drive/MyDrive/FinalYearProject/PYNQ/Models/TemporalConvNets/AsymLinearTCN_32_64.model', map_location=torch.device(device), weights_only=True))\n",
        "AutoEncoder = AutoEncoder.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc5w35duWoxt"
      },
      "source": [
        "# **Brevitas Example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCqZcuDVWtjG"
      },
      "outputs": [],
      "source": [
        "# Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.\n",
        "# SPDX-License-Identifier: BSD-3-Clause\n",
        "\n",
        "from dependencies import value\n",
        "\n",
        "from brevitas.core.bit_width import BitWidthImplType\n",
        "from brevitas.core.quant import QuantType\n",
        "from brevitas.core.restrict_val import FloatToIntImplType\n",
        "from brevitas.core.restrict_val import RestrictValueType\n",
        "from brevitas.core.scaling import ScalingImplType\n",
        "from brevitas.core.zero_point import ZeroZeroPoint\n",
        "from brevitas.inject import ExtendedInjector\n",
        "from brevitas.quant.solver import ActQuantSolver\n",
        "from brevitas.quant.solver import WeightQuantSolver\n",
        "\n",
        "\n",
        "class CommonQuant(ExtendedInjector):\n",
        "    bit_width_impl_type = BitWidthImplType.CONST\n",
        "    scaling_impl_type = ScalingImplType.CONST\n",
        "    restrict_scaling_type = RestrictValueType.FP\n",
        "    zero_point_impl = ZeroZeroPoint\n",
        "    float_to_int_impl_type = FloatToIntImplType.ROUND\n",
        "    scaling_per_output_channel = False\n",
        "    narrow_range = True\n",
        "    signed = True\n",
        "\n",
        "    @value\n",
        "    def quant_type(bit_width):\n",
        "        if bit_width is None:\n",
        "            return QuantType.FP\n",
        "        elif bit_width == 1:\n",
        "            return QuantType.BINARY\n",
        "        else:\n",
        "            return QuantType.INT\n",
        "\n",
        "\n",
        "class CommonWeightQuant(CommonQuant, WeightQuantSolver):\n",
        "    scaling_const = 1.0\n",
        "\n",
        "\n",
        "class CommonActQuant(CommonQuant, ActQuantSolver):\n",
        "    min_val = -1.0\n",
        "    max_val = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = (Train_Normal_DS[0][0])\n",
        "print(input)\n",
        "quantised_input = qn.QuantIdentity(act_quant=CommonActQuant, bit_width=BIT_WIDTH)(input[0])\n",
        "print(quantised_input) # TODO: Also might need to physically access the first QuantId from the model itself"
      ],
      "metadata": {
        "id": "O6e2S8jiNBDd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "3d1d2a60-157d-4f4d-a543-80e6c2b292e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
            "         0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
            "         0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
            "         0., 1., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
            "        [0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
            "         0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "         0., 1., 0., 0., 0., 0., 1., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
            "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
            "         0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
            "         0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
            "         1., 1., 1., 0., 0., 1., 0., 1., 1., 1.],\n",
            "        [0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
            "         0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,\n",
            "         0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
            "         1., 1., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
            "        [1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
            "         1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
            "         1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
            "         0., 0., 1., 1., 1., 1., 0., 1., 0., 0.],\n",
            "        [1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
            "         0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
            "         0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "         1., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1.,\n",
            "         0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
            "         0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
            "         0., 0., 1., 0., 0., 1., 0., 1., 1., 1.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
            "         0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
            "         0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
            "         1., 0., 1., 0., 1., 1., 0., 0., 0., 0.],\n",
            "        [0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
            "         1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
            "         1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
            "tensor([0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 1., 0., 1.], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3Pe8QtLW0P0"
      },
      "outputs": [],
      "source": [
        "class AE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.seq1 = nn.Sequential(\n",
        "\n",
        "            qn.QuantIdentity(act_quant=CommonActQuant, bit_width=8),\n",
        "            qn.QuantConv1d(11, 32, 3,\n",
        "                           stride=1, padding=1,\n",
        "                           weight_bit_width=8,\n",
        "                           weight_quant=CommonWeightQuant,\n",
        "                           bias=False),\n",
        "            nn.MaxPool1d(2, stride=2),\n",
        "            qn.QuantIdentity(act_quant=CommonActQuant, bit_width=8),\n",
        "        )\n",
        "        self.seq2 = nn.Sequential(\n",
        "\n",
        "            qn.QuantConv1d(32, 64, 3,\n",
        "                           stride=1, padding=1,\n",
        "                           weight_bit_width=8,\n",
        "                           weight_quant=CommonWeightQuant,\n",
        "                           bias=False),\n",
        "            nn.MaxPool1d(2, stride=2),\n",
        "            qn.QuantIdentity(act_quant=CommonActQuant, bit_width=8),\n",
        "        )\n",
        "        self.seq3 = nn.Sequential(\n",
        "\n",
        "            qn.QuantLinear(16, 64,\n",
        "                           weight_quant=CommonWeightQuant,\n",
        "                           weight_bit_width=8,\n",
        "                           bias=False),\n",
        "            qn.QuantIdentity(act_quant=CommonActQuant, bit_width=8),\n",
        "            qn.QuantConv1d(64, 11, 3,\n",
        "                           stride=1, padding=1,\n",
        "                           weight_quant=CommonWeightQuant,\n",
        "                           weight_bit_width=8,\n",
        "                           bias=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.seq1(x)\n",
        "        x = self.seq2(x)\n",
        "        x = self.seq3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCH9xnyE2iFC"
      },
      "outputs": [],
      "source": [
        "# Use non square conv2d\n",
        "\n",
        "BIT_WIDTH = 8\n",
        "\n",
        "class AE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encode = nn.Sequential(\n",
        "            qn.QuantIdentity(act_quant=CommonActQuant, bit_width=BIT_WIDTH),\n",
        "            qn.QuantConv2d(11, 32, (3, 1),\n",
        "                           stride=(1, 1), padding=(1, 0),\n",
        "                           weight_bit_width=BIT_WIDTH,\n",
        "                           weight_quant=CommonWeightQuant,\n",
        "                           bias=False),\n",
        "            nn.MaxPool2d((2,1), stride=(2,1)),\n",
        "\n",
        "            qn.QuantIdentity(act_quant=CommonActQuant, bit_width=BIT_WIDTH),\n",
        "            qn.QuantConv2d(32, 64, (3, 1),\n",
        "                           stride=(1, 1), padding=(1, 0),\n",
        "                           weight_bit_width=BIT_WIDTH,\n",
        "                           weight_quant=CommonWeightQuant,\n",
        "                           bias=False),\n",
        "            nn.MaxPool2d((2,1), stride=(2,1)),\n",
        "        )\n",
        "        self.decode = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=(2,1)),\n",
        "            qn.QuantIdentity(act_quant=CommonActQuant, bit_width=BIT_WIDTH),\n",
        "            qn.QuantConv2d(64, 32, (3, 1),\n",
        "                           stride=1, padding=(1, 0),\n",
        "                           weight_quant=CommonWeightQuant,\n",
        "                           weight_bit_width=BIT_WIDTH,\n",
        "                           bias=False),\n",
        "            nn.Upsample(scale_factor=(2,1)),\n",
        "            qn.QuantIdentity(act_quant=CommonActQuant, bit_width=BIT_WIDTH),\n",
        "            qn.QuantConv2d(32, 11, (3, 1),\n",
        "                           stride=1, padding=(1, 0),\n",
        "                           weight_quant=CommonWeightQuant,\n",
        "                           weight_bit_width=BIT_WIDTH,\n",
        "                           bias=False),\n",
        "            qn.QuantIdentity(act_quant=CommonActQuant, bit_width=BIT_WIDTH),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encode(x)\n",
        "        x = self.decode(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use non square conv2d\n",
        "\n",
        "\n",
        "class AE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encode = nn.Sequential(\n",
        "            # qn.QuantIdentity(act_quant=CommonActQuant, bit_width=BIT_WIDTH),\n",
        "            nn.Conv2d(11, 32, (3, 1),\n",
        "                           stride=(1, 1), padding=(1, 0),\n",
        "                           bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2,1), stride=(2,1)),\n",
        "\n",
        "            nn.Conv2d(32, 64, (3, 1),\n",
        "                           stride=(1, 1), padding=(1, 0),\n",
        "                           bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2,1), stride=(2,1)),\n",
        "        )\n",
        "        self.decode = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=(2,1)),\n",
        "            nn.Conv2d(64, 32, (3, 1),\n",
        "                           stride=1, padding=(1, 0),\n",
        "                           bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=(2,1)),\n",
        "            nn.Conv2d(32, 11, (3, 1),\n",
        "                           stride=1, padding=(1, 0),\n",
        "                           bias=False),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encode(x)\n",
        "        x = self.decode(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "QO5D6Vxeo1Jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glu7CK8sZquh"
      },
      "outputs": [],
      "source": [
        "ae = AE().to(device)\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(ae.parameters(), lr=0.001)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MT8HFfXkaFlW",
        "outputId": "1ad94f9e-1acd-45ef-9a48-355f23bd3c63"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.]],\n",
              "\n",
              "         [[0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.]],\n",
              "\n",
              "         [[0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.]],\n",
              "\n",
              "         [[0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.]],\n",
              "\n",
              "         [[0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.]],\n",
              "\n",
              "         [[0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.]],\n",
              "\n",
              "         [[0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.]],\n",
              "\n",
              "         [[0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.]],\n",
              "\n",
              "         [[0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.]],\n",
              "\n",
              "         [[0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.]],\n",
              "\n",
              "         [[0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.]]]], device='cuda:0', grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "_input = torch.rand(1, 11, 64, 1).to(device)\n",
        "ae(_input)\n",
        "\n",
        "\n",
        "# conv_res = nn.Conv2d(11, 32, (1, 3), padding=(0, 1))(_input)\n",
        "# print(conv_res.shape)\n",
        "# pool_res = nn.MaxPool2d((1, 2), stride=(1, 2))(conv_res)\n",
        "# print(pool_res.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcyHuiIO4Rr3",
        "outputId": "ae9f4073-9c36-4d8d-df1c-dea5c8a4585e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "ae.load_state_dict(torch.load('/content/drive/MyDrive/FinalYearProject/PYNQ/Models/TemporalConvNets/NonSquareConv2D_AE_2Stride_OutputQuantised_10_3_2025.model', map_location=torch.device(device), weights_only=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py-nYpLpND0Z"
      },
      "source": [
        "# **TLD**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySJJgCpDtX63"
      },
      "outputs": [],
      "source": [
        "class System(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.AE = AutoEncoder1D()\n",
        "        self.threshold = 0.6 # TODO:\n",
        "        self.classifier = nn.Sequential(\n",
        "            qn.QuantConv1d(11, 6, 3,\n",
        "                           stride=1, padding=1,\n",
        "                           input_quant=ActQuant,\n",
        "                           weight_quant=WeightQuant,\n",
        "                           bias_quant=Int8Bias,\n",
        "                           output_quant=ActQuant,\n",
        "                           return_quant_tensor=True,\n",
        "                           bias=use_bias),\n",
        "            qn.QuantReLU(act_quant=ActQuant, return_quant_tensor=True),\n",
        "            nn.MaxPool1d(2, stride=2),\n",
        "            qn.QuantConv1d(6, 1, 3,\n",
        "                           stride=1, padding=1,\n",
        "                           input_quant=ActQuant,\n",
        "                           weight_quant=WeightQuant,\n",
        "                           bias_quant=Int8Bias,\n",
        "                           output_quant=ActQuant,\n",
        "                           return_quant_tensor=True,\n",
        "                           bias=use_bias),\n",
        "            qn.QuantReLU(act_quant=ActQuant, return_quant_tensor=True),\n",
        "\n",
        "            qn.QuantLinear(8, 4,\n",
        "                           input_quant=ActQuant,\n",
        "                           weight_quant=WeightQuant,\n",
        "                           bias_quant=Int8Bias,\n",
        "                           output_quant=ActQuant,\n",
        "                           return_quant_tensor=True,\n",
        "                           bias=use_bias),\n",
        "            nn.Softmax(),\n",
        "        )\n",
        "\n",
        "        def forward(self, x):\n",
        "            latent = self.AE.encoder(x)\n",
        "            reconstructed = self.AE.decoder(latent)\n",
        "\n",
        "            if reconstructed_loss > self.threshold:\n",
        "                out = torch.zeros(1)\n",
        "            else:\n",
        "                out = self.classifier(latent)\n",
        "            return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlVxgRDsWuhL"
      },
      "source": [
        "# **Non-Quantised**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNIOhZSzWt7r"
      },
      "outputs": [],
      "source": [
        "\n",
        "use_bias = True\n",
        "DEBUG = False\n",
        "\n",
        "class NonQuantisedAutoEncoder1D(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.encode = nn.Sequential(\n",
        "            # This is a TCN - Temporal Convolutional network!\n",
        "            nn.ConstantPad1d((2,0), 0), # leftside padding means causal convolution\n",
        "            nn.Conv1d(11, 11, 3,\n",
        "                      stride=1, padding=0,\n",
        "                      bias=use_bias),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2, stride=2),\n",
        "\n",
        "            nn.ConstantPad1d((2,0), 0),\n",
        "            nn.Conv1d(11, 11, 3,\n",
        "                      stride=1, padding=0,\n",
        "                      bias=use_bias),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.decode = nn.Sequential(\n",
        "            nn.ConstantPad1d((2,0), 0),\n",
        "            nn.Conv1d(11, 11, 3,\n",
        "                      stride=1, padding=0,\n",
        "                      bias=use_bias),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.ConstantPad1d((2,0), 0),\n",
        "            nn.Conv1d(11, 11, 3,\n",
        "                      stride=1, padding=0,\n",
        "                      bias=use_bias),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.ConstantPad1d((2,0), 0),\n",
        "            nn.Conv1d(11, 11, 3,\n",
        "                      stride=1, padding=0,\n",
        "                      bias=use_bias),\n",
        "\n",
        "\n",
        "\n",
        "            nn.Conv1d(11, 11, 3,\n",
        "                      stride=1, padding=1,\n",
        "                      bias=use_bias),\n",
        "\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1d convolutions\n",
        "        out = self.encode(x)\n",
        "\n",
        "        out = self.decode(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZKHMydjXO02"
      },
      "outputs": [],
      "source": [
        "NQAutoEncoder = NonQuantisedAutoEncoder1D().to(device)\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(NQAutoEncoder.parameters(), lr=0.001)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfTIEjC4PmLY"
      },
      "source": [
        "# **Basic Model Training test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFbzTb1GPqax"
      },
      "outputs": [],
      "source": [
        "losses_to_plot = []\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    INNER_TRAINING_SAVE_PATH = '/content/drive/MyDrive/FinalYearProject/PYNQ/Models/CurrentModelInsideLoop'\n",
        "    size = len(dataloader)\n",
        "    GRANULARITY = 100\n",
        "    largest_in_grain = 0\n",
        "    running_loss = 0\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    for batch, (X, labels, cls) in enumerate(dataloader):\n",
        "        try:\n",
        "            if (len(X) != batch_size) and (len(X) != train_batch_size):\n",
        "                print(f\"Batch size warning! [{X.shape}]\")\n",
        "                continue\n",
        "\n",
        "            # Compute prediction and loss\n",
        "            # X = torch.reshape(X, (-1, 1, 64, 12))\n",
        "            # X = torch.reshape(X, (-1, 64, 12))\n",
        "            X = torch.reshape(X, (-1, 11, 64, 1))\n",
        "            # X = (X * 2) - 1.0\n",
        "            pred = model(X)\n",
        "\n",
        "            loss = loss_fn(pred, X)\n",
        "\n",
        "            if DEBUG:\n",
        "                print(f\"X   : {X}\", end='\\n-----------------------------------\\n')\n",
        "                print(f\"Pred: {pred}\")\n",
        "                print(f\"X Shape   : {X.shape}\", end='\\n-----------------------------------\\n')\n",
        "                print(f\"Pred Shape: {pred.shape}\")\n",
        "                return\n",
        "\n",
        "\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()       # Figures out the loss gradient of each weight\n",
        "            optimizer.step()      # Adjust all weights according to the given optimiser\n",
        "            optimizer.zero_grad() # Reset the optimiser gradient for next iteration\n",
        "\n",
        "            running_loss += loss\n",
        "\n",
        "            if loss > largest_in_grain:\n",
        "                largest_in_grain = loss\n",
        "\n",
        "            if batch % GRANULARITY == 0:\n",
        "                loss = loss.item()\n",
        "                losses_to_plot.append(loss)\n",
        "                current = batch\n",
        "\n",
        "                avg_loss = running_loss / GRANULARITY\n",
        "                running_loss = 0\n",
        "                print(f\"loss: {avg_loss:>7f}  [{current:>5d}/{size:>5d}] <-> largest in grain: {largest_in_grain:>7f}\")\n",
        "                largest_in_grain = 0\n",
        "                if loss > 1000:\n",
        "                    print(f\"X:    {X.shape} -\\n{X}\")\n",
        "                    print(f\"Pred: {pred.shape} -\\n{pred}\")\n",
        "            if batch % 2000 == 0:\n",
        "                torch.save(model.state_dict(), INNER_TRAINING_SAVE_PATH)\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"Interrupted\")\n",
        "            torch.save(model.state_dict(), INNER_TRAINING_SAVE_PATH)\n",
        "            return False\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DiJJfxBCKCs"
      },
      "source": [
        "# **Train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PHv52gR3oDSq",
        "outputId": "49f13fe9-5fcb-43ea-ecc3-a81ccb62efbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.003573  [    0/12360] <-> largest in grain: 0.357305\n",
            "loss: 0.236344  [  100/12360] <-> largest in grain: 0.350370\n",
            "loss: 0.171267  [  200/12360] <-> largest in grain: 0.202922\n",
            "loss: 0.126250  [  300/12360] <-> largest in grain: 0.144887\n",
            "loss: 0.097509  [  400/12360] <-> largest in grain: 0.109646\n",
            "loss: 0.080999  [  500/12360] <-> largest in grain: 0.091219\n",
            "loss: 0.071329  [  600/12360] <-> largest in grain: 0.079184\n",
            "loss: 0.065409  [  700/12360] <-> largest in grain: 0.071083\n",
            "loss: 0.061061  [  800/12360] <-> largest in grain: 0.066082\n",
            "loss: 0.057033  [  900/12360] <-> largest in grain: 0.061660\n",
            "loss: 0.054057  [ 1000/12360] <-> largest in grain: 0.058923\n",
            "loss: 0.051464  [ 1100/12360] <-> largest in grain: 0.055724\n",
            "loss: 0.049014  [ 1200/12360] <-> largest in grain: 0.052358\n",
            "loss: 0.047317  [ 1300/12360] <-> largest in grain: 0.051027\n",
            "loss: 0.045634  [ 1400/12360] <-> largest in grain: 0.049008\n",
            "loss: 0.044126  [ 1500/12360] <-> largest in grain: 0.047339\n",
            "loss: 0.043414  [ 1600/12360] <-> largest in grain: 0.046530\n",
            "loss: 0.042214  [ 1700/12360] <-> largest in grain: 0.047513\n",
            "loss: 0.041647  [ 1800/12360] <-> largest in grain: 0.044363\n",
            "loss: 0.040280  [ 1900/12360] <-> largest in grain: 0.044187\n",
            "loss: 0.039996  [ 2000/12360] <-> largest in grain: 0.042978\n",
            "loss: 0.039348  [ 2100/12360] <-> largest in grain: 0.042929\n",
            "loss: 0.038671  [ 2200/12360] <-> largest in grain: 0.041703\n",
            "loss: 0.038107  [ 2300/12360] <-> largest in grain: 0.041715\n",
            "loss: 0.037572  [ 2400/12360] <-> largest in grain: 0.040556\n",
            "loss: 0.037060  [ 2500/12360] <-> largest in grain: 0.039662\n",
            "loss: 0.036530  [ 2600/12360] <-> largest in grain: 0.039908\n",
            "loss: 0.036070  [ 2700/12360] <-> largest in grain: 0.038815\n",
            "loss: 0.035773  [ 2800/12360] <-> largest in grain: 0.038340\n",
            "loss: 0.035465  [ 2900/12360] <-> largest in grain: 0.038153\n",
            "loss: 0.034960  [ 3000/12360] <-> largest in grain: 0.037838\n",
            "loss: 0.034623  [ 3100/12360] <-> largest in grain: 0.037598\n",
            "loss: 0.034248  [ 3200/12360] <-> largest in grain: 0.037595\n",
            "loss: 0.033958  [ 3300/12360] <-> largest in grain: 0.036236\n",
            "loss: 0.033823  [ 3400/12360] <-> largest in grain: 0.036183\n",
            "loss: 0.033583  [ 3500/12360] <-> largest in grain: 0.036271\n",
            "loss: 0.033309  [ 3600/12360] <-> largest in grain: 0.035753\n",
            "loss: 0.032783  [ 3700/12360] <-> largest in grain: 0.035352\n",
            "loss: 0.032741  [ 3800/12360] <-> largest in grain: 0.036349\n",
            "loss: 0.032455  [ 3900/12360] <-> largest in grain: 0.035114\n",
            "loss: 0.032449  [ 4000/12360] <-> largest in grain: 0.036807\n",
            "loss: 0.032165  [ 4100/12360] <-> largest in grain: 0.034495\n",
            "loss: 0.032087  [ 4200/12360] <-> largest in grain: 0.034907\n",
            "loss: 0.031966  [ 4300/12360] <-> largest in grain: 0.034907\n",
            "loss: 0.031603  [ 4400/12360] <-> largest in grain: 0.034506\n",
            "loss: 0.031396  [ 4500/12360] <-> largest in grain: 0.035550\n",
            "loss: 0.031392  [ 4600/12360] <-> largest in grain: 0.034449\n",
            "loss: 0.031285  [ 4700/12360] <-> largest in grain: 0.033706\n",
            "loss: 0.031190  [ 4800/12360] <-> largest in grain: 0.036559\n",
            "loss: 0.031065  [ 4900/12360] <-> largest in grain: 0.033190\n",
            "loss: 0.030818  [ 5000/12360] <-> largest in grain: 0.033081\n",
            "loss: 0.030636  [ 5100/12360] <-> largest in grain: 0.033554\n",
            "loss: 0.030587  [ 5200/12360] <-> largest in grain: 0.033283\n",
            "loss: 0.030238  [ 5300/12360] <-> largest in grain: 0.033582\n",
            "loss: 0.030000  [ 5400/12360] <-> largest in grain: 0.032387\n",
            "loss: 0.030164  [ 5500/12360] <-> largest in grain: 0.032677\n",
            "loss: 0.029890  [ 5600/12360] <-> largest in grain: 0.032808\n",
            "loss: 0.030007  [ 5700/12360] <-> largest in grain: 0.032744\n",
            "loss: 0.029809  [ 5800/12360] <-> largest in grain: 0.032002\n",
            "loss: 0.029729  [ 5900/12360] <-> largest in grain: 0.032173\n",
            "loss: 0.029711  [ 6000/12360] <-> largest in grain: 0.032649\n",
            "loss: 0.029561  [ 6100/12360] <-> largest in grain: 0.033226\n",
            "loss: 0.029161  [ 6200/12360] <-> largest in grain: 0.031730\n",
            "loss: 0.029049  [ 6300/12360] <-> largest in grain: 0.032178\n",
            "loss: 0.029124  [ 6400/12360] <-> largest in grain: 0.031198\n",
            "loss: 0.028951  [ 6500/12360] <-> largest in grain: 0.031749\n",
            "loss: 0.028951  [ 6600/12360] <-> largest in grain: 0.032034\n",
            "loss: 0.028869  [ 6700/12360] <-> largest in grain: 0.030945\n",
            "loss: 0.028803  [ 6800/12360] <-> largest in grain: 0.031736\n",
            "loss: 0.028724  [ 6900/12360] <-> largest in grain: 0.031077\n",
            "loss: 0.028476  [ 7000/12360] <-> largest in grain: 0.031447\n",
            "loss: 0.028522  [ 7100/12360] <-> largest in grain: 0.030567\n",
            "loss: 0.028398  [ 7200/12360] <-> largest in grain: 0.030552\n",
            "loss: 0.028308  [ 7300/12360] <-> largest in grain: 0.030837\n",
            "loss: 0.028291  [ 7400/12360] <-> largest in grain: 0.030953\n",
            "loss: 0.028434  [ 7500/12360] <-> largest in grain: 0.030939\n",
            "loss: 0.028191  [ 7600/12360] <-> largest in grain: 0.030704\n",
            "loss: 0.027954  [ 7700/12360] <-> largest in grain: 0.030995\n",
            "loss: 0.028083  [ 7800/12360] <-> largest in grain: 0.031251\n",
            "loss: 0.027999  [ 7900/12360] <-> largest in grain: 0.030032\n",
            "loss: 0.027780  [ 8000/12360] <-> largest in grain: 0.029998\n",
            "loss: 0.027654  [ 8100/12360] <-> largest in grain: 0.030050\n",
            "loss: 0.027760  [ 8200/12360] <-> largest in grain: 0.030348\n",
            "loss: 0.027825  [ 8300/12360] <-> largest in grain: 0.030679\n",
            "loss: 0.027672  [ 8400/12360] <-> largest in grain: 0.030271\n",
            "loss: 0.027494  [ 8500/12360] <-> largest in grain: 0.030508\n",
            "loss: 0.027604  [ 8600/12360] <-> largest in grain: 0.029897\n",
            "loss: 0.027599  [ 8700/12360] <-> largest in grain: 0.029567\n",
            "loss: 0.027494  [ 8800/12360] <-> largest in grain: 0.029964\n",
            "loss: 0.027472  [ 8900/12360] <-> largest in grain: 0.031247\n",
            "loss: 0.027294  [ 9000/12360] <-> largest in grain: 0.029300\n",
            "loss: 0.027155  [ 9100/12360] <-> largest in grain: 0.029582\n",
            "loss: 0.027197  [ 9200/12360] <-> largest in grain: 0.029889\n",
            "loss: 0.027264  [ 9300/12360] <-> largest in grain: 0.029336\n",
            "loss: 0.027179  [ 9400/12360] <-> largest in grain: 0.029517\n",
            "loss: 0.027031  [ 9500/12360] <-> largest in grain: 0.029284\n",
            "loss: 0.027159  [ 9600/12360] <-> largest in grain: 0.029071\n",
            "loss: 0.027095  [ 9700/12360] <-> largest in grain: 0.029267\n",
            "loss: 0.026687  [ 9800/12360] <-> largest in grain: 0.028384\n",
            "loss: 0.026995  [ 9900/12360] <-> largest in grain: 0.029632\n",
            "loss: 0.027148  [10000/12360] <-> largest in grain: 0.028781\n",
            "loss: 0.026844  [10100/12360] <-> largest in grain: 0.028651\n",
            "loss: 0.026784  [10200/12360] <-> largest in grain: 0.029288\n",
            "loss: 0.026510  [10300/12360] <-> largest in grain: 0.029678\n",
            "loss: 0.026581  [10400/12360] <-> largest in grain: 0.029049\n",
            "loss: 0.026600  [10500/12360] <-> largest in grain: 0.029439\n",
            "loss: 0.026538  [10600/12360] <-> largest in grain: 0.029328\n",
            "loss: 0.026547  [10700/12360] <-> largest in grain: 0.028985\n",
            "loss: 0.026567  [10800/12360] <-> largest in grain: 0.029606\n",
            "loss: 0.026466  [10900/12360] <-> largest in grain: 0.028996\n",
            "loss: 0.026449  [11000/12360] <-> largest in grain: 0.028677\n",
            "loss: 0.026435  [11100/12360] <-> largest in grain: 0.029114\n",
            "loss: 0.026208  [11200/12360] <-> largest in grain: 0.028592\n",
            "loss: 0.026137  [11300/12360] <-> largest in grain: 0.029396\n",
            "loss: 0.026140  [11400/12360] <-> largest in grain: 0.028488\n",
            "loss: 0.026287  [11500/12360] <-> largest in grain: 0.028321\n",
            "loss: 0.026072  [11600/12360] <-> largest in grain: 0.029107\n",
            "loss: 0.026073  [11700/12360] <-> largest in grain: 0.028366\n",
            "loss: 0.025966  [11800/12360] <-> largest in grain: 0.028296\n",
            "loss: 0.025952  [11900/12360] <-> largest in grain: 0.028551\n",
            "loss: 0.026120  [12000/12360] <-> largest in grain: 0.028136\n",
            "loss: 0.025866  [12100/12360] <-> largest in grain: 0.028024\n",
            "loss: 0.026010  [12200/12360] <-> largest in grain: 0.028528\n",
            "loss: 0.026011  [12300/12360] <-> largest in grain: 0.028002\n",
            "Batch size warning! [torch.Size([56, 11, 64])]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.000260  [    0/12360] <-> largest in grain: 0.025968\n",
            "loss: 0.025792  [  100/12360] <-> largest in grain: 0.027462\n",
            "loss: 0.025714  [  200/12360] <-> largest in grain: 0.028727\n",
            "loss: 0.025916  [  300/12360] <-> largest in grain: 0.028146\n",
            "loss: 0.025682  [  400/12360] <-> largest in grain: 0.028534\n",
            "loss: 0.025870  [  500/12360] <-> largest in grain: 0.027822\n",
            "loss: 0.025607  [  600/12360] <-> largest in grain: 0.027606\n",
            "loss: 0.025646  [  700/12360] <-> largest in grain: 0.027655\n",
            "loss: 0.025726  [  800/12360] <-> largest in grain: 0.027714\n",
            "loss: 0.025712  [  900/12360] <-> largest in grain: 0.029073\n",
            "loss: 0.025615  [ 1000/12360] <-> largest in grain: 0.029032\n",
            "loss: 0.025840  [ 1100/12360] <-> largest in grain: 0.027825\n",
            "loss: 0.025596  [ 1200/12360] <-> largest in grain: 0.027342\n",
            "loss: 0.025476  [ 1300/12360] <-> largest in grain: 0.028054\n",
            "loss: 0.025509  [ 1400/12360] <-> largest in grain: 0.027641\n",
            "loss: 0.025585  [ 1500/12360] <-> largest in grain: 0.028274\n",
            "loss: 0.025403  [ 1600/12360] <-> largest in grain: 0.027778\n",
            "loss: 0.025298  [ 1700/12360] <-> largest in grain: 0.028100\n",
            "loss: 0.025501  [ 1800/12360] <-> largest in grain: 0.028552\n",
            "loss: 0.025396  [ 1900/12360] <-> largest in grain: 0.028234\n",
            "loss: 0.025455  [ 2000/12360] <-> largest in grain: 0.027164\n",
            "loss: 0.025287  [ 2100/12360] <-> largest in grain: 0.028197\n",
            "loss: 0.025349  [ 2200/12360] <-> largest in grain: 0.027301\n",
            "loss: 0.025167  [ 2300/12360] <-> largest in grain: 0.027099\n",
            "loss: 0.025024  [ 2400/12360] <-> largest in grain: 0.027124\n",
            "loss: 0.025253  [ 2500/12360] <-> largest in grain: 0.027169\n",
            "loss: 0.025172  [ 2600/12360] <-> largest in grain: 0.028086\n",
            "loss: 0.025176  [ 2700/12360] <-> largest in grain: 0.027168\n",
            "loss: 0.025246  [ 2800/12360] <-> largest in grain: 0.027244\n",
            "loss: 0.025129  [ 2900/12360] <-> largest in grain: 0.027154\n",
            "loss: 0.025094  [ 3000/12360] <-> largest in grain: 0.027325\n",
            "loss: 0.024942  [ 3100/12360] <-> largest in grain: 0.027768\n",
            "loss: 0.025076  [ 3200/12360] <-> largest in grain: 0.026939\n",
            "loss: 0.025000  [ 3300/12360] <-> largest in grain: 0.027224\n",
            "loss: 0.024944  [ 3400/12360] <-> largest in grain: 0.026966\n",
            "loss: 0.024796  [ 3500/12360] <-> largest in grain: 0.027212\n",
            "loss: 0.024901  [ 3600/12360] <-> largest in grain: 0.027123\n",
            "loss: 0.024798  [ 3700/12360] <-> largest in grain: 0.026648\n",
            "loss: 0.024890  [ 3800/12360] <-> largest in grain: 0.026802\n",
            "loss: 0.024989  [ 3900/12360] <-> largest in grain: 0.027374\n",
            "loss: 0.024803  [ 4000/12360] <-> largest in grain: 0.027357\n",
            "loss: 0.024821  [ 4100/12360] <-> largest in grain: 0.026459\n",
            "loss: 0.024778  [ 4200/12360] <-> largest in grain: 0.026614\n",
            "loss: 0.024961  [ 4300/12360] <-> largest in grain: 0.026700\n",
            "loss: 0.024640  [ 4400/12360] <-> largest in grain: 0.026737\n",
            "loss: 0.024785  [ 4500/12360] <-> largest in grain: 0.027108\n",
            "loss: 0.024630  [ 4600/12360] <-> largest in grain: 0.027317\n",
            "loss: 0.024576  [ 4700/12360] <-> largest in grain: 0.026893\n",
            "loss: 0.024727  [ 4800/12360] <-> largest in grain: 0.026527\n",
            "loss: 0.024773  [ 4900/12360] <-> largest in grain: 0.026498\n",
            "loss: 0.024717  [ 5000/12360] <-> largest in grain: 0.027390\n",
            "loss: 0.024757  [ 5100/12360] <-> largest in grain: 0.026491\n",
            "loss: 0.024589  [ 5200/12360] <-> largest in grain: 0.026374\n",
            "loss: 0.024503  [ 5300/12360] <-> largest in grain: 0.026869\n",
            "loss: 0.024505  [ 5400/12360] <-> largest in grain: 0.026317\n",
            "loss: 0.024561  [ 5500/12360] <-> largest in grain: 0.026904\n",
            "loss: 0.024579  [ 5600/12360] <-> largest in grain: 0.027258\n",
            "loss: 0.024540  [ 5700/12360] <-> largest in grain: 0.026433\n",
            "loss: 0.024371  [ 5800/12360] <-> largest in grain: 0.026094\n",
            "loss: 0.024446  [ 5900/12360] <-> largest in grain: 0.026289\n",
            "loss: 0.024663  [ 6000/12360] <-> largest in grain: 0.026648\n",
            "loss: 0.024324  [ 6100/12360] <-> largest in grain: 0.026692\n",
            "loss: 0.024317  [ 6200/12360] <-> largest in grain: 0.027287\n",
            "loss: 0.024298  [ 6300/12360] <-> largest in grain: 0.026835\n",
            "loss: 0.024218  [ 6400/12360] <-> largest in grain: 0.027895\n",
            "loss: 0.024394  [ 6500/12360] <-> largest in grain: 0.027198\n",
            "loss: 0.024311  [ 6600/12360] <-> largest in grain: 0.027557\n",
            "loss: 0.024294  [ 6700/12360] <-> largest in grain: 0.026455\n",
            "loss: 0.024414  [ 6800/12360] <-> largest in grain: 0.027242\n",
            "loss: 0.024346  [ 6900/12360] <-> largest in grain: 0.026599\n",
            "loss: 0.024226  [ 7000/12360] <-> largest in grain: 0.025766\n",
            "loss: 0.024239  [ 7100/12360] <-> largest in grain: 0.026460\n",
            "loss: 0.024226  [ 7200/12360] <-> largest in grain: 0.026986\n",
            "loss: 0.024201  [ 7300/12360] <-> largest in grain: 0.027145\n",
            "loss: 0.023964  [ 7400/12360] <-> largest in grain: 0.026064\n",
            "loss: 0.024137  [ 7500/12360] <-> largest in grain: 0.027017\n",
            "loss: 0.024193  [ 7600/12360] <-> largest in grain: 0.026120\n",
            "loss: 0.024273  [ 7700/12360] <-> largest in grain: 0.026149\n",
            "loss: 0.024166  [ 7800/12360] <-> largest in grain: 0.026853\n",
            "loss: 0.024117  [ 7900/12360] <-> largest in grain: 0.026252\n",
            "loss: 0.023893  [ 8000/12360] <-> largest in grain: 0.025734\n",
            "loss: 0.023894  [ 8100/12360] <-> largest in grain: 0.026358\n",
            "loss: 0.023944  [ 8200/12360] <-> largest in grain: 0.026419\n",
            "loss: 0.023877  [ 8300/12360] <-> largest in grain: 0.025636\n",
            "loss: 0.023746  [ 8400/12360] <-> largest in grain: 0.026161\n",
            "loss: 0.024028  [ 8500/12360] <-> largest in grain: 0.026121\n",
            "loss: 0.023952  [ 8600/12360] <-> largest in grain: 0.026427\n",
            "loss: 0.023895  [ 8700/12360] <-> largest in grain: 0.026317\n",
            "loss: 0.023780  [ 8800/12360] <-> largest in grain: 0.025969\n",
            "loss: 0.023765  [ 8900/12360] <-> largest in grain: 0.025723\n",
            "loss: 0.023804  [ 9000/12360] <-> largest in grain: 0.025463\n",
            "loss: 0.023813  [ 9100/12360] <-> largest in grain: 0.026528\n",
            "loss: 0.023723  [ 9200/12360] <-> largest in grain: 0.026326\n",
            "loss: 0.023768  [ 9300/12360] <-> largest in grain: 0.025742\n",
            "loss: 0.023746  [ 9400/12360] <-> largest in grain: 0.025659\n",
            "loss: 0.023765  [ 9500/12360] <-> largest in grain: 0.026699\n",
            "loss: 0.023729  [ 9600/12360] <-> largest in grain: 0.026351\n",
            "loss: 0.023736  [ 9700/12360] <-> largest in grain: 0.025883\n",
            "loss: 0.023651  [ 9800/12360] <-> largest in grain: 0.025640\n",
            "loss: 0.023587  [ 9900/12360] <-> largest in grain: 0.025932\n",
            "loss: 0.023683  [10000/12360] <-> largest in grain: 0.026944\n",
            "loss: 0.023692  [10100/12360] <-> largest in grain: 0.027016\n",
            "loss: 0.023529  [10200/12360] <-> largest in grain: 0.026659\n",
            "loss: 0.023626  [10300/12360] <-> largest in grain: 0.025246\n",
            "loss: 0.023552  [10400/12360] <-> largest in grain: 0.025116\n",
            "loss: 0.023687  [10500/12360] <-> largest in grain: 0.025828\n",
            "loss: 0.023499  [10600/12360] <-> largest in grain: 0.025802\n",
            "loss: 0.023551  [10700/12360] <-> largest in grain: 0.025202\n",
            "loss: 0.023442  [10800/12360] <-> largest in grain: 0.025006\n",
            "loss: 0.023585  [10900/12360] <-> largest in grain: 0.025369\n",
            "loss: 0.023425  [11000/12360] <-> largest in grain: 0.025757\n",
            "loss: 0.023574  [11100/12360] <-> largest in grain: 0.025416\n",
            "loss: 0.023402  [11200/12360] <-> largest in grain: 0.026115\n",
            "loss: 0.023541  [11300/12360] <-> largest in grain: 0.025934\n",
            "loss: 0.023386  [11400/12360] <-> largest in grain: 0.025978\n",
            "loss: 0.023363  [11500/12360] <-> largest in grain: 0.025254\n",
            "loss: 0.023503  [11600/12360] <-> largest in grain: 0.025308\n",
            "loss: 0.023473  [11700/12360] <-> largest in grain: 0.026365\n",
            "loss: 0.023469  [11800/12360] <-> largest in grain: 0.025254\n",
            "loss: 0.023375  [11900/12360] <-> largest in grain: 0.025189\n",
            "loss: 0.023342  [12000/12360] <-> largest in grain: 0.025211\n",
            "loss: 0.023265  [12100/12360] <-> largest in grain: 0.025276\n",
            "loss: 0.023244  [12200/12360] <-> largest in grain: 0.025086\n",
            "loss: 0.023038  [12300/12360] <-> largest in grain: 0.024748\n",
            "Batch size warning! [torch.Size([56, 11, 64])]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.000239  [    0/12360] <-> largest in grain: 0.023890\n",
            "loss: 0.023240  [  100/12360] <-> largest in grain: 0.024941\n",
            "loss: 0.023118  [  200/12360] <-> largest in grain: 0.025947\n",
            "loss: 0.023189  [  300/12360] <-> largest in grain: 0.025470\n",
            "loss: 0.023142  [  400/12360] <-> largest in grain: 0.024957\n",
            "loss: 0.023032  [  500/12360] <-> largest in grain: 0.024985\n",
            "loss: 0.023063  [  600/12360] <-> largest in grain: 0.025418\n",
            "loss: 0.023118  [  700/12360] <-> largest in grain: 0.025302\n",
            "loss: 0.023116  [  800/12360] <-> largest in grain: 0.024990\n",
            "loss: 0.023134  [  900/12360] <-> largest in grain: 0.025996\n",
            "loss: 0.022990  [ 1000/12360] <-> largest in grain: 0.025051\n",
            "loss: 0.023086  [ 1100/12360] <-> largest in grain: 0.025577\n",
            "loss: 0.022984  [ 1200/12360] <-> largest in grain: 0.024842\n",
            "loss: 0.023119  [ 1300/12360] <-> largest in grain: 0.025133\n",
            "loss: 0.023096  [ 1400/12360] <-> largest in grain: 0.024933\n",
            "loss: 0.022926  [ 1500/12360] <-> largest in grain: 0.024931\n",
            "loss: 0.023080  [ 1600/12360] <-> largest in grain: 0.025139\n",
            "loss: 0.022982  [ 1700/12360] <-> largest in grain: 0.024952\n",
            "loss: 0.023090  [ 1800/12360] <-> largest in grain: 0.024922\n",
            "loss: 0.023020  [ 1900/12360] <-> largest in grain: 0.025809\n",
            "loss: 0.023075  [ 2000/12360] <-> largest in grain: 0.025021\n",
            "loss: 0.023085  [ 2100/12360] <-> largest in grain: 0.024782\n",
            "loss: 0.023029  [ 2200/12360] <-> largest in grain: 0.026218\n",
            "loss: 0.023030  [ 2300/12360] <-> largest in grain: 0.025407\n",
            "loss: 0.022870  [ 2400/12360] <-> largest in grain: 0.025102\n",
            "loss: 0.023122  [ 2500/12360] <-> largest in grain: 0.025488\n",
            "loss: 0.023044  [ 2600/12360] <-> largest in grain: 0.025303\n",
            "loss: 0.022982  [ 2700/12360] <-> largest in grain: 0.024650\n",
            "loss: 0.022991  [ 2800/12360] <-> largest in grain: 0.025415\n",
            "loss: 0.023205  [ 2900/12360] <-> largest in grain: 0.025422\n",
            "loss: 0.022939  [ 3000/12360] <-> largest in grain: 0.025012\n",
            "loss: 0.022908  [ 3100/12360] <-> largest in grain: 0.024990\n",
            "loss: 0.023123  [ 3200/12360] <-> largest in grain: 0.025799\n",
            "loss: 0.022927  [ 3300/12360] <-> largest in grain: 0.025677\n",
            "loss: 0.022905  [ 3400/12360] <-> largest in grain: 0.024515\n",
            "loss: 0.023005  [ 3500/12360] <-> largest in grain: 0.025219\n",
            "loss: 0.022958  [ 3600/12360] <-> largest in grain: 0.024667\n",
            "loss: 0.022931  [ 3700/12360] <-> largest in grain: 0.025519\n",
            "loss: 0.022792  [ 3800/12360] <-> largest in grain: 0.024742\n",
            "loss: 0.022912  [ 3900/12360] <-> largest in grain: 0.024922\n",
            "loss: 0.022981  [ 4000/12360] <-> largest in grain: 0.025244\n",
            "loss: 0.022799  [ 4100/12360] <-> largest in grain: 0.025288\n",
            "loss: 0.022824  [ 4200/12360] <-> largest in grain: 0.025062\n",
            "loss: 0.022887  [ 4300/12360] <-> largest in grain: 0.025169\n",
            "loss: 0.022964  [ 4400/12360] <-> largest in grain: 0.025031\n",
            "loss: 0.022868  [ 4500/12360] <-> largest in grain: 0.024918\n",
            "loss: 0.022825  [ 4600/12360] <-> largest in grain: 0.025275\n",
            "loss: 0.022908  [ 4700/12360] <-> largest in grain: 0.025018\n",
            "loss: 0.022770  [ 4800/12360] <-> largest in grain: 0.025315\n",
            "loss: 0.022946  [ 4900/12360] <-> largest in grain: 0.025344\n",
            "loss: 0.022784  [ 5000/12360] <-> largest in grain: 0.024920\n",
            "loss: 0.022861  [ 5100/12360] <-> largest in grain: 0.025230\n",
            "loss: 0.022712  [ 5200/12360] <-> largest in grain: 0.024746\n",
            "loss: 0.022759  [ 5300/12360] <-> largest in grain: 0.025067\n",
            "loss: 0.022859  [ 5400/12360] <-> largest in grain: 0.024831\n",
            "loss: 0.022647  [ 5500/12360] <-> largest in grain: 0.024943\n",
            "loss: 0.022774  [ 5600/12360] <-> largest in grain: 0.024509\n",
            "loss: 0.022817  [ 5700/12360] <-> largest in grain: 0.024707\n",
            "loss: 0.022772  [ 5800/12360] <-> largest in grain: 0.025266\n",
            "loss: 0.022745  [ 5900/12360] <-> largest in grain: 0.024557\n",
            "loss: 0.022677  [ 6000/12360] <-> largest in grain: 0.024967\n",
            "loss: 0.022774  [ 6100/12360] <-> largest in grain: 0.025016\n",
            "loss: 0.022728  [ 6200/12360] <-> largest in grain: 0.024772\n",
            "loss: 0.022678  [ 6300/12360] <-> largest in grain: 0.024581\n",
            "loss: 0.022748  [ 6400/12360] <-> largest in grain: 0.025480\n",
            "loss: 0.022630  [ 6500/12360] <-> largest in grain: 0.024865\n",
            "loss: 0.022704  [ 6600/12360] <-> largest in grain: 0.024450\n",
            "loss: 0.022670  [ 6700/12360] <-> largest in grain: 0.024923\n",
            "loss: 0.022608  [ 6800/12360] <-> largest in grain: 0.024847\n",
            "loss: 0.022715  [ 6900/12360] <-> largest in grain: 0.024352\n",
            "loss: 0.022751  [ 7000/12360] <-> largest in grain: 0.025071\n",
            "loss: 0.022793  [ 7100/12360] <-> largest in grain: 0.024806\n",
            "loss: 0.022662  [ 7200/12360] <-> largest in grain: 0.024609\n",
            "loss: 0.022673  [ 7300/12360] <-> largest in grain: 0.024239\n",
            "loss: 0.022543  [ 7400/12360] <-> largest in grain: 0.024810\n",
            "loss: 0.022695  [ 7500/12360] <-> largest in grain: 0.024784\n",
            "loss: 0.022626  [ 7600/12360] <-> largest in grain: 0.024513\n",
            "loss: 0.022551  [ 7700/12360] <-> largest in grain: 0.024712\n",
            "loss: 0.022537  [ 7800/12360] <-> largest in grain: 0.024453\n",
            "loss: 0.022637  [ 7900/12360] <-> largest in grain: 0.024622\n",
            "loss: 0.022458  [ 8000/12360] <-> largest in grain: 0.024703\n",
            "loss: 0.022568  [ 8100/12360] <-> largest in grain: 0.024180\n",
            "loss: 0.022540  [ 8200/12360] <-> largest in grain: 0.024845\n",
            "loss: 0.022485  [ 8300/12360] <-> largest in grain: 0.024135\n",
            "loss: 0.022644  [ 8400/12360] <-> largest in grain: 0.025287\n",
            "loss: 0.022486  [ 8500/12360] <-> largest in grain: 0.024453\n",
            "loss: 0.022450  [ 8600/12360] <-> largest in grain: 0.024421\n",
            "loss: 0.022643  [ 8700/12360] <-> largest in grain: 0.025121\n",
            "loss: 0.022621  [ 8800/12360] <-> largest in grain: 0.024189\n",
            "loss: 0.022566  [ 8900/12360] <-> largest in grain: 0.024393\n",
            "loss: 0.022735  [ 9000/12360] <-> largest in grain: 0.024765\n",
            "loss: 0.022480  [ 9100/12360] <-> largest in grain: 0.024924\n",
            "loss: 0.022394  [ 9200/12360] <-> largest in grain: 0.024204\n",
            "loss: 0.022551  [ 9300/12360] <-> largest in grain: 0.025011\n",
            "loss: 0.022621  [ 9400/12360] <-> largest in grain: 0.024850\n",
            "loss: 0.022557  [ 9500/12360] <-> largest in grain: 0.024572\n",
            "loss: 0.022490  [ 9600/12360] <-> largest in grain: 0.024683\n",
            "loss: 0.022516  [ 9700/12360] <-> largest in grain: 0.024278\n",
            "loss: 0.022644  [ 9800/12360] <-> largest in grain: 0.024361\n",
            "loss: 0.022335  [ 9900/12360] <-> largest in grain: 0.024323\n",
            "loss: 0.022414  [10000/12360] <-> largest in grain: 0.024182\n",
            "loss: 0.022279  [10100/12360] <-> largest in grain: 0.024116\n",
            "loss: 0.022459  [10200/12360] <-> largest in grain: 0.024232\n",
            "loss: 0.022182  [10300/12360] <-> largest in grain: 0.024594\n",
            "loss: 0.022404  [10400/12360] <-> largest in grain: 0.024976\n",
            "loss: 0.022405  [10500/12360] <-> largest in grain: 0.024443\n",
            "loss: 0.022403  [10600/12360] <-> largest in grain: 0.023828\n",
            "loss: 0.022363  [10700/12360] <-> largest in grain: 0.024442\n",
            "loss: 0.022408  [10800/12360] <-> largest in grain: 0.025449\n",
            "loss: 0.022384  [10900/12360] <-> largest in grain: 0.024382\n",
            "loss: 0.022524  [11000/12360] <-> largest in grain: 0.024997\n",
            "loss: 0.022505  [11100/12360] <-> largest in grain: 0.024525\n",
            "loss: 0.022334  [11200/12360] <-> largest in grain: 0.024209\n",
            "loss: 0.022334  [11300/12360] <-> largest in grain: 0.023898\n",
            "loss: 0.022465  [11400/12360] <-> largest in grain: 0.024360\n",
            "loss: 0.022345  [11500/12360] <-> largest in grain: 0.025223\n",
            "loss: 0.022348  [11600/12360] <-> largest in grain: 0.024833\n",
            "loss: 0.022408  [11700/12360] <-> largest in grain: 0.024912\n",
            "loss: 0.022479  [11800/12360] <-> largest in grain: 0.024206\n",
            "loss: 0.022359  [11900/12360] <-> largest in grain: 0.024592\n",
            "loss: 0.022365  [12000/12360] <-> largest in grain: 0.024041\n",
            "loss: 0.022499  [12100/12360] <-> largest in grain: 0.024510\n",
            "loss: 0.022414  [12200/12360] <-> largest in grain: 0.024433\n",
            "loss: 0.022261  [12300/12360] <-> largest in grain: 0.024504\n",
            "Batch size warning! [torch.Size([56, 11, 64])]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.000214  [    0/12360] <-> largest in grain: 0.021424\n",
            "loss: 0.022458  [  100/12360] <-> largest in grain: 0.024293\n",
            "loss: 0.022215  [  200/12360] <-> largest in grain: 0.024003\n",
            "loss: 0.022397  [  300/12360] <-> largest in grain: 0.025315\n",
            "loss: 0.022279  [  400/12360] <-> largest in grain: 0.024272\n",
            "loss: 0.022334  [  500/12360] <-> largest in grain: 0.024740\n",
            "loss: 0.022474  [  600/12360] <-> largest in grain: 0.024504\n",
            "loss: 0.022077  [  700/12360] <-> largest in grain: 0.024046\n",
            "loss: 0.022155  [  800/12360] <-> largest in grain: 0.024464\n",
            "loss: 0.022228  [  900/12360] <-> largest in grain: 0.024218\n",
            "loss: 0.022344  [ 1000/12360] <-> largest in grain: 0.023814\n",
            "loss: 0.022274  [ 1100/12360] <-> largest in grain: 0.024571\n",
            "loss: 0.022313  [ 1200/12360] <-> largest in grain: 0.024855\n",
            "loss: 0.022218  [ 1300/12360] <-> largest in grain: 0.023800\n",
            "loss: 0.022243  [ 1400/12360] <-> largest in grain: 0.025140\n",
            "loss: 0.022187  [ 1500/12360] <-> largest in grain: 0.024084\n",
            "loss: 0.022299  [ 1600/12360] <-> largest in grain: 0.024174\n",
            "loss: 0.022213  [ 1700/12360] <-> largest in grain: 0.024014\n",
            "loss: 0.022292  [ 1800/12360] <-> largest in grain: 0.025100\n",
            "loss: 0.022143  [ 1900/12360] <-> largest in grain: 0.023871\n",
            "loss: 0.022219  [ 2000/12360] <-> largest in grain: 0.024041\n",
            "loss: 0.022214  [ 2100/12360] <-> largest in grain: 0.023888\n",
            "loss: 0.022172  [ 2200/12360] <-> largest in grain: 0.024569\n",
            "loss: 0.022171  [ 2300/12360] <-> largest in grain: 0.024105\n",
            "loss: 0.022292  [ 2400/12360] <-> largest in grain: 0.023932\n",
            "loss: 0.022196  [ 2500/12360] <-> largest in grain: 0.024034\n",
            "loss: 0.022187  [ 2600/12360] <-> largest in grain: 0.023993\n",
            "loss: 0.022145  [ 2700/12360] <-> largest in grain: 0.023691\n",
            "loss: 0.022236  [ 2800/12360] <-> largest in grain: 0.023965\n",
            "loss: 0.022090  [ 2900/12360] <-> largest in grain: 0.024273\n",
            "loss: 0.022254  [ 3000/12360] <-> largest in grain: 0.024653\n",
            "loss: 0.022140  [ 3100/12360] <-> largest in grain: 0.023793\n",
            "loss: 0.022147  [ 3200/12360] <-> largest in grain: 0.024154\n",
            "loss: 0.022092  [ 3300/12360] <-> largest in grain: 0.023793\n",
            "loss: 0.022144  [ 3400/12360] <-> largest in grain: 0.023994\n",
            "loss: 0.022252  [ 3500/12360] <-> largest in grain: 0.024684\n",
            "loss: 0.022166  [ 3600/12360] <-> largest in grain: 0.023982\n",
            "loss: 0.022118  [ 3700/12360] <-> largest in grain: 0.024698\n",
            "loss: 0.022165  [ 3800/12360] <-> largest in grain: 0.024367\n",
            "loss: 0.022143  [ 3900/12360] <-> largest in grain: 0.024136\n",
            "loss: 0.022259  [ 4000/12360] <-> largest in grain: 0.024924\n",
            "loss: 0.022161  [ 4100/12360] <-> largest in grain: 0.024672\n",
            "loss: 0.022099  [ 4200/12360] <-> largest in grain: 0.024550\n",
            "loss: 0.022153  [ 4300/12360] <-> largest in grain: 0.024442\n",
            "loss: 0.022239  [ 4400/12360] <-> largest in grain: 0.023891\n",
            "loss: 0.022084  [ 4500/12360] <-> largest in grain: 0.024681\n",
            "loss: 0.022144  [ 4600/12360] <-> largest in grain: 0.024194\n",
            "loss: 0.022114  [ 4700/12360] <-> largest in grain: 0.023811\n",
            "loss: 0.022123  [ 4800/12360] <-> largest in grain: 0.023849\n",
            "loss: 0.022089  [ 4900/12360] <-> largest in grain: 0.023670\n",
            "loss: 0.021985  [ 5000/12360] <-> largest in grain: 0.024337\n",
            "loss: 0.021992  [ 5100/12360] <-> largest in grain: 0.023911\n",
            "loss: 0.022220  [ 5200/12360] <-> largest in grain: 0.024590\n",
            "loss: 0.022065  [ 5300/12360] <-> largest in grain: 0.024806\n",
            "loss: 0.021995  [ 5400/12360] <-> largest in grain: 0.024067\n",
            "loss: 0.022240  [ 5500/12360] <-> largest in grain: 0.024299\n",
            "loss: 0.021959  [ 5600/12360] <-> largest in grain: 0.024062\n",
            "loss: 0.021996  [ 5700/12360] <-> largest in grain: 0.024403\n",
            "loss: 0.022073  [ 5800/12360] <-> largest in grain: 0.023772\n",
            "loss: 0.021983  [ 5900/12360] <-> largest in grain: 0.024506\n",
            "loss: 0.021818  [ 6000/12360] <-> largest in grain: 0.023258\n",
            "loss: 0.022216  [ 6100/12360] <-> largest in grain: 0.023887\n",
            "loss: 0.022169  [ 6200/12360] <-> largest in grain: 0.024292\n",
            "loss: 0.021847  [ 6300/12360] <-> largest in grain: 0.023894\n",
            "loss: 0.021927  [ 6400/12360] <-> largest in grain: 0.023925\n",
            "loss: 0.022016  [ 6500/12360] <-> largest in grain: 0.024556\n",
            "loss: 0.021869  [ 6600/12360] <-> largest in grain: 0.023442\n",
            "loss: 0.021986  [ 6700/12360] <-> largest in grain: 0.024686\n",
            "loss: 0.022027  [ 6800/12360] <-> largest in grain: 0.024306\n",
            "loss: 0.021940  [ 6900/12360] <-> largest in grain: 0.024367\n",
            "loss: 0.021912  [ 7000/12360] <-> largest in grain: 0.024004\n",
            "loss: 0.021949  [ 7100/12360] <-> largest in grain: 0.024880\n",
            "loss: 0.021884  [ 7200/12360] <-> largest in grain: 0.023991\n",
            "loss: 0.021874  [ 7300/12360] <-> largest in grain: 0.023219\n",
            "loss: 0.021952  [ 7400/12360] <-> largest in grain: 0.023833\n",
            "loss: 0.021924  [ 7500/12360] <-> largest in grain: 0.024657\n",
            "loss: 0.021914  [ 7600/12360] <-> largest in grain: 0.024290\n",
            "loss: 0.021923  [ 7700/12360] <-> largest in grain: 0.024630\n",
            "loss: 0.021962  [ 7800/12360] <-> largest in grain: 0.024471\n",
            "loss: 0.021943  [ 7900/12360] <-> largest in grain: 0.023932\n",
            "loss: 0.021925  [ 8000/12360] <-> largest in grain: 0.024681\n",
            "loss: 0.022002  [ 8100/12360] <-> largest in grain: 0.024261\n",
            "loss: 0.021847  [ 8200/12360] <-> largest in grain: 0.023539\n",
            "loss: 0.021857  [ 8300/12360] <-> largest in grain: 0.024226\n",
            "loss: 0.021707  [ 8400/12360] <-> largest in grain: 0.024273\n",
            "loss: 0.022044  [ 8500/12360] <-> largest in grain: 0.024178\n",
            "loss: 0.021997  [ 8600/12360] <-> largest in grain: 0.024363\n",
            "loss: 0.021953  [ 8700/12360] <-> largest in grain: 0.023879\n",
            "loss: 0.021957  [ 8800/12360] <-> largest in grain: 0.023600\n",
            "loss: 0.021832  [ 8900/12360] <-> largest in grain: 0.024337\n",
            "loss: 0.021877  [ 9000/12360] <-> largest in grain: 0.023778\n",
            "loss: 0.021836  [ 9100/12360] <-> largest in grain: 0.023753\n",
            "loss: 0.021719  [ 9200/12360] <-> largest in grain: 0.023224\n",
            "loss: 0.021938  [ 9300/12360] <-> largest in grain: 0.023945\n",
            "loss: 0.021845  [ 9400/12360] <-> largest in grain: 0.023448\n",
            "loss: 0.021869  [ 9500/12360] <-> largest in grain: 0.024693\n",
            "loss: 0.021742  [ 9600/12360] <-> largest in grain: 0.023521\n",
            "loss: 0.021806  [ 9700/12360] <-> largest in grain: 0.023797\n",
            "loss: 0.021840  [ 9800/12360] <-> largest in grain: 0.024523\n",
            "loss: 0.021863  [ 9900/12360] <-> largest in grain: 0.023948\n",
            "loss: 0.021924  [10000/12360] <-> largest in grain: 0.024236\n",
            "loss: 0.021952  [10100/12360] <-> largest in grain: 0.023581\n",
            "loss: 0.021812  [10200/12360] <-> largest in grain: 0.023606\n",
            "loss: 0.021774  [10300/12360] <-> largest in grain: 0.023524\n",
            "loss: 0.021908  [10400/12360] <-> largest in grain: 0.024357\n",
            "loss: 0.021845  [10500/12360] <-> largest in grain: 0.023946\n",
            "loss: 0.021741  [10600/12360] <-> largest in grain: 0.023806\n",
            "loss: 0.021899  [10700/12360] <-> largest in grain: 0.023537\n",
            "loss: 0.021662  [10800/12360] <-> largest in grain: 0.023517\n",
            "loss: 0.021889  [10900/12360] <-> largest in grain: 0.024080\n",
            "loss: 0.021753  [11000/12360] <-> largest in grain: 0.024370\n",
            "loss: 0.021818  [11100/12360] <-> largest in grain: 0.024233\n",
            "loss: 0.021832  [11200/12360] <-> largest in grain: 0.023817\n",
            "loss: 0.021812  [11300/12360] <-> largest in grain: 0.023393\n",
            "loss: 0.021750  [11400/12360] <-> largest in grain: 0.023895\n",
            "loss: 0.021813  [11500/12360] <-> largest in grain: 0.024560\n",
            "loss: 0.021662  [11600/12360] <-> largest in grain: 0.023442\n",
            "loss: 0.021735  [11700/12360] <-> largest in grain: 0.024173\n",
            "loss: 0.021726  [11800/12360] <-> largest in grain: 0.024570\n",
            "loss: 0.021540  [11900/12360] <-> largest in grain: 0.023800\n",
            "loss: 0.021655  [12000/12360] <-> largest in grain: 0.023344\n",
            "loss: 0.021589  [12100/12360] <-> largest in grain: 0.024079\n",
            "loss: 0.021765  [12200/12360] <-> largest in grain: 0.024281\n",
            "loss: 0.021658  [12300/12360] <-> largest in grain: 0.023955\n",
            "Batch size warning! [torch.Size([56, 11, 64])]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.000196  [    0/12360] <-> largest in grain: 0.019597\n",
            "loss: 0.021678  [  100/12360] <-> largest in grain: 0.023060\n",
            "loss: 0.021446  [  200/12360] <-> largest in grain: 0.023366\n",
            "loss: 0.021514  [  300/12360] <-> largest in grain: 0.023804\n",
            "loss: 0.021541  [  400/12360] <-> largest in grain: 0.023262\n",
            "loss: 0.021590  [  500/12360] <-> largest in grain: 0.023905\n",
            "loss: 0.021347  [  600/12360] <-> largest in grain: 0.023417\n",
            "loss: 0.021294  [  700/12360] <-> largest in grain: 0.023043\n",
            "loss: 0.021514  [  800/12360] <-> largest in grain: 0.023970\n",
            "loss: 0.021535  [  900/12360] <-> largest in grain: 0.023491\n",
            "loss: 0.021366  [ 1000/12360] <-> largest in grain: 0.023647\n",
            "loss: 0.021640  [ 1100/12360] <-> largest in grain: 0.023556\n",
            "loss: 0.021551  [ 1200/12360] <-> largest in grain: 0.023321\n",
            "loss: 0.021545  [ 1300/12360] <-> largest in grain: 0.023330\n",
            "loss: 0.021526  [ 1400/12360] <-> largest in grain: 0.023388\n",
            "loss: 0.021370  [ 1500/12360] <-> largest in grain: 0.023299\n",
            "loss: 0.021377  [ 1600/12360] <-> largest in grain: 0.023503\n",
            "loss: 0.021484  [ 1700/12360] <-> largest in grain: 0.023438\n",
            "loss: 0.021412  [ 1800/12360] <-> largest in grain: 0.023318\n",
            "loss: 0.021544  [ 1900/12360] <-> largest in grain: 0.023774\n",
            "loss: 0.021581  [ 2000/12360] <-> largest in grain: 0.023627\n",
            "loss: 0.021492  [ 2100/12360] <-> largest in grain: 0.023307\n",
            "loss: 0.021430  [ 2200/12360] <-> largest in grain: 0.023240\n",
            "loss: 0.021500  [ 2300/12360] <-> largest in grain: 0.023146\n",
            "loss: 0.021458  [ 2400/12360] <-> largest in grain: 0.023275\n",
            "loss: 0.021448  [ 2500/12360] <-> largest in grain: 0.023931\n",
            "loss: 0.021413  [ 2600/12360] <-> largest in grain: 0.023859\n",
            "loss: 0.021538  [ 2700/12360] <-> largest in grain: 0.023275\n",
            "loss: 0.021436  [ 2800/12360] <-> largest in grain: 0.023516\n",
            "loss: 0.021471  [ 2900/12360] <-> largest in grain: 0.023323\n",
            "loss: 0.021371  [ 3000/12360] <-> largest in grain: 0.024776\n",
            "loss: 0.021451  [ 3100/12360] <-> largest in grain: 0.023341\n",
            "loss: 0.021427  [ 3200/12360] <-> largest in grain: 0.023758\n",
            "loss: 0.021516  [ 3300/12360] <-> largest in grain: 0.022883\n",
            "loss: 0.021401  [ 3400/12360] <-> largest in grain: 0.023340\n",
            "loss: 0.021514  [ 3500/12360] <-> largest in grain: 0.023638\n",
            "loss: 0.021334  [ 3600/12360] <-> largest in grain: 0.022996\n",
            "loss: 0.021363  [ 3700/12360] <-> largest in grain: 0.023924\n",
            "loss: 0.021394  [ 3800/12360] <-> largest in grain: 0.023074\n",
            "loss: 0.021374  [ 3900/12360] <-> largest in grain: 0.023350\n",
            "loss: 0.021485  [ 4000/12360] <-> largest in grain: 0.023781\n",
            "loss: 0.021331  [ 4100/12360] <-> largest in grain: 0.023058\n",
            "loss: 0.021363  [ 4200/12360] <-> largest in grain: 0.024415\n",
            "loss: 0.021391  [ 4300/12360] <-> largest in grain: 0.023245\n",
            "loss: 0.021482  [ 4400/12360] <-> largest in grain: 0.023645\n",
            "loss: 0.021519  [ 4500/12360] <-> largest in grain: 0.023267\n",
            "loss: 0.021288  [ 4600/12360] <-> largest in grain: 0.023138\n",
            "loss: 0.021436  [ 4700/12360] <-> largest in grain: 0.023270\n",
            "loss: 0.021468  [ 4800/12360] <-> largest in grain: 0.023444\n",
            "loss: 0.021330  [ 4900/12360] <-> largest in grain: 0.022925\n",
            "loss: 0.021467  [ 5000/12360] <-> largest in grain: 0.023793\n",
            "loss: 0.021547  [ 5100/12360] <-> largest in grain: 0.023510\n",
            "loss: 0.021405  [ 5200/12360] <-> largest in grain: 0.022844\n",
            "loss: 0.021430  [ 5300/12360] <-> largest in grain: 0.023486\n",
            "loss: 0.021514  [ 5400/12360] <-> largest in grain: 0.023514\n",
            "loss: 0.021430  [ 5500/12360] <-> largest in grain: 0.023110\n",
            "loss: 0.021384  [ 5600/12360] <-> largest in grain: 0.023317\n",
            "loss: 0.021401  [ 5700/12360] <-> largest in grain: 0.023418\n",
            "loss: 0.021386  [ 5800/12360] <-> largest in grain: 0.023108\n",
            "loss: 0.021469  [ 5900/12360] <-> largest in grain: 0.022981\n",
            "loss: 0.021335  [ 6000/12360] <-> largest in grain: 0.023137\n",
            "loss: 0.021510  [ 6100/12360] <-> largest in grain: 0.023033\n",
            "loss: 0.021244  [ 6200/12360] <-> largest in grain: 0.023343\n",
            "loss: 0.021455  [ 6300/12360] <-> largest in grain: 0.023686\n",
            "loss: 0.021290  [ 6400/12360] <-> largest in grain: 0.023253\n",
            "loss: 0.021259  [ 6500/12360] <-> largest in grain: 0.023179\n",
            "loss: 0.021420  [ 6600/12360] <-> largest in grain: 0.023338\n",
            "loss: 0.021403  [ 6700/12360] <-> largest in grain: 0.023884\n",
            "loss: 0.021308  [ 6800/12360] <-> largest in grain: 0.023091\n",
            "loss: 0.021405  [ 6900/12360] <-> largest in grain: 0.023615\n",
            "loss: 0.021314  [ 7000/12360] <-> largest in grain: 0.022886\n",
            "loss: 0.021282  [ 7100/12360] <-> largest in grain: 0.023607\n",
            "loss: 0.021187  [ 7200/12360] <-> largest in grain: 0.022850\n",
            "loss: 0.021332  [ 7300/12360] <-> largest in grain: 0.023406\n",
            "loss: 0.021346  [ 7400/12360] <-> largest in grain: 0.023032\n",
            "loss: 0.021312  [ 7500/12360] <-> largest in grain: 0.023203\n",
            "loss: 0.021293  [ 7600/12360] <-> largest in grain: 0.023372\n",
            "loss: 0.021295  [ 7700/12360] <-> largest in grain: 0.023352\n",
            "loss: 0.021208  [ 7800/12360] <-> largest in grain: 0.023551\n",
            "loss: 0.021468  [ 7900/12360] <-> largest in grain: 0.023480\n",
            "loss: 0.021321  [ 8000/12360] <-> largest in grain: 0.023328\n",
            "loss: 0.021422  [ 8100/12360] <-> largest in grain: 0.023245\n",
            "loss: 0.021305  [ 8200/12360] <-> largest in grain: 0.024189\n",
            "loss: 0.021209  [ 8300/12360] <-> largest in grain: 0.023151\n",
            "loss: 0.021353  [ 8400/12360] <-> largest in grain: 0.023632\n",
            "loss: 0.021263  [ 8500/12360] <-> largest in grain: 0.023377\n",
            "loss: 0.021132  [ 8600/12360] <-> largest in grain: 0.023008\n",
            "loss: 0.021275  [ 8700/12360] <-> largest in grain: 0.023570\n",
            "loss: 0.021246  [ 8800/12360] <-> largest in grain: 0.024236\n",
            "loss: 0.021376  [ 8900/12360] <-> largest in grain: 0.023705\n",
            "loss: 0.021250  [ 9000/12360] <-> largest in grain: 0.023812\n",
            "loss: 0.021369  [ 9100/12360] <-> largest in grain: 0.023058\n",
            "loss: 0.021146  [ 9200/12360] <-> largest in grain: 0.023228\n",
            "loss: 0.021186  [ 9300/12360] <-> largest in grain: 0.022940\n",
            "loss: 0.021268  [ 9400/12360] <-> largest in grain: 0.023737\n",
            "loss: 0.021274  [ 9500/12360] <-> largest in grain: 0.023644\n",
            "loss: 0.021264  [ 9600/12360] <-> largest in grain: 0.023133\n",
            "loss: 0.021230  [ 9700/12360] <-> largest in grain: 0.023215\n",
            "loss: 0.021329  [ 9800/12360] <-> largest in grain: 0.023116\n",
            "loss: 0.021139  [ 9900/12360] <-> largest in grain: 0.022761\n",
            "loss: 0.021280  [10000/12360] <-> largest in grain: 0.023783\n",
            "loss: 0.021265  [10100/12360] <-> largest in grain: 0.022979\n",
            "loss: 0.021265  [10200/12360] <-> largest in grain: 0.023554\n",
            "loss: 0.021142  [10300/12360] <-> largest in grain: 0.023153\n",
            "loss: 0.021101  [10400/12360] <-> largest in grain: 0.022739\n",
            "loss: 0.021138  [10500/12360] <-> largest in grain: 0.023114\n",
            "loss: 0.021083  [10600/12360] <-> largest in grain: 0.023105\n",
            "loss: 0.021255  [10700/12360] <-> largest in grain: 0.022992\n",
            "loss: 0.021260  [10800/12360] <-> largest in grain: 0.024088\n",
            "loss: 0.021326  [10900/12360] <-> largest in grain: 0.024066\n",
            "loss: 0.021321  [11000/12360] <-> largest in grain: 0.023923\n",
            "loss: 0.021095  [11100/12360] <-> largest in grain: 0.022899\n",
            "loss: 0.021088  [11200/12360] <-> largest in grain: 0.023182\n",
            "loss: 0.021050  [11300/12360] <-> largest in grain: 0.022896\n",
            "loss: 0.021144  [11400/12360] <-> largest in grain: 0.023048\n",
            "loss: 0.021275  [11500/12360] <-> largest in grain: 0.023003\n",
            "loss: 0.021038  [11600/12360] <-> largest in grain: 0.022819\n",
            "loss: 0.021226  [11700/12360] <-> largest in grain: 0.023766\n",
            "loss: 0.021302  [11800/12360] <-> largest in grain: 0.023942\n",
            "loss: 0.021211  [11900/12360] <-> largest in grain: 0.023948\n",
            "loss: 0.021164  [12000/12360] <-> largest in grain: 0.023495\n",
            "loss: 0.021151  [12100/12360] <-> largest in grain: 0.022920\n",
            "loss: 0.021121  [12200/12360] <-> largest in grain: 0.023141\n",
            "loss: 0.021232  [12300/12360] <-> largest in grain: 0.023119\n",
            "Batch size warning! [torch.Size([56, 11, 64])]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.000221  [    0/12360] <-> largest in grain: 0.022092\n",
            "loss: 0.021292  [  100/12360] <-> largest in grain: 0.023225\n",
            "loss: 0.021124  [  200/12360] <-> largest in grain: 0.023137\n",
            "loss: 0.021107  [  300/12360] <-> largest in grain: 0.023605\n",
            "loss: 0.021209  [  400/12360] <-> largest in grain: 0.022817\n",
            "loss: 0.021138  [  500/12360] <-> largest in grain: 0.023491\n",
            "loss: 0.021279  [  600/12360] <-> largest in grain: 0.022680\n",
            "loss: 0.021166  [  700/12360] <-> largest in grain: 0.023241\n",
            "loss: 0.021143  [  800/12360] <-> largest in grain: 0.023041\n",
            "loss: 0.021207  [  900/12360] <-> largest in grain: 0.023162\n",
            "loss: 0.021221  [ 1000/12360] <-> largest in grain: 0.023831\n",
            "loss: 0.021091  [ 1100/12360] <-> largest in grain: 0.022833\n",
            "loss: 0.021174  [ 1200/12360] <-> largest in grain: 0.022772\n",
            "loss: 0.021167  [ 1300/12360] <-> largest in grain: 0.022717\n",
            "loss: 0.021129  [ 1400/12360] <-> largest in grain: 0.023275\n",
            "loss: 0.021241  [ 1500/12360] <-> largest in grain: 0.023279\n",
            "loss: 0.021215  [ 1600/12360] <-> largest in grain: 0.023371\n",
            "loss: 0.021079  [ 1700/12360] <-> largest in grain: 0.023123\n",
            "loss: 0.021183  [ 1800/12360] <-> largest in grain: 0.023109\n",
            "loss: 0.021086  [ 1900/12360] <-> largest in grain: 0.023475\n",
            "loss: 0.021065  [ 2000/12360] <-> largest in grain: 0.022898\n",
            "loss: 0.021125  [ 2100/12360] <-> largest in grain: 0.022832\n",
            "loss: 0.021053  [ 2200/12360] <-> largest in grain: 0.023231\n",
            "loss: 0.021133  [ 2300/12360] <-> largest in grain: 0.023427\n",
            "loss: 0.021151  [ 2400/12360] <-> largest in grain: 0.022903\n",
            "loss: 0.021053  [ 2500/12360] <-> largest in grain: 0.023251\n",
            "loss: 0.021131  [ 2600/12360] <-> largest in grain: 0.022775\n",
            "loss: 0.021179  [ 2700/12360] <-> largest in grain: 0.022904\n",
            "loss: 0.021102  [ 2800/12360] <-> largest in grain: 0.022990\n",
            "loss: 0.021010  [ 2900/12360] <-> largest in grain: 0.023156\n",
            "loss: 0.021048  [ 3000/12360] <-> largest in grain: 0.023283\n",
            "loss: 0.021198  [ 3100/12360] <-> largest in grain: 0.023266\n",
            "loss: 0.021108  [ 3200/12360] <-> largest in grain: 0.022956\n",
            "loss: 0.020895  [ 3300/12360] <-> largest in grain: 0.022852\n",
            "loss: 0.021060  [ 3400/12360] <-> largest in grain: 0.023362\n",
            "loss: 0.020987  [ 3500/12360] <-> largest in grain: 0.023468\n",
            "loss: 0.021025  [ 3600/12360] <-> largest in grain: 0.023511\n",
            "loss: 0.021103  [ 3700/12360] <-> largest in grain: 0.023086\n",
            "loss: 0.021082  [ 3800/12360] <-> largest in grain: 0.023225\n",
            "loss: 0.021157  [ 3900/12360] <-> largest in grain: 0.023546\n",
            "loss: 0.021218  [ 4000/12360] <-> largest in grain: 0.022749\n",
            "loss: 0.021137  [ 4100/12360] <-> largest in grain: 0.023234\n",
            "loss: 0.021137  [ 4200/12360] <-> largest in grain: 0.023442\n",
            "loss: 0.021036  [ 4300/12360] <-> largest in grain: 0.023521\n",
            "loss: 0.020907  [ 4400/12360] <-> largest in grain: 0.022706\n",
            "loss: 0.021114  [ 4500/12360] <-> largest in grain: 0.022832\n",
            "loss: 0.020873  [ 4600/12360] <-> largest in grain: 0.022872\n",
            "loss: 0.021075  [ 4700/12360] <-> largest in grain: 0.023278\n",
            "loss: 0.021007  [ 4800/12360] <-> largest in grain: 0.022935\n",
            "loss: 0.021059  [ 4900/12360] <-> largest in grain: 0.022581\n",
            "loss: 0.021103  [ 5000/12360] <-> largest in grain: 0.023433\n",
            "loss: 0.021165  [ 5100/12360] <-> largest in grain: 0.022915\n",
            "loss: 0.021095  [ 5200/12360] <-> largest in grain: 0.022551\n",
            "loss: 0.021130  [ 5300/12360] <-> largest in grain: 0.022950\n",
            "loss: 0.021130  [ 5400/12360] <-> largest in grain: 0.023371\n",
            "loss: 0.020943  [ 5500/12360] <-> largest in grain: 0.022435\n",
            "loss: 0.021151  [ 5600/12360] <-> largest in grain: 0.023230\n",
            "loss: 0.021038  [ 5700/12360] <-> largest in grain: 0.022881\n",
            "loss: 0.021064  [ 5800/12360] <-> largest in grain: 0.023290\n",
            "loss: 0.021092  [ 5900/12360] <-> largest in grain: 0.022489\n",
            "loss: 0.020904  [ 6000/12360] <-> largest in grain: 0.022884\n",
            "loss: 0.021106  [ 6100/12360] <-> largest in grain: 0.022989\n",
            "loss: 0.021039  [ 6200/12360] <-> largest in grain: 0.022964\n",
            "loss: 0.021100  [ 6300/12360] <-> largest in grain: 0.022625\n",
            "loss: 0.021040  [ 6400/12360] <-> largest in grain: 0.022683\n",
            "loss: 0.021051  [ 6500/12360] <-> largest in grain: 0.023293\n",
            "loss: 0.021030  [ 6600/12360] <-> largest in grain: 0.022968\n",
            "loss: 0.021026  [ 6700/12360] <-> largest in grain: 0.023135\n",
            "loss: 0.021037  [ 6800/12360] <-> largest in grain: 0.023505\n",
            "loss: 0.021078  [ 6900/12360] <-> largest in grain: 0.022816\n",
            "loss: 0.020988  [ 7000/12360] <-> largest in grain: 0.022817\n",
            "loss: 0.021017  [ 7100/12360] <-> largest in grain: 0.023317\n",
            "loss: 0.020934  [ 7200/12360] <-> largest in grain: 0.023376\n",
            "loss: 0.021047  [ 7300/12360] <-> largest in grain: 0.023128\n",
            "loss: 0.021018  [ 7400/12360] <-> largest in grain: 0.023044\n",
            "loss: 0.021015  [ 7500/12360] <-> largest in grain: 0.023566\n",
            "loss: 0.020953  [ 7600/12360] <-> largest in grain: 0.023207\n",
            "loss: 0.020836  [ 7700/12360] <-> largest in grain: 0.022559\n",
            "loss: 0.021017  [ 7800/12360] <-> largest in grain: 0.022800\n",
            "loss: 0.021037  [ 7900/12360] <-> largest in grain: 0.023071\n",
            "loss: 0.020921  [ 8000/12360] <-> largest in grain: 0.023101\n",
            "loss: 0.021098  [ 8100/12360] <-> largest in grain: 0.022930\n",
            "loss: 0.021124  [ 8200/12360] <-> largest in grain: 0.023734\n",
            "loss: 0.020926  [ 8300/12360] <-> largest in grain: 0.022665\n",
            "loss: 0.020929  [ 8400/12360] <-> largest in grain: 0.022901\n",
            "loss: 0.021035  [ 8500/12360] <-> largest in grain: 0.022617\n",
            "loss: 0.021015  [ 8600/12360] <-> largest in grain: 0.022632\n",
            "loss: 0.021016  [ 8700/12360] <-> largest in grain: 0.023040\n",
            "loss: 0.020944  [ 8800/12360] <-> largest in grain: 0.023205\n",
            "loss: 0.020975  [ 8900/12360] <-> largest in grain: 0.022551\n",
            "loss: 0.020995  [ 9000/12360] <-> largest in grain: 0.023188\n",
            "loss: 0.021022  [ 9100/12360] <-> largest in grain: 0.022619\n",
            "loss: 0.021074  [ 9200/12360] <-> largest in grain: 0.022651\n",
            "loss: 0.021144  [ 9300/12360] <-> largest in grain: 0.023326\n",
            "loss: 0.020731  [ 9400/12360] <-> largest in grain: 0.023241\n",
            "loss: 0.020901  [ 9500/12360] <-> largest in grain: 0.022513\n",
            "loss: 0.021064  [ 9600/12360] <-> largest in grain: 0.023199\n",
            "loss: 0.021048  [ 9700/12360] <-> largest in grain: 0.023402\n",
            "loss: 0.020959  [ 9800/12360] <-> largest in grain: 0.022744\n",
            "loss: 0.020996  [ 9900/12360] <-> largest in grain: 0.022846\n",
            "loss: 0.021020  [10000/12360] <-> largest in grain: 0.023004\n",
            "loss: 0.021005  [10100/12360] <-> largest in grain: 0.023401\n",
            "loss: 0.021064  [10200/12360] <-> largest in grain: 0.022971\n",
            "loss: 0.020915  [10300/12360] <-> largest in grain: 0.022966\n",
            "loss: 0.020868  [10400/12360] <-> largest in grain: 0.023421\n",
            "loss: 0.021014  [10500/12360] <-> largest in grain: 0.023561\n",
            "loss: 0.020947  [10600/12360] <-> largest in grain: 0.022666\n",
            "loss: 0.020919  [10700/12360] <-> largest in grain: 0.023004\n",
            "loss: 0.020864  [10800/12360] <-> largest in grain: 0.022666\n",
            "loss: 0.020977  [10900/12360] <-> largest in grain: 0.022968\n",
            "loss: 0.020904  [11000/12360] <-> largest in grain: 0.023045\n",
            "loss: 0.020935  [11100/12360] <-> largest in grain: 0.022908\n",
            "loss: 0.020771  [11200/12360] <-> largest in grain: 0.022473\n",
            "loss: 0.021097  [11300/12360] <-> largest in grain: 0.023078\n",
            "loss: 0.020866  [11400/12360] <-> largest in grain: 0.022502\n",
            "loss: 0.020886  [11500/12360] <-> largest in grain: 0.023269\n",
            "loss: 0.020924  [11600/12360] <-> largest in grain: 0.023129\n",
            "loss: 0.020901  [11700/12360] <-> largest in grain: 0.023664\n",
            "loss: 0.020998  [11800/12360] <-> largest in grain: 0.023842\n",
            "loss: 0.020876  [11900/12360] <-> largest in grain: 0.022914\n",
            "loss: 0.020843  [12000/12360] <-> largest in grain: 0.022748\n",
            "loss: 0.020838  [12100/12360] <-> largest in grain: 0.022949\n",
            "loss: 0.020976  [12200/12360] <-> largest in grain: 0.022836\n",
            "loss: 0.020898  [12300/12360] <-> largest in grain: 0.023020\n",
            "Batch size warning! [torch.Size([56, 11, 64])]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.000218  [    0/12360] <-> largest in grain: 0.021837\n",
            "loss: 0.020839  [  100/12360] <-> largest in grain: 0.024199\n",
            "loss: 0.020882  [  200/12360] <-> largest in grain: 0.023080\n",
            "loss: 0.020843  [  300/12360] <-> largest in grain: 0.022864\n",
            "loss: 0.020933  [  400/12360] <-> largest in grain: 0.023442\n",
            "loss: 0.020913  [  500/12360] <-> largest in grain: 0.023174\n",
            "loss: 0.020871  [  600/12360] <-> largest in grain: 0.022315\n",
            "loss: 0.020794  [  700/12360] <-> largest in grain: 0.022955\n",
            "loss: 0.020834  [  800/12360] <-> largest in grain: 0.023072\n",
            "loss: 0.020956  [  900/12360] <-> largest in grain: 0.022792\n",
            "loss: 0.020945  [ 1000/12360] <-> largest in grain: 0.022728\n",
            "loss: 0.020786  [ 1100/12360] <-> largest in grain: 0.022869\n",
            "loss: 0.020862  [ 1200/12360] <-> largest in grain: 0.022682\n",
            "loss: 0.020871  [ 1300/12360] <-> largest in grain: 0.022991\n",
            "loss: 0.020934  [ 1400/12360] <-> largest in grain: 0.023308\n",
            "loss: 0.020712  [ 1500/12360] <-> largest in grain: 0.022666\n",
            "loss: 0.020884  [ 1600/12360] <-> largest in grain: 0.023748\n",
            "loss: 0.020746  [ 1700/12360] <-> largest in grain: 0.023543\n",
            "loss: 0.020831  [ 1800/12360] <-> largest in grain: 0.023161\n",
            "loss: 0.020949  [ 1900/12360] <-> largest in grain: 0.022980\n",
            "loss: 0.020897  [ 2000/12360] <-> largest in grain: 0.022341\n",
            "loss: 0.020818  [ 2100/12360] <-> largest in grain: 0.022637\n",
            "loss: 0.020755  [ 2200/12360] <-> largest in grain: 0.023259\n",
            "loss: 0.020840  [ 2300/12360] <-> largest in grain: 0.022788\n",
            "loss: 0.020890  [ 2400/12360] <-> largest in grain: 0.023758\n",
            "loss: 0.020790  [ 2500/12360] <-> largest in grain: 0.022555\n",
            "loss: 0.020941  [ 2600/12360] <-> largest in grain: 0.023078\n",
            "loss: 0.020753  [ 2700/12360] <-> largest in grain: 0.022506\n",
            "loss: 0.020810  [ 2800/12360] <-> largest in grain: 0.023040\n",
            "loss: 0.020914  [ 2900/12360] <-> largest in grain: 0.023105\n",
            "loss: 0.020944  [ 3000/12360] <-> largest in grain: 0.023211\n",
            "loss: 0.020761  [ 3100/12360] <-> largest in grain: 0.023008\n",
            "loss: 0.020806  [ 3200/12360] <-> largest in grain: 0.022775\n",
            "loss: 0.020822  [ 3300/12360] <-> largest in grain: 0.022681\n",
            "loss: 0.020748  [ 3400/12360] <-> largest in grain: 0.023301\n",
            "loss: 0.020673  [ 3500/12360] <-> largest in grain: 0.022951\n",
            "loss: 0.020966  [ 3600/12360] <-> largest in grain: 0.023115\n",
            "loss: 0.020806  [ 3700/12360] <-> largest in grain: 0.023477\n",
            "loss: 0.020750  [ 3800/12360] <-> largest in grain: 0.022869\n",
            "loss: 0.020736  [ 3900/12360] <-> largest in grain: 0.022557\n",
            "loss: 0.020812  [ 4000/12360] <-> largest in grain: 0.022840\n",
            "loss: 0.020831  [ 4100/12360] <-> largest in grain: 0.023176\n",
            "loss: 0.020808  [ 4200/12360] <-> largest in grain: 0.022647\n",
            "loss: 0.020770  [ 4300/12360] <-> largest in grain: 0.022942\n",
            "loss: 0.020792  [ 4400/12360] <-> largest in grain: 0.022873\n",
            "loss: 0.020792  [ 4500/12360] <-> largest in grain: 0.022579\n",
            "loss: 0.020748  [ 4600/12360] <-> largest in grain: 0.022406\n",
            "loss: 0.020763  [ 4700/12360] <-> largest in grain: 0.022632\n",
            "loss: 0.020883  [ 4800/12360] <-> largest in grain: 0.022754\n",
            "loss: 0.020723  [ 4900/12360] <-> largest in grain: 0.022293\n",
            "loss: 0.020827  [ 5000/12360] <-> largest in grain: 0.023077\n",
            "loss: 0.020884  [ 5100/12360] <-> largest in grain: 0.022691\n",
            "loss: 0.020843  [ 5200/12360] <-> largest in grain: 0.023519\n",
            "loss: 0.020741  [ 5300/12360] <-> largest in grain: 0.022474\n",
            "loss: 0.020635  [ 5400/12360] <-> largest in grain: 0.022549\n",
            "loss: 0.020701  [ 5500/12360] <-> largest in grain: 0.022869\n",
            "loss: 0.020847  [ 5600/12360] <-> largest in grain: 0.022525\n",
            "loss: 0.020859  [ 5700/12360] <-> largest in grain: 0.022902\n",
            "loss: 0.020800  [ 5800/12360] <-> largest in grain: 0.022376\n",
            "loss: 0.020848  [ 5900/12360] <-> largest in grain: 0.022378\n",
            "loss: 0.020773  [ 6000/12360] <-> largest in grain: 0.022913\n",
            "loss: 0.020820  [ 6100/12360] <-> largest in grain: 0.022646\n",
            "loss: 0.020678  [ 6200/12360] <-> largest in grain: 0.022412\n",
            "loss: 0.020746  [ 6300/12360] <-> largest in grain: 0.022824\n",
            "loss: 0.020647  [ 6400/12360] <-> largest in grain: 0.023607\n",
            "loss: 0.020875  [ 6500/12360] <-> largest in grain: 0.022963\n",
            "loss: 0.020695  [ 6600/12360] <-> largest in grain: 0.022735\n",
            "loss: 0.020665  [ 6700/12360] <-> largest in grain: 0.023028\n",
            "loss: 0.020842  [ 6800/12360] <-> largest in grain: 0.023581\n",
            "loss: 0.020847  [ 6900/12360] <-> largest in grain: 0.022759\n",
            "loss: 0.020791  [ 7000/12360] <-> largest in grain: 0.022950\n",
            "loss: 0.020802  [ 7100/12360] <-> largest in grain: 0.022703\n",
            "loss: 0.020777  [ 7200/12360] <-> largest in grain: 0.022699\n",
            "loss: 0.020772  [ 7300/12360] <-> largest in grain: 0.022512\n",
            "loss: 0.020737  [ 7400/12360] <-> largest in grain: 0.022248\n",
            "loss: 0.020891  [ 7500/12360] <-> largest in grain: 0.023057\n",
            "loss: 0.020807  [ 7600/12360] <-> largest in grain: 0.022802\n",
            "loss: 0.020722  [ 7700/12360] <-> largest in grain: 0.022156\n",
            "loss: 0.020829  [ 7800/12360] <-> largest in grain: 0.022433\n",
            "loss: 0.020826  [ 7900/12360] <-> largest in grain: 0.023578\n",
            "loss: 0.020792  [ 8000/12360] <-> largest in grain: 0.023676\n",
            "loss: 0.020837  [ 8100/12360] <-> largest in grain: 0.022657\n",
            "loss: 0.020867  [ 8200/12360] <-> largest in grain: 0.023331\n",
            "loss: 0.020874  [ 8300/12360] <-> largest in grain: 0.022503\n",
            "loss: 0.020827  [ 8400/12360] <-> largest in grain: 0.022712\n",
            "loss: 0.020682  [ 8500/12360] <-> largest in grain: 0.022357\n",
            "loss: 0.020857  [ 8600/12360] <-> largest in grain: 0.023304\n",
            "loss: 0.020721  [ 8700/12360] <-> largest in grain: 0.022879\n",
            "loss: 0.020800  [ 8800/12360] <-> largest in grain: 0.023035\n",
            "loss: 0.020566  [ 8900/12360] <-> largest in grain: 0.022159\n",
            "loss: 0.020685  [ 9000/12360] <-> largest in grain: 0.022574\n",
            "loss: 0.020714  [ 9100/12360] <-> largest in grain: 0.022405\n",
            "loss: 0.020708  [ 9200/12360] <-> largest in grain: 0.022842\n",
            "loss: 0.020759  [ 9300/12360] <-> largest in grain: 0.022608\n",
            "loss: 0.020710  [ 9400/12360] <-> largest in grain: 0.022801\n",
            "loss: 0.020678  [ 9500/12360] <-> largest in grain: 0.022656\n",
            "loss: 0.020780  [ 9600/12360] <-> largest in grain: 0.023089\n",
            "loss: 0.020745  [ 9700/12360] <-> largest in grain: 0.023077\n",
            "loss: 0.020750  [ 9800/12360] <-> largest in grain: 0.022620\n",
            "loss: 0.020763  [ 9900/12360] <-> largest in grain: 0.022327\n",
            "loss: 0.020793  [10000/12360] <-> largest in grain: 0.023274\n",
            "loss: 0.020854  [10100/12360] <-> largest in grain: 0.023642\n",
            "loss: 0.020706  [10200/12360] <-> largest in grain: 0.022664\n",
            "loss: 0.020719  [10300/12360] <-> largest in grain: 0.022811\n",
            "loss: 0.020630  [10400/12360] <-> largest in grain: 0.022536\n",
            "loss: 0.020816  [10500/12360] <-> largest in grain: 0.022199\n",
            "loss: 0.020751  [10600/12360] <-> largest in grain: 0.022563\n",
            "loss: 0.020719  [10700/12360] <-> largest in grain: 0.023214\n",
            "loss: 0.020634  [10800/12360] <-> largest in grain: 0.022477\n",
            "loss: 0.020774  [10900/12360] <-> largest in grain: 0.023559\n",
            "loss: 0.020817  [11000/12360] <-> largest in grain: 0.022644\n",
            "loss: 0.020623  [11100/12360] <-> largest in grain: 0.022468\n",
            "loss: 0.020696  [11200/12360] <-> largest in grain: 0.023256\n",
            "loss: 0.020729  [11300/12360] <-> largest in grain: 0.022839\n",
            "loss: 0.020605  [11400/12360] <-> largest in grain: 0.022610\n",
            "loss: 0.020904  [11500/12360] <-> largest in grain: 0.022596\n",
            "loss: 0.020620  [11600/12360] <-> largest in grain: 0.022506\n",
            "loss: 0.020821  [11700/12360] <-> largest in grain: 0.022628\n",
            "loss: 0.020709  [11800/12360] <-> largest in grain: 0.022373\n",
            "loss: 0.020694  [11900/12360] <-> largest in grain: 0.022008\n",
            "loss: 0.020745  [12000/12360] <-> largest in grain: 0.022684\n",
            "loss: 0.020702  [12100/12360] <-> largest in grain: 0.022209\n",
            "loss: 0.020783  [12200/12360] <-> largest in grain: 0.023127\n",
            "loss: 0.020560  [12300/12360] <-> largest in grain: 0.022058\n",
            "Batch size warning! [torch.Size([56, 11, 64])]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.000203  [    0/12360] <-> largest in grain: 0.020333\n",
            "loss: 0.020764  [  100/12360] <-> largest in grain: 0.023115\n",
            "loss: 0.020677  [  200/12360] <-> largest in grain: 0.022588\n",
            "loss: 0.020671  [  300/12360] <-> largest in grain: 0.022850\n",
            "loss: 0.020675  [  400/12360] <-> largest in grain: 0.022410\n",
            "loss: 0.020871  [  500/12360] <-> largest in grain: 0.023021\n",
            "loss: 0.020564  [  600/12360] <-> largest in grain: 0.022586\n",
            "loss: 0.020738  [  700/12360] <-> largest in grain: 0.022721\n",
            "loss: 0.020610  [  800/12360] <-> largest in grain: 0.022566\n",
            "loss: 0.020751  [  900/12360] <-> largest in grain: 0.022431\n",
            "loss: 0.020737  [ 1000/12360] <-> largest in grain: 0.022492\n",
            "loss: 0.020738  [ 1100/12360] <-> largest in grain: 0.023690\n",
            "loss: 0.020593  [ 1200/12360] <-> largest in grain: 0.022441\n",
            "loss: 0.020629  [ 1300/12360] <-> largest in grain: 0.022810\n",
            "loss: 0.020574  [ 1400/12360] <-> largest in grain: 0.022706\n",
            "loss: 0.020643  [ 1500/12360] <-> largest in grain: 0.023005\n",
            "loss: 0.020692  [ 1600/12360] <-> largest in grain: 0.022515\n",
            "loss: 0.020492  [ 1700/12360] <-> largest in grain: 0.022297\n",
            "loss: 0.020734  [ 1800/12360] <-> largest in grain: 0.022946\n",
            "loss: 0.020615  [ 1900/12360] <-> largest in grain: 0.022691\n",
            "loss: 0.020650  [ 2000/12360] <-> largest in grain: 0.022691\n",
            "loss: 0.020761  [ 2100/12360] <-> largest in grain: 0.022368\n",
            "loss: 0.020637  [ 2200/12360] <-> largest in grain: 0.022491\n",
            "loss: 0.020627  [ 2300/12360] <-> largest in grain: 0.022857\n",
            "loss: 0.020720  [ 2400/12360] <-> largest in grain: 0.022371\n",
            "loss: 0.020737  [ 2500/12360] <-> largest in grain: 0.022622\n",
            "loss: 0.020819  [ 2600/12360] <-> largest in grain: 0.022880\n",
            "loss: 0.020786  [ 2700/12360] <-> largest in grain: 0.022620\n",
            "loss: 0.020657  [ 2800/12360] <-> largest in grain: 0.022276\n",
            "loss: 0.020654  [ 2900/12360] <-> largest in grain: 0.022358\n",
            "loss: 0.020631  [ 3000/12360] <-> largest in grain: 0.022849\n",
            "loss: 0.020732  [ 3100/12360] <-> largest in grain: 0.022453\n",
            "loss: 0.020630  [ 3200/12360] <-> largest in grain: 0.022346\n",
            "loss: 0.020738  [ 3300/12360] <-> largest in grain: 0.022894\n",
            "loss: 0.020712  [ 3400/12360] <-> largest in grain: 0.023081\n",
            "loss: 0.020670  [ 3500/12360] <-> largest in grain: 0.022405\n",
            "loss: 0.020588  [ 3600/12360] <-> largest in grain: 0.022654\n",
            "loss: 0.020669  [ 3700/12360] <-> largest in grain: 0.022854\n",
            "loss: 0.020629  [ 3800/12360] <-> largest in grain: 0.022615\n",
            "loss: 0.020521  [ 3900/12360] <-> largest in grain: 0.021937\n",
            "loss: 0.020720  [ 4000/12360] <-> largest in grain: 0.022867\n",
            "loss: 0.020637  [ 4100/12360] <-> largest in grain: 0.022735\n",
            "loss: 0.020620  [ 4200/12360] <-> largest in grain: 0.022766\n",
            "loss: 0.020647  [ 4300/12360] <-> largest in grain: 0.022791\n",
            "loss: 0.020707  [ 4400/12360] <-> largest in grain: 0.022709\n",
            "loss: 0.020605  [ 4500/12360] <-> largest in grain: 0.023017\n",
            "loss: 0.020758  [ 4600/12360] <-> largest in grain: 0.022430\n",
            "loss: 0.020583  [ 4700/12360] <-> largest in grain: 0.022079\n",
            "loss: 0.020623  [ 4800/12360] <-> largest in grain: 0.022568\n",
            "loss: 0.020632  [ 4900/12360] <-> largest in grain: 0.022512\n",
            "loss: 0.020674  [ 5000/12360] <-> largest in grain: 0.022808\n",
            "loss: 0.020573  [ 5100/12360] <-> largest in grain: 0.022576\n",
            "loss: 0.020698  [ 5200/12360] <-> largest in grain: 0.023659\n",
            "loss: 0.020659  [ 5300/12360] <-> largest in grain: 0.022415\n",
            "loss: 0.020735  [ 5400/12360] <-> largest in grain: 0.023710\n",
            "loss: 0.020615  [ 5500/12360] <-> largest in grain: 0.023248\n",
            "loss: 0.020872  [ 5600/12360] <-> largest in grain: 0.022869\n",
            "loss: 0.020652  [ 5700/12360] <-> largest in grain: 0.022351\n",
            "loss: 0.020566  [ 5800/12360] <-> largest in grain: 0.022540\n",
            "loss: 0.020523  [ 5900/12360] <-> largest in grain: 0.022495\n",
            "loss: 0.020640  [ 6000/12360] <-> largest in grain: 0.022846\n",
            "loss: 0.020671  [ 6100/12360] <-> largest in grain: 0.022680\n",
            "loss: 0.020703  [ 6200/12360] <-> largest in grain: 0.023132\n",
            "loss: 0.020523  [ 6300/12360] <-> largest in grain: 0.022342\n",
            "loss: 0.020677  [ 6400/12360] <-> largest in grain: 0.022791\n",
            "loss: 0.020497  [ 6500/12360] <-> largest in grain: 0.023278\n",
            "loss: 0.020599  [ 6600/12360] <-> largest in grain: 0.022244\n",
            "loss: 0.020657  [ 6700/12360] <-> largest in grain: 0.022589\n",
            "loss: 0.020559  [ 6800/12360] <-> largest in grain: 0.022143\n",
            "loss: 0.020635  [ 6900/12360] <-> largest in grain: 0.022290\n",
            "loss: 0.020681  [ 7000/12360] <-> largest in grain: 0.022531\n",
            "loss: 0.020789  [ 7100/12360] <-> largest in grain: 0.022755\n",
            "loss: 0.020688  [ 7200/12360] <-> largest in grain: 0.022351\n",
            "loss: 0.020591  [ 7300/12360] <-> largest in grain: 0.022374\n",
            "loss: 0.020722  [ 7400/12360] <-> largest in grain: 0.022136\n",
            "loss: 0.020653  [ 7500/12360] <-> largest in grain: 0.023408\n",
            "loss: 0.020553  [ 7600/12360] <-> largest in grain: 0.023123\n",
            "loss: 0.020648  [ 7700/12360] <-> largest in grain: 0.022565\n",
            "loss: 0.020673  [ 7800/12360] <-> largest in grain: 0.023290\n",
            "loss: 0.020631  [ 7900/12360] <-> largest in grain: 0.022871\n",
            "loss: 0.020680  [ 8000/12360] <-> largest in grain: 0.023227\n",
            "loss: 0.020655  [ 8100/12360] <-> largest in grain: 0.022774\n",
            "loss: 0.020603  [ 8200/12360] <-> largest in grain: 0.023131\n",
            "loss: 0.020690  [ 8300/12360] <-> largest in grain: 0.022205\n",
            "loss: 0.020823  [ 8400/12360] <-> largest in grain: 0.023039\n",
            "loss: 0.020601  [ 8500/12360] <-> largest in grain: 0.022949\n",
            "loss: 0.020592  [ 8600/12360] <-> largest in grain: 0.022764\n",
            "loss: 0.020518  [ 8700/12360] <-> largest in grain: 0.022659\n",
            "loss: 0.020650  [ 8800/12360] <-> largest in grain: 0.022733\n",
            "loss: 0.020577  [ 8900/12360] <-> largest in grain: 0.021863\n",
            "loss: 0.020559  [ 9000/12360] <-> largest in grain: 0.022739\n",
            "loss: 0.020569  [ 9100/12360] <-> largest in grain: 0.022177\n",
            "loss: 0.020684  [ 9200/12360] <-> largest in grain: 0.022430\n",
            "loss: 0.020547  [ 9300/12360] <-> largest in grain: 0.022488\n",
            "loss: 0.020581  [ 9400/12360] <-> largest in grain: 0.022336\n",
            "loss: 0.020439  [ 9500/12360] <-> largest in grain: 0.022488\n",
            "loss: 0.020631  [ 9600/12360] <-> largest in grain: 0.022713\n",
            "loss: 0.020711  [ 9700/12360] <-> largest in grain: 0.022379\n",
            "loss: 0.020797  [ 9800/12360] <-> largest in grain: 0.022928\n",
            "loss: 0.020584  [ 9900/12360] <-> largest in grain: 0.022607\n",
            "loss: 0.020566  [10000/12360] <-> largest in grain: 0.022004\n",
            "loss: 0.020483  [10100/12360] <-> largest in grain: 0.022622\n",
            "loss: 0.020644  [10200/12360] <-> largest in grain: 0.022151\n",
            "loss: 0.020558  [10300/12360] <-> largest in grain: 0.022525\n",
            "loss: 0.020550  [10400/12360] <-> largest in grain: 0.022335\n",
            "loss: 0.020647  [10500/12360] <-> largest in grain: 0.022483\n",
            "loss: 0.020649  [10600/12360] <-> largest in grain: 0.022823\n",
            "loss: 0.020496  [10700/12360] <-> largest in grain: 0.022395\n",
            "loss: 0.020570  [10800/12360] <-> largest in grain: 0.022481\n",
            "loss: 0.020645  [10900/12360] <-> largest in grain: 0.022695\n",
            "loss: 0.020638  [11000/12360] <-> largest in grain: 0.022240\n",
            "loss: 0.020550  [11100/12360] <-> largest in grain: 0.022948\n",
            "loss: 0.020505  [11200/12360] <-> largest in grain: 0.022450\n",
            "loss: 0.020531  [11300/12360] <-> largest in grain: 0.022195\n",
            "loss: 0.020586  [11400/12360] <-> largest in grain: 0.022803\n",
            "loss: 0.020620  [11500/12360] <-> largest in grain: 0.022617\n",
            "loss: 0.020621  [11600/12360] <-> largest in grain: 0.022932\n",
            "loss: 0.020584  [11700/12360] <-> largest in grain: 0.022299\n",
            "loss: 0.020807  [11800/12360] <-> largest in grain: 0.022528\n",
            "loss: 0.020575  [11900/12360] <-> largest in grain: 0.022378\n",
            "loss: 0.020589  [12000/12360] <-> largest in grain: 0.022173\n",
            "loss: 0.020575  [12100/12360] <-> largest in grain: 0.022593\n",
            "loss: 0.020620  [12200/12360] <-> largest in grain: 0.022489\n",
            "loss: 0.020595  [12300/12360] <-> largest in grain: 0.022449\n",
            "Batch size warning! [torch.Size([56, 11, 64])]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.000210  [    0/12360] <-> largest in grain: 0.021039\n",
            "loss: 0.020573  [  100/12360] <-> largest in grain: 0.022146\n",
            "loss: 0.020658  [  200/12360] <-> largest in grain: 0.022986\n",
            "loss: 0.020496  [  300/12360] <-> largest in grain: 0.022747\n",
            "loss: 0.020447  [  400/12360] <-> largest in grain: 0.022123\n",
            "loss: 0.020585  [  500/12360] <-> largest in grain: 0.022393\n",
            "loss: 0.020680  [  600/12360] <-> largest in grain: 0.022739\n",
            "loss: 0.020552  [  700/12360] <-> largest in grain: 0.021997\n",
            "loss: 0.020508  [  800/12360] <-> largest in grain: 0.021831\n",
            "loss: 0.020474  [  900/12360] <-> largest in grain: 0.022451\n",
            "loss: 0.020493  [ 1000/12360] <-> largest in grain: 0.022779\n",
            "loss: 0.020541  [ 1100/12360] <-> largest in grain: 0.022646\n",
            "loss: 0.020512  [ 1200/12360] <-> largest in grain: 0.022207\n",
            "loss: 0.020607  [ 1300/12360] <-> largest in grain: 0.023221\n",
            "loss: 0.020639  [ 1400/12360] <-> largest in grain: 0.022523\n",
            "loss: 0.020542  [ 1500/12360] <-> largest in grain: 0.022808\n",
            "loss: 0.020531  [ 1600/12360] <-> largest in grain: 0.022341\n",
            "loss: 0.020517  [ 1700/12360] <-> largest in grain: 0.022473\n",
            "loss: 0.020392  [ 1800/12360] <-> largest in grain: 0.022061\n",
            "loss: 0.020391  [ 1900/12360] <-> largest in grain: 0.022576\n",
            "loss: 0.020590  [ 2000/12360] <-> largest in grain: 0.022240\n",
            "loss: 0.020487  [ 2100/12360] <-> largest in grain: 0.022336\n",
            "loss: 0.020510  [ 2200/12360] <-> largest in grain: 0.022316\n",
            "loss: 0.020587  [ 2300/12360] <-> largest in grain: 0.022229\n",
            "loss: 0.020682  [ 2400/12360] <-> largest in grain: 0.022193\n",
            "loss: 0.020525  [ 2500/12360] <-> largest in grain: 0.022737\n",
            "loss: 0.020415  [ 2600/12360] <-> largest in grain: 0.022825\n",
            "loss: 0.020540  [ 2700/12360] <-> largest in grain: 0.022420\n",
            "loss: 0.020610  [ 2800/12360] <-> largest in grain: 0.023276\n",
            "loss: 0.020571  [ 2900/12360] <-> largest in grain: 0.022060\n",
            "loss: 0.020403  [ 3000/12360] <-> largest in grain: 0.022484\n",
            "loss: 0.020376  [ 3100/12360] <-> largest in grain: 0.022528\n",
            "loss: 0.020519  [ 3200/12360] <-> largest in grain: 0.022448\n",
            "loss: 0.020470  [ 3300/12360] <-> largest in grain: 0.022126\n",
            "loss: 0.020520  [ 3400/12360] <-> largest in grain: 0.022141\n",
            "loss: 0.020462  [ 3500/12360] <-> largest in grain: 0.022245\n",
            "loss: 0.020449  [ 3600/12360] <-> largest in grain: 0.022221\n",
            "loss: 0.020652  [ 3700/12360] <-> largest in grain: 0.022433\n",
            "loss: 0.020482  [ 3800/12360] <-> largest in grain: 0.022012\n",
            "loss: 0.020549  [ 3900/12360] <-> largest in grain: 0.022251\n",
            "loss: 0.020523  [ 4000/12360] <-> largest in grain: 0.022651\n",
            "loss: 0.020599  [ 4100/12360] <-> largest in grain: 0.022541\n",
            "loss: 0.020509  [ 4200/12360] <-> largest in grain: 0.022449\n",
            "loss: 0.020602  [ 4300/12360] <-> largest in grain: 0.022920\n",
            "loss: 0.020632  [ 4400/12360] <-> largest in grain: 0.023008\n",
            "loss: 0.020419  [ 4500/12360] <-> largest in grain: 0.023251\n",
            "loss: 0.020581  [ 4600/12360] <-> largest in grain: 0.022345\n",
            "loss: 0.020380  [ 4700/12360] <-> largest in grain: 0.022220\n",
            "loss: 0.020507  [ 4800/12360] <-> largest in grain: 0.022434\n",
            "loss: 0.020495  [ 4900/12360] <-> largest in grain: 0.022513\n",
            "loss: 0.020589  [ 5000/12360] <-> largest in grain: 0.022228\n",
            "loss: 0.020710  [ 5100/12360] <-> largest in grain: 0.022615\n",
            "loss: 0.020606  [ 5200/12360] <-> largest in grain: 0.022245\n",
            "loss: 0.020562  [ 5300/12360] <-> largest in grain: 0.022534\n",
            "loss: 0.020506  [ 5400/12360] <-> largest in grain: 0.022121\n",
            "loss: 0.020569  [ 5500/12360] <-> largest in grain: 0.022291\n",
            "loss: 0.020452  [ 5600/12360] <-> largest in grain: 0.022486\n",
            "loss: 0.020582  [ 5700/12360] <-> largest in grain: 0.022737\n",
            "loss: 0.020519  [ 5800/12360] <-> largest in grain: 0.022627\n",
            "loss: 0.020477  [ 5900/12360] <-> largest in grain: 0.022788\n",
            "loss: 0.020334  [ 6000/12360] <-> largest in grain: 0.022050\n",
            "loss: 0.020599  [ 6100/12360] <-> largest in grain: 0.022401\n",
            "loss: 0.020562  [ 6200/12360] <-> largest in grain: 0.023644\n",
            "loss: 0.020478  [ 6300/12360] <-> largest in grain: 0.022273\n",
            "loss: 0.020479  [ 6400/12360] <-> largest in grain: 0.022414\n",
            "loss: 0.020548  [ 6500/12360] <-> largest in grain: 0.022195\n",
            "loss: 0.020475  [ 6600/12360] <-> largest in grain: 0.022257\n",
            "loss: 0.020608  [ 6700/12360] <-> largest in grain: 0.022677\n",
            "loss: 0.020438  [ 6800/12360] <-> largest in grain: 0.022092\n",
            "loss: 0.020435  [ 6900/12360] <-> largest in grain: 0.022185\n",
            "loss: 0.020571  [ 7000/12360] <-> largest in grain: 0.022062\n",
            "loss: 0.020523  [ 7100/12360] <-> largest in grain: 0.022997\n",
            "loss: 0.020455  [ 7200/12360] <-> largest in grain: 0.022077\n",
            "loss: 0.020374  [ 7300/12360] <-> largest in grain: 0.022516\n",
            "loss: 0.020473  [ 7400/12360] <-> largest in grain: 0.022448\n",
            "loss: 0.020567  [ 7500/12360] <-> largest in grain: 0.022295\n",
            "loss: 0.020534  [ 7600/12360] <-> largest in grain: 0.023463\n",
            "loss: 0.020533  [ 7700/12360] <-> largest in grain: 0.022490\n",
            "loss: 0.020475  [ 7800/12360] <-> largest in grain: 0.022482\n",
            "loss: 0.020507  [ 7900/12360] <-> largest in grain: 0.022561\n",
            "loss: 0.020396  [ 8000/12360] <-> largest in grain: 0.022241\n",
            "loss: 0.020514  [ 8100/12360] <-> largest in grain: 0.022807\n",
            "loss: 0.020589  [ 8200/12360] <-> largest in grain: 0.022335\n",
            "loss: 0.020534  [ 8300/12360] <-> largest in grain: 0.022667\n",
            "loss: 0.020515  [ 8400/12360] <-> largest in grain: 0.022786\n",
            "loss: 0.020492  [ 8500/12360] <-> largest in grain: 0.022585\n",
            "loss: 0.020441  [ 8600/12360] <-> largest in grain: 0.022923\n",
            "loss: 0.020375  [ 8700/12360] <-> largest in grain: 0.022441\n",
            "loss: 0.020522  [ 8800/12360] <-> largest in grain: 0.023646\n",
            "loss: 0.020617  [ 8900/12360] <-> largest in grain: 0.022864\n",
            "loss: 0.020379  [ 9000/12360] <-> largest in grain: 0.022531\n",
            "loss: 0.020594  [ 9100/12360] <-> largest in grain: 0.023132\n",
            "loss: 0.020398  [ 9200/12360] <-> largest in grain: 0.022874\n",
            "loss: 0.020510  [ 9300/12360] <-> largest in grain: 0.022518\n",
            "loss: 0.020449  [ 9400/12360] <-> largest in grain: 0.021854\n",
            "loss: 0.020477  [ 9500/12360] <-> largest in grain: 0.022710\n",
            "loss: 0.020478  [ 9600/12360] <-> largest in grain: 0.023383\n",
            "loss: 0.020406  [ 9700/12360] <-> largest in grain: 0.022388\n",
            "loss: 0.020414  [ 9800/12360] <-> largest in grain: 0.023243\n",
            "loss: 0.020537  [ 9900/12360] <-> largest in grain: 0.023193\n",
            "loss: 0.020560  [10000/12360] <-> largest in grain: 0.022403\n",
            "loss: 0.020479  [10100/12360] <-> largest in grain: 0.022282\n",
            "loss: 0.020544  [10200/12360] <-> largest in grain: 0.022631\n",
            "loss: 0.020582  [10300/12360] <-> largest in grain: 0.022357\n",
            "loss: 0.020451  [10400/12360] <-> largest in grain: 0.022413\n",
            "loss: 0.020440  [10500/12360] <-> largest in grain: 0.022054\n",
            "loss: 0.020490  [10600/12360] <-> largest in grain: 0.022122\n",
            "loss: 0.020535  [10700/12360] <-> largest in grain: 0.022471\n",
            "loss: 0.020553  [10800/12360] <-> largest in grain: 0.022622\n",
            "loss: 0.020501  [10900/12360] <-> largest in grain: 0.022846\n",
            "loss: 0.020622  [11000/12360] <-> largest in grain: 0.022528\n",
            "loss: 0.020391  [11100/12360] <-> largest in grain: 0.022957\n",
            "loss: 0.020392  [11200/12360] <-> largest in grain: 0.022126\n",
            "loss: 0.020354  [11300/12360] <-> largest in grain: 0.021942\n",
            "loss: 0.020449  [11400/12360] <-> largest in grain: 0.022407\n",
            "loss: 0.020530  [11500/12360] <-> largest in grain: 0.022539\n",
            "loss: 0.020561  [11600/12360] <-> largest in grain: 0.022364\n",
            "loss: 0.020546  [11700/12360] <-> largest in grain: 0.022467\n",
            "loss: 0.020353  [11800/12360] <-> largest in grain: 0.022592\n",
            "loss: 0.020469  [11900/12360] <-> largest in grain: 0.022253\n",
            "loss: 0.020493  [12000/12360] <-> largest in grain: 0.022530\n",
            "loss: 0.020577  [12100/12360] <-> largest in grain: 0.022497\n",
            "loss: 0.020526  [12200/12360] <-> largest in grain: 0.023192\n",
            "loss: 0.020530  [12300/12360] <-> largest in grain: 0.022736\n",
            "Batch size warning! [torch.Size([56, 11, 64])]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.000207  [    0/12360] <-> largest in grain: 0.020719\n",
            "loss: 0.020425  [  100/12360] <-> largest in grain: 0.022844\n",
            "loss: 0.020433  [  200/12360] <-> largest in grain: 0.022466\n",
            "loss: 0.020540  [  300/12360] <-> largest in grain: 0.022450\n",
            "loss: 0.020356  [  400/12360] <-> largest in grain: 0.022138\n",
            "loss: 0.020549  [  500/12360] <-> largest in grain: 0.022639\n",
            "loss: 0.020509  [  600/12360] <-> largest in grain: 0.022030\n",
            "loss: 0.020464  [  700/12360] <-> largest in grain: 0.022520\n",
            "loss: 0.020544  [  800/12360] <-> largest in grain: 0.022811\n",
            "loss: 0.020559  [  900/12360] <-> largest in grain: 0.022351\n",
            "loss: 0.020472  [ 1000/12360] <-> largest in grain: 0.022090\n",
            "loss: 0.020450  [ 1100/12360] <-> largest in grain: 0.022168\n",
            "loss: 0.020409  [ 1200/12360] <-> largest in grain: 0.022205\n",
            "loss: 0.020518  [ 1300/12360] <-> largest in grain: 0.022004\n",
            "loss: 0.020463  [ 1400/12360] <-> largest in grain: 0.022661\n",
            "loss: 0.020564  [ 1500/12360] <-> largest in grain: 0.022271\n",
            "loss: 0.020643  [ 1600/12360] <-> largest in grain: 0.022977\n",
            "loss: 0.020466  [ 1700/12360] <-> largest in grain: 0.022073\n",
            "loss: 0.020392  [ 1800/12360] <-> largest in grain: 0.022643\n",
            "loss: 0.020447  [ 1900/12360] <-> largest in grain: 0.023071\n",
            "loss: 0.020503  [ 2000/12360] <-> largest in grain: 0.022435\n",
            "loss: 0.020466  [ 2100/12360] <-> largest in grain: 0.022135\n",
            "loss: 0.020463  [ 2200/12360] <-> largest in grain: 0.021960\n",
            "loss: 0.020450  [ 2300/12360] <-> largest in grain: 0.022604\n",
            "loss: 0.020396  [ 2400/12360] <-> largest in grain: 0.022012\n",
            "loss: 0.020368  [ 2500/12360] <-> largest in grain: 0.022455\n",
            "loss: 0.020442  [ 2600/12360] <-> largest in grain: 0.022718\n",
            "loss: 0.020466  [ 2700/12360] <-> largest in grain: 0.022682\n",
            "loss: 0.020387  [ 2800/12360] <-> largest in grain: 0.023417\n",
            "loss: 0.020456  [ 2900/12360] <-> largest in grain: 0.023110\n",
            "loss: 0.020363  [ 3000/12360] <-> largest in grain: 0.022173\n",
            "loss: 0.020551  [ 3100/12360] <-> largest in grain: 0.022253\n",
            "loss: 0.020368  [ 3200/12360] <-> largest in grain: 0.023182\n",
            "loss: 0.020626  [ 3300/12360] <-> largest in grain: 0.022256\n",
            "loss: 0.020325  [ 3400/12360] <-> largest in grain: 0.021964\n",
            "loss: 0.020496  [ 3500/12360] <-> largest in grain: 0.022232\n",
            "loss: 0.020553  [ 3600/12360] <-> largest in grain: 0.023194\n",
            "loss: 0.020486  [ 3700/12360] <-> largest in grain: 0.022696\n",
            "loss: 0.020514  [ 3800/12360] <-> largest in grain: 0.022510\n",
            "loss: 0.020427  [ 3900/12360] <-> largest in grain: 0.021951\n",
            "loss: 0.020541  [ 4000/12360] <-> largest in grain: 0.022305\n",
            "loss: 0.020462  [ 4100/12360] <-> largest in grain: 0.022689\n",
            "loss: 0.020511  [ 4200/12360] <-> largest in grain: 0.022409\n",
            "loss: 0.020592  [ 4300/12360] <-> largest in grain: 0.022400\n",
            "loss: 0.020515  [ 4400/12360] <-> largest in grain: 0.022020\n",
            "loss: 0.020360  [ 4500/12360] <-> largest in grain: 0.022681\n",
            "loss: 0.020446  [ 4600/12360] <-> largest in grain: 0.022488\n",
            "loss: 0.020482  [ 4700/12360] <-> largest in grain: 0.022405\n",
            "loss: 0.020550  [ 4800/12360] <-> largest in grain: 0.022676\n",
            "loss: 0.020503  [ 4900/12360] <-> largest in grain: 0.022353\n",
            "loss: 0.020414  [ 5000/12360] <-> largest in grain: 0.022397\n",
            "loss: 0.020439  [ 5100/12360] <-> largest in grain: 0.022396\n",
            "loss: 0.020423  [ 5200/12360] <-> largest in grain: 0.022242\n",
            "loss: 0.020590  [ 5300/12360] <-> largest in grain: 0.022641\n",
            "loss: 0.020265  [ 5400/12360] <-> largest in grain: 0.022728\n",
            "loss: 0.020543  [ 5500/12360] <-> largest in grain: 0.022377\n",
            "loss: 0.020397  [ 5600/12360] <-> largest in grain: 0.022539\n",
            "loss: 0.020550  [ 5700/12360] <-> largest in grain: 0.022274\n",
            "loss: 0.020604  [ 5800/12360] <-> largest in grain: 0.022878\n",
            "loss: 0.020391  [ 5900/12360] <-> largest in grain: 0.021818\n",
            "loss: 0.020398  [ 6000/12360] <-> largest in grain: 0.022578\n",
            "loss: 0.020400  [ 6100/12360] <-> largest in grain: 0.022240\n",
            "loss: 0.020331  [ 6200/12360] <-> largest in grain: 0.022391\n",
            "loss: 0.020382  [ 6300/12360] <-> largest in grain: 0.022167\n",
            "loss: 0.020439  [ 6400/12360] <-> largest in grain: 0.022384\n",
            "loss: 0.020393  [ 6500/12360] <-> largest in grain: 0.022057\n",
            "loss: 0.020369  [ 6600/12360] <-> largest in grain: 0.022151\n",
            "loss: 0.020540  [ 6700/12360] <-> largest in grain: 0.022893\n",
            "loss: 0.020391  [ 6800/12360] <-> largest in grain: 0.022166\n",
            "loss: 0.020479  [ 6900/12360] <-> largest in grain: 0.022812\n",
            "loss: 0.020360  [ 7000/12360] <-> largest in grain: 0.022794\n",
            "loss: 0.020326  [ 7100/12360] <-> largest in grain: 0.023065\n",
            "loss: 0.020580  [ 7200/12360] <-> largest in grain: 0.022677\n",
            "loss: 0.020237  [ 7300/12360] <-> largest in grain: 0.022161\n",
            "loss: 0.020390  [ 7400/12360] <-> largest in grain: 0.022113\n",
            "loss: 0.020403  [ 7500/12360] <-> largest in grain: 0.022487\n",
            "loss: 0.020478  [ 7600/12360] <-> largest in grain: 0.022197\n",
            "loss: 0.020292  [ 7700/12360] <-> largest in grain: 0.022333\n",
            "loss: 0.020381  [ 7800/12360] <-> largest in grain: 0.021801\n",
            "loss: 0.020471  [ 7900/12360] <-> largest in grain: 0.022302\n",
            "loss: 0.020490  [ 8000/12360] <-> largest in grain: 0.022461\n",
            "loss: 0.020451  [ 8100/12360] <-> largest in grain: 0.022244\n",
            "loss: 0.020303  [ 8200/12360] <-> largest in grain: 0.022582\n",
            "loss: 0.020579  [ 8300/12360] <-> largest in grain: 0.022879\n",
            "loss: 0.020268  [ 8400/12360] <-> largest in grain: 0.021666\n",
            "loss: 0.020390  [ 8500/12360] <-> largest in grain: 0.022040\n",
            "loss: 0.020447  [ 8600/12360] <-> largest in grain: 0.022555\n",
            "loss: 0.020462  [ 8700/12360] <-> largest in grain: 0.022239\n",
            "loss: 0.020413  [ 8800/12360] <-> largest in grain: 0.022136\n",
            "loss: 0.020391  [ 8900/12360] <-> largest in grain: 0.022246\n",
            "loss: 0.020364  [ 9000/12360] <-> largest in grain: 0.022490\n",
            "loss: 0.020497  [ 9100/12360] <-> largest in grain: 0.021998\n",
            "loss: 0.020564  [ 9200/12360] <-> largest in grain: 0.022303\n",
            "loss: 0.020440  [ 9300/12360] <-> largest in grain: 0.022380\n",
            "loss: 0.020470  [ 9400/12360] <-> largest in grain: 0.023417\n",
            "loss: 0.020429  [ 9500/12360] <-> largest in grain: 0.022143\n",
            "loss: 0.020435  [ 9600/12360] <-> largest in grain: 0.022156\n",
            "loss: 0.020417  [ 9700/12360] <-> largest in grain: 0.022336\n",
            "loss: 0.020347  [ 9800/12360] <-> largest in grain: 0.022643\n",
            "loss: 0.020489  [ 9900/12360] <-> largest in grain: 0.023356\n",
            "loss: 0.020371  [10000/12360] <-> largest in grain: 0.022487\n",
            "loss: 0.020368  [10100/12360] <-> largest in grain: 0.022002\n",
            "loss: 0.020488  [10200/12360] <-> largest in grain: 0.022036\n",
            "loss: 0.020375  [10300/12360] <-> largest in grain: 0.023350\n",
            "loss: 0.020365  [10400/12360] <-> largest in grain: 0.022194\n",
            "loss: 0.020401  [10500/12360] <-> largest in grain: 0.022144\n",
            "loss: 0.020428  [10600/12360] <-> largest in grain: 0.022499\n",
            "loss: 0.020354  [10700/12360] <-> largest in grain: 0.022672\n",
            "loss: 0.020332  [10800/12360] <-> largest in grain: 0.022496\n",
            "loss: 0.020430  [10900/12360] <-> largest in grain: 0.022338\n",
            "loss: 0.020452  [11000/12360] <-> largest in grain: 0.022636\n",
            "loss: 0.020227  [11100/12360] <-> largest in grain: 0.022977\n",
            "loss: 0.020523  [11200/12360] <-> largest in grain: 0.022163\n",
            "loss: 0.020362  [11300/12360] <-> largest in grain: 0.023073\n",
            "loss: 0.020400  [11400/12360] <-> largest in grain: 0.022378\n",
            "loss: 0.020248  [11500/12360] <-> largest in grain: 0.022243\n",
            "loss: 0.020464  [11600/12360] <-> largest in grain: 0.022809\n",
            "loss: 0.020347  [11700/12360] <-> largest in grain: 0.022775\n",
            "loss: 0.020482  [11800/12360] <-> largest in grain: 0.022341\n",
            "loss: 0.020401  [11900/12360] <-> largest in grain: 0.022761\n",
            "loss: 0.020235  [12000/12360] <-> largest in grain: 0.021724\n",
            "loss: 0.020400  [12100/12360] <-> largest in grain: 0.022320\n",
            "loss: 0.020263  [12200/12360] <-> largest in grain: 0.022004\n",
            "loss: 0.020344  [12300/12360] <-> largest in grain: 0.022404\n",
            "Batch size warning! [torch.Size([56, 11, 64])]\n"
          ]
        }
      ],
      "source": [
        "losses_to_plot = [] # Reset\n",
        "DEBUG = False\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
        "    if train_loop(Train_Normal_DL, ae, loss_fn, optimizer) == False:\n",
        "        # Interrupted\n",
        "        break\n",
        "\n",
        "    # Step the scheduler\n",
        "    lr_scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3qKM-AfFctq"
      },
      "outputs": [],
      "source": [
        "# Fine tune the learning rate\n",
        "for g in optimizer.param_groups:\n",
        "    g['lr'] = 0.0001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBNHRNP-Kqkc"
      },
      "outputs": [],
      "source": [
        "# Save Model\n",
        "torch.save(ae.state_dict(), '/content/drive/MyDrive/FinalYearProject/PYNQ/Models/TemporalConvNets/.model')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the losses over time\n",
        "plt.plot(losses_to_plot[:len(losses_to_plot)//4], color='blue', label='Non Quantised')\n",
        "plt.plot(bit_8_losses[:len(bit_8_losses)//4], color='red',  label='8 Bit Quantised')\n",
        "plt.xlabel('Batches /100')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.title('Quantised vs Non-Quantised Training losses')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "FW6OHvZUhFZw",
        "outputId": "16838150-5a0a-498e-f319-c208df95bdf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bit_8_losses = losses_to_plot"
      ],
      "metadata": {
        "id": "ufojs8apndw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3tUYfc9jrQb"
      },
      "source": [
        "# **Model Statistics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eFTkKBtF78E6",
        "outputId": "eb9a1f7d-c295-4035-9745-e535abca35f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2932552\n",
            "791032\n",
            "DoS Mean: 196.81311\n",
            "Normal Mean: 258.00318\n",
            "Fuzzy Mean: 269.43503\n",
            "Gear Mean: 294.00737\n",
            "RPM Mean: 269.82123\n"
          ]
        }
      ],
      "source": [
        "print(len(Train_DoS_DS))\n",
        "print(len(Train_Normal_DS))\n",
        "\n",
        "SampleSize = 100000\n",
        "DoS_RT, Normal_RT, Fuzzy_RT, Gear_RT, RPM_RT = 0, 0, 0, 0, 0\n",
        "for i in range(SampleSize):\n",
        "    DoS_RT += torch.sum(Train_DoS_DS[i][0]).item()\n",
        "    Normal_RT += torch.sum(Train_Normal_DS[i][0]).item()\n",
        "    Fuzzy_RT += torch.sum(Train_Fuzzy_DS[i][0]).item()\n",
        "    Gear_RT += torch.sum(Train_Gear_DS[i][0]).item()\n",
        "    RPM_RT += torch.sum(Train_RPM_DS[i][0]).item()\n",
        "\n",
        "print(f\"DoS Mean: {DoS_RT/SampleSize}\")\n",
        "print(f\"Normal Mean: {Normal_RT/SampleSize}\")\n",
        "print(f\"Fuzzy Mean: {Fuzzy_RT/SampleSize}\")\n",
        "print(f\"Gear Mean: {Gear_RT/SampleSize}\")\n",
        "print(f\"RPM Mean: {RPM_RT/SampleSize}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwQM_v4p0Q7r"
      },
      "source": [
        "This data (possibly) shows why it is much harder to detect DoS attacks, because DoS data is much sparser (includes much fewer 1's) rewarding guessing 0's\n",
        "\n",
        "\n",
        "Solution to this is to pre-process the data to be 0 = -1 and 1 = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Aiye-XWjt-B"
      },
      "outputs": [],
      "source": [
        "from math import sqrt\n",
        "\n",
        "class FastModelStats:\n",
        "    def __init__(self):\n",
        "        self.n       = 0\n",
        "        self.mean    = 0\n",
        "        self.std_dev = 0\n",
        "        self.z       = 3 # 3 std dev = 99.7% 4 std dev = 99.99%\n",
        "        self.losses  = None\n",
        "        self.calculated = False\n",
        "\n",
        "\n",
        "    def calculate(self, model : nn.Module, normal_train_dl : DataLoader, loss_fn, num_batches=-1, threat=False, threshold=-1) -> None:\n",
        "        \"\"\" This can be used to get the models statistics for the normal data and anomaly data and then can be compared \"\"\"\n",
        "\n",
        "        size = len(normal_train_dl) if num_batches == -1 else num_batches\n",
        "        self.losses = torch.zeros(size, device=torch.device(device))\n",
        "        self.calculated = True\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "\n",
        "        # Only use these values if threshold is set.\n",
        "        self.FP = 0 # False Positive\n",
        "        self.FN = 0 # False Negative\n",
        "        self.TP = 0 # True Positive\n",
        "        self.TN = 0 # True Negative\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch, (X, labels, cls) in enumerate(normal_train_dl):\n",
        "                if len(X) != 1:\n",
        "                    # Batch Size should only be 1\n",
        "                    raise Exception(f\"Batch size warning! [{X.shape}]\")\n",
        "                    continue\n",
        "\n",
        "                if threat == True and torch.sum(labels) == 0:\n",
        "                    # Non threat in threat dataset\n",
        "                    ThreatImage = False\n",
        "                else:\n",
        "                    ThreatImage = True\n",
        "\n",
        "                # X = (X * 2) - 1.0\n",
        "                X = torch.reshape(X, (-1, 11, 64, 1))\n",
        "                pred = model(X)\n",
        "                reconstruction_loss = loss_fn(pred, X)\n",
        "\n",
        "                if ThreatImage == True:\n",
        "                    self.losses[self.n] = reconstruction_loss\n",
        "                    self.n += 1\n",
        "\n",
        "                    if reconstruction_loss > threshold:\n",
        "                        self.TP += 1\n",
        "                    else:\n",
        "                        self.FN += 1\n",
        "                else:\n",
        "                    if reconstruction_loss > threshold:\n",
        "                        self.FP += 1\n",
        "                    else:\n",
        "                        self.TN += 1\n",
        "\n",
        "\n",
        "\n",
        "                if (batch % 1000) == 0:\n",
        "                    print(f\"\\rBatch {batch} loss: {reconstruction_loss:<5f} [{batch}/{size}]\", end='', flush=True)\n",
        "                if batch >= (num_batches - 1):\n",
        "                    print(f\"{batch + 1} Batches Complete!\")\n",
        "                    break\n",
        "\n",
        "    def clear(self):\n",
        "        self.std_dev = 0\n",
        "        self.mean = 0\n",
        "        self.n = 0\n",
        "        self.losses = None\n",
        "        self.calculated = False\n",
        "\n",
        "    def calculate_mean(self):\n",
        "        if not self.calculated:\n",
        "            raise Exception(\"No losses have been calculated\")\n",
        "        self.mean = np.mean(self.losses[:self.n].cpu().detach().numpy())\n",
        "        return self.mean\n",
        "\n",
        "    def calculate_standard_deviation(self):\n",
        "        if not self.calculated:\n",
        "            raise Exception(\"No losses have been calculated\")\n",
        "        self.std_dev = np.std(self.losses[:self.n].cpu().detach().numpy())\n",
        "        return self.std_dev\n",
        "\n",
        "    def getThreshold(self) -> float:\n",
        "        if not self.calculated:\n",
        "            raise Exception(\"No losses have been calculated\")\n",
        "        return self.mean + (self.z * self.std_dev)\n",
        "\n",
        "\n",
        "    def getRecall(self):\n",
        "        if (self.TP + self.FN):\n",
        "            self.recall = self.TP / (self.TP + self.FN)\n",
        "        else:\n",
        "            self.recall = 0\n",
        "        return self.recall\n",
        "\n",
        "    def getPrecision(self):\n",
        "        if (self.TP + self.FP):\n",
        "            self.precision = self.TP / (self.TP + self.FP)\n",
        "        else:\n",
        "            self.precision = 0\n",
        "        return self.precision\n",
        "\n",
        "    def getF1(self):\n",
        "        if (self.precision + self.recall):\n",
        "            self.F1 = (2 * self.precision * self.recall) / (self.precision + self.recall)\n",
        "        else:\n",
        "            self.F1 = 0\n",
        "        return self.F1\n",
        "\n",
        "    def getAccuracy(self):\n",
        "        if (self.TP + self.TN + self.FP + self.FN):\n",
        "            self.accuracy = (self.TP + self.TN) / (self.TP + self.TN + self.FP + self.FN)\n",
        "        else:\n",
        "            self.accuracy = 0\n",
        "        return self.accuracy\n",
        "\n",
        "    def plot_losses(self, title : str = \"\", threat=False, normal_threshold=-1):\n",
        "        if not self.calculated:\n",
        "            raise Exception(\"No losses have been calculated\")\n",
        "\n",
        "        plt.hist(self.losses[:self.n].cpu().detach().numpy(), bins=100)\n",
        "\n",
        "        # Plot control lines\n",
        "        if threat == True:\n",
        "            plt.axvline(normal_threshold, color='r', linestyle='dashed', linewidth=1, label=\"Normal Threshold\")\n",
        "        else:\n",
        "            one_std_dev = self.mean + self.std_dev\n",
        "            two_std_dev = self.mean + (self.std_dev * 2)\n",
        "            three_std_dev = self.mean + (self.std_dev * 3)\n",
        "            plt.axvline(self.mean, color='k', linestyle='dashed', linewidth=1, label=\"Mean\")\n",
        "            plt.axvline(one_std_dev, color='g', linestyle='dashed', linewidth=1, label=\"1 Standard Deviation\")\n",
        "            plt.axvline(two_std_dev, color='b', linestyle='dashed', linewidth=1, label=\"2 Standard Deviation\")\n",
        "            plt.axvline(three_std_dev , color='r', linestyle='dashed', linewidth=1, label=\"3 Standard Deviation\")\n",
        "\n",
        "        plt.legend(loc='upper right')\n",
        "        plt.title(title)\n",
        "        plt.xlabel('Reconstruction Loss')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def print_stats(self):\n",
        "        if not self.calculated:\n",
        "            raise Exception(\"No losses have been calculated\")\n",
        "        print(f\"Model Stats: \")\n",
        "        print(f\"Mean:      {self.mean:>5f}\")\n",
        "        print(f\"Std Dev:   {self.std_dev:>5f}\")\n",
        "        print(f\"Threshold: {self.getThreshold():>5f}\")\n",
        "\n",
        "        print(f\"Recall:    {self.recall:>5f}\")\n",
        "        print(f\"Precision: {self.precision:>5f}\")\n",
        "        print(f\"F1:        {self.F1:>5f}\")\n",
        "        print(f\"Accuracy:  {self.accuracy:>5f}\")\n",
        "\n",
        "\n",
        "    def get_stats(self, model : nn.Module, normal_train_dl : DataLoader, loss_fn, num_batches=-1, title : str = \"\", threat=False, normal_threshold=-1):\n",
        "        self.clear()\n",
        "        self.calculate(model, normal_train_dl, loss_fn, num_batches, threat, normal_threshold)\n",
        "        self.calculate_mean()\n",
        "        self.calculate_standard_deviation()\n",
        "\n",
        "        self.getPrecision()\n",
        "        self.getRecall()\n",
        "        self.getF1()\n",
        "        self.getAccuracy()\n",
        "\n",
        "        self.print_stats()\n",
        "        self.plot_losses(title, threat, normal_threshold)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCEL_WgziiQk"
      },
      "outputs": [],
      "source": [
        "NUM_STATS_BATCHES = 10000 # Run this many data points as sample size\n",
        "DUT = ae # Design (Model) Under Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "T_gg-EgYqDXV",
        "outputId": "e50ef141-ca50-4daf-e5fc-6d2a64ca9922"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 9000 loss: 0.012934 [9000/10000]10000 Batches Complete!\n",
            "Model Stats: \n",
            "Mean:      0.017158\n",
            "Std Dev:   0.004837\n",
            "Threshold: 0.031668\n",
            "Recall:    1.000000\n",
            "Precision: 1.000000\n",
            "F1:        1.000000\n",
            "Accuracy:  1.000000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "Train_Normal_DL_1_batch = DataLoader(Train_Normal_DS, batch_size=1, shuffle=True)\n",
        "normal_model_stats = FastModelStats()\n",
        "normal_model_stats.get_stats(DUT, Train_Normal_DL_1_batch, loss_fn, NUM_STATS_BATCHES, \"Normal Train Data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "2wS5TF13qe2T",
        "outputId": "95097276-026e-4bcb-a457-73bf4380bf48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 9000 loss: 0.014301 [9000/10000]10000 Batches Complete!\n",
            "Model Stats: \n",
            "Mean:      0.016977\n",
            "Std Dev:   0.004770\n",
            "Threshold: 0.031287\n",
            "Recall:    1.000000\n",
            "Precision: 1.000000\n",
            "F1:        1.000000\n",
            "Accuracy:  1.000000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "TestNormal_model_stats = FastModelStats()\n",
        "TestNormal_model_stats.get_stats(DUT, Test_Normal_DL, loss_fn, NUM_STATS_BATCHES, \"Normal Test Data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAaEIGG1qkjU"
      },
      "outputs": [],
      "source": [
        "FP_err_count = 0\n",
        "for i in range(TestNormal_model_stats.n):\n",
        "    if TestNormal_model_stats.losses[i] > normal_model_stats.getThreshold():\n",
        "        FP_err_count += 1\n",
        "print(f\"False Positive (FP) Errors: {FP_err_count}/{TestNormal_model_stats.n} = {(FP_err_count/TestNormal_model_stats.n)*100}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "w5gsA1EcoEtI",
        "outputId": "066192d4-d5f7-46d0-a2d0-de32c778f967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 9000 loss: 0.120335 [9000/10000]10000 Batches Complete!\n",
            "Model Stats: \n",
            "Mean:      0.102969\n",
            "Std Dev:   0.011594\n",
            "Threshold: 0.137750\n",
            "Recall:    0.995444\n",
            "Precision: 0.962176\n",
            "F1:        0.978527\n",
            "Accuracy:  0.983700\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "DoS_model_stats = FastModelStats()\n",
        "DoS_model_stats.get_stats(DUT, Train_DoS_DL, loss_fn, NUM_STATS_BATCHES, \"DoS Train Data\", True, normal_model_stats.getThreshold())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbkXmI94DMSv"
      },
      "outputs": [],
      "source": [
        "DoS_err_count = 0\n",
        "for i in range(DoS_model_stats.n):\n",
        "    if DoS_model_stats.losses[i] < normal_model_stats.getThreshold():\n",
        "        DoS_err_count += 1\n",
        "print(f\"DoS Errors: {DoS_err_count}/{DoS_model_stats.n} = {(DoS_err_count/DoS_model_stats.n)*100}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8fwEGZZbrKo"
      },
      "outputs": [],
      "source": [
        "fuzzy_model_stats = FastModelStats()\n",
        "fuzzy_model_stats.get_stats(DUT, Train_Fuzzy_DL, loss_fn, NUM_STATS_BATCHES, \"Fuzzy Train Data\", True, normal_model_stats.getThreshold())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTUHV8usbyIS",
        "outputId": "30587c22-4e9e-4668-83ab-a4911e2c05cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fuzzy Errors: 6/4314 = 0.13908205841446453%\n"
          ]
        }
      ],
      "source": [
        "fuzzy_err_count = 0\n",
        "for i in range(fuzzy_model_stats.n):\n",
        "    if fuzzy_model_stats.losses[i] < normal_model_stats.getThreshold():\n",
        "        fuzzy_err_count += 1\n",
        "print(f\"Fuzzy Errors: {fuzzy_err_count}/{fuzzy_model_stats.n} = {(fuzzy_err_count/fuzzy_model_stats.n)*100}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6ZgeAqEXA3Do"
      },
      "outputs": [],
      "source": [
        "gear_model_stats = FastModelStats()\n",
        "gear_model_stats.get_stats(DUT, Train_Gear_DL, loss_fn, NUM_STATS_BATCHES, \"Gear Train Data\", True, normal_model_stats.getThreshold())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwdhtC6oDdcl"
      },
      "outputs": [],
      "source": [
        "gear_err_count = 0\n",
        "for i in range(gear_model_stats.n):\n",
        "    if gear_model_stats.losses[i] < normal_model_stats.getThreshold():\n",
        "        gear_err_count += 1\n",
        "print(f\"Gear Errors: {gear_err_count}/{gear_model_stats.n} = {(gear_err_count/gear_model_stats.n) * 100}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "collapsed": true,
        "id": "jr1yzPXRbSXn",
        "outputId": "1f18b58f-f491-4fc3-a054-5675fdc55609"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 9000 loss: 0.011507 [9000/10000]10000 Batches Complete!\n",
            "Model Stats: \n",
            "Mean:      0.064569\n",
            "Std Dev:   0.009322\n",
            "Threshold: 0.092535\n",
            "Recall:    0.995331\n",
            "Precision: 0.990706\n",
            "F1:        0.993013\n",
            "Accuracy:  0.992200\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "RPM_model_stats = FastModelStats()\n",
        "RPM_model_stats.get_stats(DUT, Train_RPM_DL, loss_fn, NUM_STATS_BATCHES, \"RPM Train Data\", True, normal_model_stats.getThreshold())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hi9Bgxdlbd0h"
      },
      "outputs": [],
      "source": [
        "RPM_err_count = 0\n",
        "for i in range(RPM_model_stats.n):\n",
        "    if RPM_model_stats.losses[i] < normal_model_stats.getThreshold():\n",
        "        RPM_err_count += 1\n",
        "print(f\"RPM Errors: {RPM_err_count}/{RPM_model_stats.n} = {(RPM_err_count/RPM_model_stats.n) * 100}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(RPM_model_stats.FP)\n",
        "print(RPM_model_stats.FN)\n",
        "print(RPM_model_stats.TP)\n",
        "print(RPM_model_stats.TN)"
      ],
      "metadata": {
        "id": "ZgUDOrn6j56y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(gear_model_stats.FP)\n",
        "print(gear_model_stats.FN)\n",
        "print(gear_model_stats.TP)\n",
        "print(gear_model_stats.TN)"
      ],
      "metadata": {
        "id": "BGeJq67SkEU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(DoS_model_stats.FP)\n",
        "print(DoS_model_stats.FN)\n",
        "print(DoS_model_stats.TP)\n",
        "print(DoS_model_stats.TN)"
      ],
      "metadata": {
        "id": "y8n5uTGMkIL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(fuzzy_model_stats.FP)\n",
        "print(fuzzy_model_stats.FN)\n",
        "print(fuzzy_model_stats.TP)\n",
        "print(fuzzy_model_stats.TN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHWaSGNQkN0P",
        "outputId": "17bcd250-1bc7-4cd3-f724-d7461ef095fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "97\n",
            "6\n",
            "4308\n",
            "5589\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "malicious = gear_model_stats\n",
        "\n",
        "# Example data\n",
        "array1 = TestNormal_model_stats.losses.cpu().detach().numpy()\n",
        "array2 = malicious.losses[:malicious.n].cpu().detach().numpy()\n",
        "\n",
        "# Plot the histogram\n",
        "plt.hist(array1, bins=100, color='blue', alpha=0.7, label='Normal')\n",
        "plt.hist(array2, bins=100, color='red', alpha=0.7, label='Malicious')\n",
        "\n",
        "# Add labels and legend\n",
        "plt.xlabel('Loss')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title(f'Normal vs malicious losses at bit-width: {BIT_WIDTH}')\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "Va8O-LF87yUw",
        "outputId": "186ebee0-45c5-42b0-e003-fe4394781694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xybrW5tTIONl"
      },
      "source": [
        "# **Benchmark Quantisation and Layers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ijw5PYPXdSiS",
        "outputId": "9f253277-28fc-4c68-ffbc-ce2dd16c3e38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n",
            "Collecting torchstat\n",
            "  Downloading torchstat-0.0.7-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from torchstat) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchstat) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from torchstat) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->torchstat) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->torchstat) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->torchstat) (2025.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->torchstat) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchstat) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->torchstat) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->torchstat) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->torchstat) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchstat) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchstat) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchstat) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->torchstat) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->torchstat) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->torchstat) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->torchstat) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->torchstat) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->torchstat) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->torchstat) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->torchstat) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchstat) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchstat) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchstat) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->torchstat) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->torchstat) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->torchstat) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->torchstat) (3.0.2)\n",
            "Downloading torchstat-0.0.7-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: torchstat\n",
            "Successfully installed torchstat-0.0.7\n"
          ]
        }
      ],
      "source": [
        "%pip install torchinfo\n",
        "%pip install torchstat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dsj73PL8jA-"
      },
      "outputs": [],
      "source": [
        "import timeit\n",
        "from torchinfo import summary\n",
        "from torchstat import stat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xezyt8E8j1l",
        "outputId": "3628498a-d4b2-4179-b256-7fc7dbf60db4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "AE                                       [1, 11, 64, 1]            --\n",
              "Sequential: 1-1                        [1, 64, 16, 1]            --\n",
              "    Conv2d: 2-1                       [1, 32, 64, 1]            1,056\n",
              "    ReLU: 2-2                         [1, 32, 64, 1]            --\n",
              "    MaxPool2d: 2-3                    [1, 32, 32, 1]            --\n",
              "    Conv2d: 2-4                       [1, 64, 32, 1]            6,144\n",
              "    ReLU: 2-5                         [1, 64, 32, 1]            --\n",
              "    MaxPool2d: 2-6                    [1, 64, 16, 1]            --\n",
              "Sequential: 1-2                        [1, 11, 64, 1]            --\n",
              "    Upsample: 2-7                     [1, 64, 32, 1]            --\n",
              "    Conv2d: 2-8                       [1, 32, 32, 1]            6,144\n",
              "    ReLU: 2-9                         [1, 32, 32, 1]            --\n",
              "    Upsample: 2-10                    [1, 32, 64, 1]            --\n",
              "    Conv2d: 2-11                      [1, 11, 64, 1]            1,056\n",
              "    ReLU: 2-12                        [1, 11, 64, 1]            --\n",
              "==========================================================================================\n",
              "Total params: 14,400\n",
              "Trainable params: 14,400\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 0.53\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.05\n",
              "Params size (MB): 0.06\n",
              "Estimated Total Size (MB): 0.11\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "summary(ae, input_size=(1, 11, 64, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72rurUaM8mLw",
        "outputId": "a129da50-d069-4bdf-8c11-442b05487ea1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "========================================================================================================================\n",
              "Layer (type:depth-idx)                                                 Output Shape              Param #\n",
              "========================================================================================================================\n",
              "RealTime_AutoEncoder                                                   [1, 1, 64, 12]            --\n",
              "Sequential: 1-1                                                      [1, 128, 64, 12]          1\n",
              "    QuantConv2d: 2-1                                                [1, 128, 64, 12]          1,281\n",
              "        ActQuantProxyFromInjector: 3-1                             [1, 1, 64, 12]            1\n",
              "Sequential: 1-27                                                     --                        (recursive)\n",
              "    QuantSigmoid: 2-36                                              --                        (recursive)\n",
              "        ActQuantProxyFromInjector: 3-55                            --                        (recursive)\n",
              "Sequential: 1-7                                                      --                        (recursive)\n",
              "    QuantConv2d: 2-5                                                --                        (recursive)\n",
              "        ActQuantProxyFromInjector: 3-3                             --                        (recursive)\n",
              "        WeightQuantProxyFromInjector: 3-4                          [128, 1, 3, 3]            1,152\n",
              "        BiasQuantProxyFromInjector: 3-5                            [128]                     --\n",
              "        ActQuantProxyFromInjector: 3-6                             [1, 128, 64, 12]          1\n",
              "Sequential: 1-27                                                     --                        (recursive)\n",
              "    QuantSigmoid: 2-36                                              --                        (recursive)\n",
              "        ActQuantProxyFromInjector: 3-55                            --                        (recursive)\n",
              "Sequential: 1-7                                                      --                        (recursive)\n",
              "    QuantConv2d: 2-5                                                --                        (recursive)\n",
              "        ActQuantProxyFromInjector: 3-8                             --                        (recursive)\n",
              "    QuantReLU: 2-6                                                  [1, 128, 64, 12]          --\n",
              "        ActQuantProxyFromInjector: 3-9                             [1, 128, 64, 12]          --\n",
              "        ActQuantProxyFromInjector: 3-10                            [1, 128, 64, 12]          1\n",
              "Sequential: 1-27                                                     --                        (recursive)\n",
              "    QuantSigmoid: 2-36                                              --                        (recursive)\n",
              "        ActQuantProxyFromInjector: 3-55                            --                        (recursive)\n",
              "Sequential: 1-7                                                      --                        (recursive)\n",
              "    QuantReLU: 2-8                                                  --                        (recursive)\n",
              "        ActQuantProxyFromInjector: 3-12                            --                        (recursive)\n",
              "Sequential: 1-8                                                      [1, 64, 32, 6]            1\n",
              "    QuantConv2d: 2-9                                                [1, 64, 32, 6]            73,793\n",
              "        ActQuantProxyFromInjector: 3-13                            [1, 128, 32, 6]           1\n",
              "Sequential: 1-27                                                     --                        (recursive)\n",
              "    QuantSigmoid: 2-36                                              --                        (recursive)\n",
              "        ActQuantProxyFromInjector: 3-55                            --                        (recursive)\n",
              "Sequential: 1-14                                                     --                        (recursive)\n",
              "    QuantConv2d: 2-13                                               --                        (recursive)\n",
              "        ActQuantProxyFromInjector: 3-15                            --                        (recursive)\n",
              "        WeightQuantProxyFromInjector: 3-16                         [64, 128, 3, 3]           73,728\n",
              "        BiasQuantProxyFromInjector: 3-17                           [64]                      --\n",
              "        ActQuantProxyFromInjector: 3-18                            [1, 64, 32, 6]            1\n",
              "Sequential: 1-27                                                     --                        (recursive)\n",
              "    QuantSigmoid: 2-36                                              --                        (recursive)\n",
              "        ActQuantProxyFromInjector: 3-55                            --                        (recursive)\n",
              "Sequential: 1-14                                                     --                        (recursive)\n",
              "    QuantConv2d: 2-13                                               --                        (recursive)\n",
              "        ActQuantProxyFromInjector: 3-20                            --                        (recursive)\n",
              "    QuantReLU: 2-14                                                 [1, 64, 32, 6]            --\n",
              "        ActQuantProxyFromInjector: 3-21                            [1, 64, 32, 6]            --\n",
              "        ActQuantProxyFromInjector: 3-22                            [1, 64, 32, 6]            1\n",
              "Sequential: 1-27                                                     --                        (recursive)\n",
              "    QuantSigmoid: 2-36                                              --                        (recursive)\n",
              "        ActQuantProxyFromInjector: 3-55                            --                        (recursive)\n",
              "Sequential: 1-14                                                     --                        (recursive)\n",
              "    QuantReLU: 2-16                                                 --                        (recursive)\n",
              "        ActQuantProxyFromInjector: 3-24                            --                        (recursive)\n",
              "Sequential: 1-15                                                     [1, 128, 32, 6]           1\n",
              "    QuantConvTranspose2d: 2-17                                      [1, 128, 32, 6]           73,857\n",
              "        ActQuantProxyFromInjector: 3-25                            [1, 64, 16, 3]            1\n",
              "Sequential: 1-27                                                     --                        (recursive)\n",
              "    QuantSigmoid: 2-36                                              --                        (recursive)\n",
              "        ActQuantProxyFromInjector: 3-55                            --                        (recursive)\n",
              "Sequential: 1-21                                                     --                        (recursive)\n",
              "    QuantConvTranspose2d: 2-21                                      --                        (recursive)\n",
              "        ActQuantProxyFromInjector: 3-27                            --                        (recursive)\n",
              "        WeightQuantProxyFromInjector: 3-28                         [64, 128, 3, 3]           73,728\n",
              "        BiasQuantProxyFromInjector: 3-29                           [128]                     --\n",
              "        ActQuantProxyFromInjector: 3-30                            [1, 128, 32, 6]           1\n",
              "Sequential: 1-27                                                     --                        (recursive)\n",
              "    QuantSigmoid: 2-36                                              --                        (recursive)\n",
              "        ActQuantProxyFromInjector: 3-55                            --                        (recursive)\n",
              "Sequential: 1-21                                                     --                        (recursive)\n",
              "    QuantConvTranspose2d: 2-21                                      --                        (recursive)\n",
              "        ActQuantProxyFromInjector: 3-32                            --                        (recursive)\n",
              "    QuantReLU: 2-22                                                 [1, 128, 32, 6]           --\n",
              "        ActQuantProxyFromInjector: 3-33                            [1, 128, 32, 6]           --\n",
              "        ActQuantProxyFromInjector: 3-34                            [1, 128, 32, 6]           1\n",
              "Sequential: 1-27                                                     --                        (recursive)\n",
              "    QuantSigmoid: 2-36                                              --                        (recursive)\n",
              "        ActQuantProxyFromInjector: 3-55                            --                        (recursive)\n",
              "Sequential: 1-21                                                     --                        (recursive)\n",
              "    QuantReLU: 2-24                                                 --                        (recursive)\n",
              "        ActQuantProxyFromInjector: 3-36                            --                        (recursive)\n",
              "Sequential: 1-22                                                     [1, 1, 64, 12]            1\n",
              "    QuantConvTranspose2d: 2-25                                      [1, 1, 64, 12]            1,154\n",
              "        ActQuantProxyFromInjector: 3-37                            [1, 128, 32, 6]           1\n",
              "Sequential: 1-27                                                     --                        (recursive)\n",
              "    QuantSigmoid: 2-36                                              --                        (recursive)\n",
              "        ActQuantProxyFromInjector: 3-55                            --                        (recursive)\n",
              "Sequential: 1-28                                                     --                        (recursive)\n",
              "    QuantConvTranspose2d: 2-29                                      --                        (recursive)\n",
              "        ActQuantProxyFromInjector: 3-39                            --                        (recursive)\n",
              "        WeightQuantProxyFromInjector: 3-40                         [128, 1, 3, 3]            1,152\n",
              "        BiasQuantProxyFromInjector: 3-41                           [1]                       --\n",
              "        ActQuantProxyFromInjector: 3-42                            [1, 1, 64, 12]            1\n",
              "Sequential: 1-27                                                     --                        (recursive)\n",
              "    QuantSigmoid: 2-36                                              --                        (recursive)\n",
              "        ActQuantProxyFromInjector: 3-55                            --                        (recursive)\n",
              "Sequential: 1-28                                                     --                        (recursive)\n",
              "    QuantConvTranspose2d: 2-29                                      --                        (recursive)\n",
              "        ActQuantProxyFromInjector: 3-44                            --                        (recursive)\n",
              "    QuantReLU: 2-30                                                 [1, 1, 64, 12]            --\n",
              "        ActQuantProxyFromInjector: 3-45                            [1, 1, 64, 12]            --\n",
              "        ActQuantProxyFromInjector: 3-46                            [1, 1, 64, 12]            1\n",
              "Sequential: 1-27                                                     --                        (recursive)\n",
              "    QuantSigmoid: 2-36                                              --                        (recursive)\n",
              "        ActQuantProxyFromInjector: 3-55                            --                        (recursive)\n",
              "Sequential: 1-28                                                     --                        (recursive)\n",
              "    QuantReLU: 2-32                                                 --                        (recursive)\n",
              "        ActQuantProxyFromInjector: 3-48                            --                        (recursive)\n",
              "Sequential: 1-29                                                     [1, 1, 64, 12]            --\n",
              "    QuantConv2d: 2-33                                               [1, 1, 64, 12]            11\n",
              "        ActQuantProxyFromInjector: 3-49                            [1, 1, 64, 12]            1\n",
              "    QuantSigmoid: 2-36                                              --                        (recursive)\n",
              "        ActQuantProxyFromInjector: 3-55                            --                        (recursive)\n",
              "    QuantConv2d: 2-37                                               --                        (recursive)\n",
              "        ActQuantProxyFromInjector: 3-51                            --                        (recursive)\n",
              "        WeightQuantProxyFromInjector: 3-52                         [1, 1, 3, 3]              9\n",
              "        BiasQuantProxyFromInjector: 3-53                           [1]                       --\n",
              "        ActQuantProxyFromInjector: 3-54                            [1, 1, 64, 12]            1\n",
              "    QuantSigmoid: 2-36                                              --                        (recursive)\n",
              "        ActQuantProxyFromInjector: 3-55                            --                        (recursive)\n",
              "    QuantConv2d: 2-37                                               --                        (recursive)\n",
              "        ActQuantProxyFromInjector: 3-56                            --                        (recursive)\n",
              "    QuantSigmoid: 2-38                                              [1, 1, 64, 12]            --\n",
              "        ActQuantProxyFromInjector: 3-57                            [1, 1, 64, 12]            --\n",
              "        ActQuantProxyFromInjector: 3-58                            [1, 1, 64, 12]            1\n",
              "========================================================================================================================\n",
              "Total params: 299,884\n",
              "Trainable params: 299,884\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 0\n",
              "========================================================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.00\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.00\n",
              "========================================================================================================================"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(RT_AutoEncoder, input_size=(1, 1, 64, 12))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1ZruqJ2eENx",
        "outputId": "6ae1b6d4-3e22-4d7d-ef7e-943df657a280"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Conv2d                                   [1, 1, 64, 12]            1,152\n",
              "==========================================================================================\n",
              "Total params: 1,152\n",
              "Trainable params: 1,152\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 0.88\n",
              "==========================================================================================\n",
              "Input size (MB): 0.39\n",
              "Forward/backward pass size (MB): 0.01\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.40\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(conv, input_size=(1, 128, 64, 12))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4ax7UjTemls",
        "outputId": "b39a316e-6b40-4b16-ceaf-ee43ba4e765b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "ConvTranspose2d                          [1, 1, 64, 12]            1,152\n",
              "==========================================================================================\n",
              "Total params: 1,152\n",
              "Trainable params: 1,152\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 0.88\n",
              "==========================================================================================\n",
              "Input size (MB): 0.10\n",
              "Forward/backward pass size (MB): 0.01\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.11\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(deconv, input_size=(1, 128, 32, 6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKL-mkkmeDsn"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "    # Conv benchmark\n",
        "    conv_benchmark = timeit.timeit(\"conv(conv_input)\", globals=locals(), number=NUMBER_OF_TIMINGS)\n",
        "\n",
        "    # Deconv benchmark\n",
        "    deconv_benchmark = timeit.timeit(\"deconv(deconv_input)\", globals=locals(), number=NUMBER_OF_TIMINGS)\n",
        "\n",
        "    # Display benchmarks\n",
        "    print(f\"Conv Benchmark:   {conv_benchmark:<5f} / {NUMBER_OF_TIMINGS} = {conv_benchmark/NUMBER_OF_TIMINGS:<5f}\")\n",
        "    print(f\"Deconv Benchmark: {deconv_benchmark:<5f} / {NUMBER_OF_TIMINGS} = {deconv_benchmark/NUMBER_OF_TIMINGS:<5f}\")\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjpFzU-eXXQk"
      },
      "source": [
        "**Results**\\\n",
        "1.\\\n",
        "Conv Benchmark:   34.695 / 10000 = 0.003469\\\n",
        "Deconv Benchmark: 30.531 / 10000 = 0.00305\\\n",
        "2.\n",
        "Conv Benchmark:   33.846728195000196 / 10000 = 0.003385\\\n",
        "Deconv Benchmark: 31.313208005999968 / 10000 = 0.003131\\\n",
        "3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMS9F4eA5eej"
      },
      "source": [
        "# **Test Against Threats**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dxR_-5xVMkoD",
        "outputId": "735bc964-b5cf-4f6f-ee64-c79eab1017fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.8171, device='cuda:0')\n",
            "IntQuantTensor(value=tensor([[[[-1.,  1.,  1.,  ..., -1., -1., -1.],\n",
            "          [ 1.,  1.,  1.,  ..., -1.,  1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          ...,\n",
            "          [-1., -1., -1.,  ...,  1., -1., -1.],\n",
            "          [-1., -1.,  1.,  ..., -1., -1., -1.],\n",
            "          [ 1., -1., -1.,  ..., -1., -1., -1.]]],\n",
            "\n",
            "\n",
            "        [[[-1.,  1.,  1.,  ..., -1., -1., -1.],\n",
            "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          ...,\n",
            "          [ 1.,  1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1.,  1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
            "\n",
            "\n",
            "        [[[-1., -1., -1.,  ...,  1., -1., -1.],\n",
            "          [-1., -1., -1.,  ...,  1., -1., -1.],\n",
            "          [ 1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          ...,\n",
            "          [-1., -1.,  1.,  ...,  1., -1., -1.],\n",
            "          [ 1., -1.,  1.,  ..., -1.,  1., -1.],\n",
            "          [-1., -1., -1.,  ...,  1.,  1., -1.]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-1.,  1.,  1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ...,  1., -1., -1.],\n",
            "          [-1., -1., -1.,  ...,  1., -1., -1.],\n",
            "          ...,\n",
            "          [-1., -1., -1.,  ...,  1., -1., -1.],\n",
            "          [-1., -1.,  1.,  ...,  1., -1., -1.],\n",
            "          [-1., -1., -1.,  ...,  1., -1., -1.]]],\n",
            "\n",
            "\n",
            "        [[[ 1.,  1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [ 1.,  1., -1.,  ...,  1., -1., -1.],\n",
            "          ...,\n",
            "          [ 1., -1., -1.,  ...,  1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
            "\n",
            "\n",
            "        [[[-1., -1., -1.,  ..., -1.,  1., -1.],\n",
            "          [-1., -1., -1.,  ...,  1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1.,  1., -1.],\n",
            "          ...,\n",
            "          [-1., -1., -1.,  ...,  1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1.,  1., -1.],\n",
            "          [ 1.,  1., -1.,  ...,  1., -1., -1.]]]], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=0.0, bit_width=1.0, signed_t=True, training_t=True)\n",
            "tensor(0.6777, device='cuda:0')\n",
            "IntQuantTensor(value=tensor([[[[ 1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [ 1.,  1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1.,  1., -1.,  ..., -1., -1., -1.],\n",
            "          ...,\n",
            "          [-1., -1., -1.,  ...,  1., -1., -1.],\n",
            "          [ 1., -1., -1.,  ...,  1., -1., -1.],\n",
            "          [-1., -1., -1.,  ...,  1., -1., -1.]]],\n",
            "\n",
            "\n",
            "        [[[ 1.,  1., -1.,  ...,  1.,  1., -1.],\n",
            "          [-1.,  1., -1.,  ..., -1.,  1., -1.],\n",
            "          [-1., -1.,  1.,  ..., -1., -1., -1.],\n",
            "          ...,\n",
            "          [ 1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ...,  1., -1., -1.],\n",
            "          [-1., -1., -1.,  ...,  1., -1., -1.]]],\n",
            "\n",
            "\n",
            "        [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [ 1.,  1.,  1.,  ...,  1., -1., -1.],\n",
            "          [-1., -1.,  1.,  ...,  1.,  1., -1.],\n",
            "          ...,\n",
            "          [ 1., -1., -1.,  ...,  1., -1., -1.],\n",
            "          [ 1., -1.,  1.,  ..., -1.,  1., -1.],\n",
            "          [-1.,  1., -1.,  ..., -1., -1., -1.]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-1., -1., -1.,  ...,  1., -1., -1.],\n",
            "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ...,  1., -1., -1.],\n",
            "          ...,\n",
            "          [-1., -1., -1.,  ...,  1., -1., -1.],\n",
            "          [-1., -1., -1.,  ...,  1., -1., -1.],\n",
            "          [ 1., -1., -1.,  ...,  1., -1., -1.]]],\n",
            "\n",
            "\n",
            "        [[[ 1.,  1.,  1.,  ...,  1., -1., -1.],\n",
            "          [-1.,  1., -1.,  ...,  1., -1., -1.],\n",
            "          [-1., -1., -1.,  ...,  1., -1., -1.],\n",
            "          ...,\n",
            "          [ 1.,  1.,  1.,  ...,  1., -1., -1.],\n",
            "          [-1., -1., -1.,  ...,  1., -1., -1.],\n",
            "          [-1., -1., -1.,  ...,  1., -1., -1.]]],\n",
            "\n",
            "\n",
            "        [[[-1., -1., -1.,  ..., -1.,  1., -1.],\n",
            "          [-1., -1., -1.,  ...,  1., -1., -1.],\n",
            "          [-1., -1., -1.,  ...,  1., -1., -1.],\n",
            "          ...,\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1.,  1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1.,  1., -1.]]]], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=0.0, bit_width=1.0, signed_t=True, training_t=True)\n"
          ]
        }
      ],
      "source": [
        "how_many = 1500\n",
        "it = iter(Train_DoS_DL)\n",
        "for i in range(how_many):\n",
        "    X = next(it)[0].reshape((-1, 1, IMAGE_DEPTH, 12))\n",
        "X = (X * 2) - 1.0\n",
        "\n",
        "\n",
        "nor_it = iter(Train_Normal_DL)\n",
        "for i in range(how_many):\n",
        "    XNor = next(nor_it)[0].reshape((-1, 1, IMAGE_DEPTH, 12))\n",
        "XNor = (XNor * 2) - 1.0\n",
        "\n",
        "# print(X)\n",
        "with torch.no_grad():\n",
        "    pred = AutoEncoder(X)\n",
        "    loss = loss_fn(pred, X)\n",
        "    print(loss)\n",
        "    print(pred)\n",
        "\n",
        "    pred = AutoEncoder(XNor)\n",
        "    loss = loss_fn(pred, XNor)\n",
        "    print(loss)\n",
        "    print(pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjMxmS-k5hym"
      },
      "outputs": [],
      "source": [
        "def test_against_anomailes(dataloader, model, loss_fn):\n",
        "    size = len(dataloader)\n",
        "\n",
        "    num_below_threshold = 0\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch, (X, labels, cls) in enumerate(dataloader):\n",
        "            if (len(X) != batch_size) and (len(X) != train_batch_size):\n",
        "                print(f\"Batch size warning! [{X.shape}]\")\n",
        "                continue\n",
        "            if torch.sum(labels) == 0:\n",
        "                continue\n",
        "\n",
        "            # Compute prediction and loss\n",
        "            X = torch.reshape(X, (-1, 1, 64, 12))\n",
        "            X = (X * 2) - 1.0\n",
        "            pred = model(X)\n",
        "\n",
        "            loss = loss_fn(pred, X)\n",
        "\n",
        "\n",
        "            if loss < RECONSTRUCTION_THRESHOLD:\n",
        "                num_below_threshold += 1\n",
        "                # print(f\"Num Below Threshold = {num_below_threshold}/{batch}\")\n",
        "\n",
        "            if batch % 100 == 0:\n",
        "                loss = loss.item()\n",
        "                current = batch * batch_size + len(X)\n",
        "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}] <-> Accuracy: {(num_below_threshold/batch) * 100}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "5vK4RiHa5zbu",
        "outputId": "b75ad1fd-4c23-43b4-f2de-de249194753f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss: 0.697917  [ 1501/2932552] <-> Accuracy: 2.8000000000000003%\n",
            "loss: 0.781250  [ 1601/2932552] <-> Accuracy: 2.75%\n",
            "loss: 0.848958  [ 1701/2932552] <-> Accuracy: 3.4705882352941178%\n",
            "loss: 0.770833  [ 1801/2932552] <-> Accuracy: 3.722222222222222%\n",
            "loss: 0.734375  [ 1901/2932552] <-> Accuracy: 3.684210526315789%\n",
            "loss: 0.833333  [ 2001/2932552] <-> Accuracy: 3.5000000000000004%\n",
            "loss: 0.864583  [ 2101/2932552] <-> Accuracy: 3.5238095238095237%\n",
            "loss: 0.869792  [ 2201/2932552] <-> Accuracy: 3.5454545454545454%\n",
            "loss: 0.760417  [ 2301/2932552] <-> Accuracy: 3.956521739130435%\n",
            "loss: 0.859375  [ 2401/2932552] <-> Accuracy: 3.875%\n",
            "loss: 0.880208  [ 2501/2932552] <-> Accuracy: 3.7199999999999998%\n",
            "loss: 0.776042  [ 2601/2932552] <-> Accuracy: 3.6923076923076925%\n",
            "loss: 0.932292  [ 2701/2932552] <-> Accuracy: 3.7777777777777777%\n",
            "loss: 0.802083  [ 2801/2932552] <-> Accuracy: 3.75%\n",
            "loss: 0.848958  [ 2901/2932552] <-> Accuracy: 3.689655172413793%\n",
            "loss: 0.828125  [ 3001/2932552] <-> Accuracy: 3.833333333333333%\n",
            "loss: 0.885417  [ 3101/2932552] <-> Accuracy: 3.741935483870968%\n",
            "loss: 0.776042  [ 3201/2932552] <-> Accuracy: 3.78125%\n",
            "loss: 0.776042  [ 3301/2932552] <-> Accuracy: 4.03030303030303%\n",
            "loss: 0.833333  [ 3401/2932552] <-> Accuracy: 4.235294117647059%\n",
            "loss: 0.718750  [ 3501/2932552] <-> Accuracy: 4.3999999999999995%\n",
            "loss: 0.802083  [ 3601/2932552] <-> Accuracy: 4.472222222222221%\n",
            "loss: 0.822917  [ 3701/2932552] <-> Accuracy: 4.351351351351351%\n",
            "loss: 0.828125  [ 3801/2932552] <-> Accuracy: 4.447368421052632%\n",
            "loss: 0.697917  [ 3901/2932552] <-> Accuracy: 4.435897435897436%\n",
            "loss: 0.848958  [ 4001/2932552] <-> Accuracy: 4.55%\n",
            "loss: 0.744792  [ 4101/2932552] <-> Accuracy: 4.780487804878049%\n",
            "loss: 0.833333  [ 4201/2932552] <-> Accuracy: 4.690476190476191%\n",
            "loss: 0.817708  [ 4301/2932552] <-> Accuracy: 4.5813953488372094%\n",
            "loss: 0.781250  [ 4401/2932552] <-> Accuracy: 4.477272727272727%\n",
            "loss: 0.796875  [ 4501/2932552] <-> Accuracy: 4.3999999999999995%\n",
            "loss: 0.833333  [ 4601/2932552] <-> Accuracy: 4.304347826086956%\n",
            "loss: 0.854167  [ 4701/2932552] <-> Accuracy: 4.319148936170213%\n",
            "loss: 0.776042  [ 4801/2932552] <-> Accuracy: 4.3125%\n",
            "loss: 0.833333  [ 4901/2932552] <-> Accuracy: 4.224489795918367%\n",
            "loss: 0.833333  [ 5001/2932552] <-> Accuracy: 4.2%\n",
            "loss: 0.744792  [ 5101/2932552] <-> Accuracy: 4.372549019607843%\n",
            "loss: 0.817708  [ 5201/2932552] <-> Accuracy: 4.538461538461538%\n",
            "loss: 0.786458  [ 5301/2932552] <-> Accuracy: 4.566037735849057%\n",
            "loss: 0.755208  [ 5401/2932552] <-> Accuracy: 4.592592592592593%\n",
            "loss: 0.781250  [ 5501/2932552] <-> Accuracy: 4.763636363636364%\n",
            "loss: 0.864583  [ 5601/2932552] <-> Accuracy: 4.875%\n",
            "loss: 0.802083  [ 5701/2932552] <-> Accuracy: 4.824561403508771%\n",
            "loss: 0.822917  [ 5801/2932552] <-> Accuracy: 4.758620689655172%\n",
            "loss: 0.760417  [ 5901/2932552] <-> Accuracy: 4.779661016949153%\n",
            "loss: 0.770833  [ 6001/2932552] <-> Accuracy: 4.9%\n",
            "loss: 0.734375  [ 6101/2932552] <-> Accuracy: 5.081967213114754%\n",
            "loss: 0.718750  [ 6201/2932552] <-> Accuracy: 5.209677419354838%\n",
            "loss: 0.708333  [ 6301/2932552] <-> Accuracy: 5.222222222222222%\n",
            "loss: 0.786458  [ 6401/2932552] <-> Accuracy: 5.484375%\n",
            "loss: 0.796875  [ 6501/2932552] <-> Accuracy: 5.476923076923077%\n",
            "loss: 0.708333  [ 6601/2932552] <-> Accuracy: 5.409090909090909%\n",
            "loss: 0.885417  [ 6701/2932552] <-> Accuracy: 5.537313432835821%\n",
            "loss: 0.843750  [ 6801/2932552] <-> Accuracy: 5.617647058823529%\n",
            "loss: 0.786458  [ 6901/2932552] <-> Accuracy: 5.768115942028985%\n",
            "loss: 0.911458  [ 7001/2932552] <-> Accuracy: 5.800000000000001%\n",
            "loss: 0.791667  [ 7101/2932552] <-> Accuracy: 5.859154929577465%\n",
            "loss: 0.739583  [ 7201/2932552] <-> Accuracy: 5.875%\n",
            "loss: 0.848958  [ 7301/2932552] <-> Accuracy: 5.821917808219178%\n",
            "loss: 0.760417  [ 7401/2932552] <-> Accuracy: 5.918918918918918%\n",
            "loss: 0.796875  [ 7501/2932552] <-> Accuracy: 5.946666666666667%\n",
            "loss: 0.796875  [ 7601/2932552] <-> Accuracy: 5.921052631578947%\n",
            "loss: 0.697917  [ 7701/2932552] <-> Accuracy: 6.116883116883117%\n",
            "loss: 0.838542  [ 7801/2932552] <-> Accuracy: 6.153846153846154%\n",
            "loss: 0.739583  [ 7901/2932552] <-> Accuracy: 6.3544303797468356%\n",
            "loss: 0.786458  [ 8001/2932552] <-> Accuracy: 6.5375000000000005%\n",
            "loss: 0.677083  [ 8101/2932552] <-> Accuracy: 6.617283950617284%\n",
            "loss: 0.838542  [ 8201/2932552] <-> Accuracy: 6.585365853658537%\n",
            "loss: 0.906250  [ 8301/2932552] <-> Accuracy: 6.554216867469879%\n",
            "loss: 0.796875  [ 8401/2932552] <-> Accuracy: 6.476190476190475%\n",
            "loss: 0.692708  [13901/2932552] <-> Accuracy: 4.503597122302158%\n",
            "loss: 0.875000  [14001/2932552] <-> Accuracy: 4.521428571428571%\n",
            "loss: 0.916667  [14101/2932552] <-> Accuracy: 4.48936170212766%\n",
            "loss: 0.796875  [14201/2932552] <-> Accuracy: 4.471830985915493%\n",
            "loss: 0.713542  [14301/2932552] <-> Accuracy: 4.48951048951049%\n",
            "loss: 0.848958  [14401/2932552] <-> Accuracy: 4.611111111111111%\n",
            "loss: 0.708333  [14501/2932552] <-> Accuracy: 4.6482758620689655%\n",
            "loss: 0.765625  [14601/2932552] <-> Accuracy: 4.801369863013698%\n",
            "loss: 0.901042  [14701/2932552] <-> Accuracy: 4.8979591836734695%\n",
            "loss: 0.786458  [14801/2932552] <-> Accuracy: 4.993243243243244%\n",
            "loss: 0.859375  [14901/2932552] <-> Accuracy: 5.201342281879195%\n",
            "loss: 0.796875  [15001/2932552] <-> Accuracy: 5.286666666666666%\n",
            "loss: 0.812500  [15101/2932552] <-> Accuracy: 5.470198675496689%\n",
            "loss: 0.671875  [15201/2932552] <-> Accuracy: 5.611842105263158%\n",
            "loss: 0.781250  [15301/2932552] <-> Accuracy: 5.718954248366013%\n",
            "loss: 0.817708  [15401/2932552] <-> Accuracy: 5.7727272727272725%\n",
            "loss: 0.817708  [15501/2932552] <-> Accuracy: 5.812903225806452%\n",
            "loss: 0.729167  [15601/2932552] <-> Accuracy: 5.801282051282051%\n",
            "loss: 0.817708  [15701/2932552] <-> Accuracy: 5.89171974522293%\n",
            "loss: 0.901042  [15801/2932552] <-> Accuracy: 5.949367088607595%\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-130d83298a4c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_against_anomailes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_DoS_DS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-0bb87b82ab57>\u001b[0m in \u001b[0;36mtest_against_anomailes\u001b[0;34m(dataloader, model, loss_fn)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-78c7b7445c40>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUpsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nearest'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/brevitas/nn/quant_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQuantTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQuantTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_weight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_bias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/brevitas/nn/quant_layer.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquant_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mquant_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_quant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquant_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     \u001b[0;31m# fmt: off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1740\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1741\u001b[0;31m         \u001b[0mforward_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1742\u001b[0m         \u001b[0;31m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0;31m# this function, and just call forward.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "test_against_anomailes(Train_DoS_DS, AutoEncoder, loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "a7KsSMwfHtLB",
        "outputId": "c1f050a1-e303-4269-af80-8b77d7d862e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss: 0.932292  [ 1601/3071024] <-> Accuracy: 1.25%\n",
            "loss: 0.833333  [ 1701/3071024] <-> Accuracy: 1.1764705882352942%\n",
            "loss: 0.932292  [ 1801/3071024] <-> Accuracy: 1.1111111111111112%\n",
            "loss: 0.859375  [ 1901/3071024] <-> Accuracy: 1.0526315789473684%\n",
            "loss: 0.958333  [ 2001/3071024] <-> Accuracy: 1.0%\n",
            "loss: 1.010417  [ 2101/3071024] <-> Accuracy: 0.9523809523809524%\n",
            "loss: 1.031250  [ 2201/3071024] <-> Accuracy: 0.9090909090909091%\n",
            "loss: 1.000000  [ 2301/3071024] <-> Accuracy: 0.8695652173913043%\n",
            "loss: 1.125000  [ 2401/3071024] <-> Accuracy: 0.8333333333333334%\n",
            "loss: 1.192708  [ 2501/3071024] <-> Accuracy: 0.8%\n",
            "loss: 1.161458  [ 2601/3071024] <-> Accuracy: 0.7692307692307693%\n",
            "loss: 1.088542  [ 2701/3071024] <-> Accuracy: 0.7407407407407408%\n",
            "loss: 0.989583  [ 2801/3071024] <-> Accuracy: 0.7142857142857143%\n",
            "loss: 0.979167  [ 2901/3071024] <-> Accuracy: 0.6896551724137931%\n",
            "loss: 1.072917  [ 3001/3071024] <-> Accuracy: 0.6666666666666667%\n",
            "loss: 0.921875  [ 3101/3071024] <-> Accuracy: 0.6451612903225806%\n",
            "loss: 0.572917  [ 8901/3071024] <-> Accuracy: 0.5280898876404494%\n",
            "loss: 0.864583  [ 9001/3071024] <-> Accuracy: 0.8444444444444443%\n",
            "loss: 0.989583  [ 9101/3071024] <-> Accuracy: 0.8351648351648353%\n",
            "loss: 0.916667  [ 9201/3071024] <-> Accuracy: 0.826086956521739%\n",
            "loss: 0.958333  [ 9301/3071024] <-> Accuracy: 0.8172043010752689%\n",
            "loss: 0.848958  [ 9401/3071024] <-> Accuracy: 0.8085106382978723%\n",
            "loss: 1.000000  [ 9501/3071024] <-> Accuracy: 0.8%\n",
            "loss: 0.942708  [ 9601/3071024] <-> Accuracy: 0.7916666666666667%\n",
            "loss: 1.057292  [ 9701/3071024] <-> Accuracy: 0.7835051546391752%\n",
            "loss: 0.973958  [ 9801/3071024] <-> Accuracy: 0.7755102040816326%\n",
            "loss: 0.890625  [ 9901/3071024] <-> Accuracy: 0.7676767676767676%\n",
            "loss: 1.046875  [10001/3071024] <-> Accuracy: 0.76%\n",
            "loss: 0.973958  [10101/3071024] <-> Accuracy: 0.7524752475247525%\n",
            "loss: 1.067708  [10201/3071024] <-> Accuracy: 0.7450980392156863%\n",
            "loss: 0.963542  [10301/3071024] <-> Accuracy: 0.7378640776699029%\n",
            "loss: 1.031250  [10401/3071024] <-> Accuracy: 0.7307692307692307%\n",
            "loss: 1.109375  [10501/3071024] <-> Accuracy: 0.7238095238095238%\n",
            "loss: 0.661458  [10601/3071024] <-> Accuracy: 0.7358490566037735%\n",
            "loss: 0.609375  [16201/3071024] <-> Accuracy: 0.5246913580246914%\n",
            "loss: 0.947917  [16301/3071024] <-> Accuracy: 0.6441717791411044%\n",
            "loss: 0.901042  [16401/3071024] <-> Accuracy: 0.6402439024390244%\n",
            "loss: 0.901042  [16501/3071024] <-> Accuracy: 0.6363636363636364%\n",
            "loss: 0.937500  [16601/3071024] <-> Accuracy: 0.6325301204819277%\n",
            "loss: 0.958333  [16701/3071024] <-> Accuracy: 0.6287425149700598%\n",
            "loss: 1.015625  [16801/3071024] <-> Accuracy: 0.625%\n",
            "loss: 0.901042  [16901/3071024] <-> Accuracy: 0.621301775147929%\n",
            "loss: 1.067708  [17001/3071024] <-> Accuracy: 0.6176470588235293%\n",
            "loss: 0.989583  [17101/3071024] <-> Accuracy: 0.6140350877192983%\n",
            "loss: 0.916667  [17201/3071024] <-> Accuracy: 0.6104651162790697%\n",
            "loss: 0.921875  [17301/3071024] <-> Accuracy: 0.6069364161849711%\n",
            "loss: 0.916667  [17401/3071024] <-> Accuracy: 0.603448275862069%\n",
            "loss: 1.000000  [17501/3071024] <-> Accuracy: 0.6%\n",
            "loss: 1.020833  [17601/3071024] <-> Accuracy: 0.5965909090909092%\n",
            "loss: 1.098958  [17701/3071024] <-> Accuracy: 0.5932203389830508%\n",
            "loss: 0.973958  [17801/3071024] <-> Accuracy: 0.5898876404494382%\n",
            "loss: 1.015625  [17901/3071024] <-> Accuracy: 0.5865921787709497%\n",
            "loss: 1.000000  [22201/3071024] <-> Accuracy: 0.581081081081081%\n",
            "loss: 1.109375  [22301/3071024] <-> Accuracy: 0.57847533632287%\n",
            "loss: 0.989583  [22401/3071024] <-> Accuracy: 0.5758928571428571%\n",
            "loss: 0.984375  [22501/3071024] <-> Accuracy: 0.5733333333333334%\n",
            "loss: 0.994792  [22601/3071024] <-> Accuracy: 0.5707964601769911%\n",
            "loss: 1.057292  [22701/3071024] <-> Accuracy: 0.5682819383259912%\n",
            "loss: 1.020833  [22801/3071024] <-> Accuracy: 0.5657894736842106%\n",
            "loss: 1.031250  [22901/3071024] <-> Accuracy: 0.5633187772925764%\n",
            "loss: 1.130208  [23001/3071024] <-> Accuracy: 0.5608695652173913%\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-099f9cc0243a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_against_anomailes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_Fuzzy_DS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-0bb87b82ab57>\u001b[0m in \u001b[0;36mtest_against_anomailes\u001b[0;34m(dataloader, model, loss_fn)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-78c7b7445c40>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# Decoder:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUpsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nearest'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUpsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nearest'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/brevitas/nn/quant_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQuantTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQuantTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_weight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_bias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/brevitas/nn/quant_layer.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquant_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mquant_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_quant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquant_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/brevitas/proxy/runtime_quant.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfused_activation_quant_proxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;31m# If y is an empty QuantTensor, we need to check if this is a passthrough proxy,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;31m# otherwise return a simple Tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/brevitas/proxy/runtime_quant.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_quant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/brevitas/core/quant/binary.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_sign_ste\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelay_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbit_width\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/brevitas/core/utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mbrevitas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscript_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     def _load_from_state_dict(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "test_against_anomailes(Train_Fuzzy_DS, AutoEncoder, loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EU79a3cxHusN",
        "outputId": "f5d88593-1d8e-46f8-bf7d-361140d808fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss: 0.744792  [ 2101/3554449] <-> Accuracy: 0.9523809523809524%\n",
            "loss: 0.682292  [ 2201/3554449] <-> Accuracy: 1.7727272727272727%\n",
            "loss: 0.828125  [ 2301/3554449] <-> Accuracy: 1.7826086956521738%\n",
            "loss: 0.802083  [ 2401/3554449] <-> Accuracy: 1.7916666666666667%\n",
            "loss: 0.828125  [ 2501/3554449] <-> Accuracy: 1.8800000000000001%\n",
            "loss: 0.661458  [ 2601/3554449] <-> Accuracy: 2.423076923076923%\n",
            "loss: 0.890625  [ 2701/3554449] <-> Accuracy: 2.5925925925925926%\n",
            "loss: 0.859375  [ 2801/3554449] <-> Accuracy: 2.5357142857142856%\n",
            "loss: 0.895833  [ 2901/3554449] <-> Accuracy: 2.5517241379310347%\n",
            "loss: 0.864583  [ 3001/3554449] <-> Accuracy: 2.566666666666667%\n",
            "loss: 0.807292  [ 3101/3554449] <-> Accuracy: 2.612903225806452%\n",
            "loss: 0.822917  [ 3201/3554449] <-> Accuracy: 2.71875%\n",
            "loss: 0.848958  [ 3301/3554449] <-> Accuracy: 2.696969696969697%\n",
            "loss: 0.895833  [ 3401/3554449] <-> Accuracy: 2.911764705882353%\n",
            "loss: 0.947917  [ 3501/3554449] <-> Accuracy: 2.8857142857142857%\n",
            "loss: 0.864583  [ 3601/3554449] <-> Accuracy: 2.861111111111111%\n",
            "loss: 0.796875  [ 3701/3554449] <-> Accuracy: 3.2432432432432434%\n",
            "loss: 0.734375  [ 3801/3554449] <-> Accuracy: 4.2105263157894735%\n",
            "loss: 0.744792  [ 3901/3554449] <-> Accuracy: 4.538461538461538%\n",
            "loss: 0.864583  [ 4001/3554449] <-> Accuracy: 4.5%\n",
            "loss: 0.739583  [ 4101/3554449] <-> Accuracy: 4.951219512195122%\n",
            "loss: 0.765625  [ 4201/3554449] <-> Accuracy: 5.3809523809523805%\n",
            "loss: 0.734375  [ 4301/3554449] <-> Accuracy: 6.186046511627907%\n",
            "loss: 0.807292  [ 4401/3554449] <-> Accuracy: 6.318181818181818%\n",
            "loss: 0.843750  [ 4501/3554449] <-> Accuracy: 6.2%\n",
            "loss: 0.817708  [ 4601/3554449] <-> Accuracy: 6.565217391304349%\n",
            "loss: 0.692708  [ 4701/3554449] <-> Accuracy: 7.297872340425532%\n",
            "loss: 0.755208  [ 4801/3554449] <-> Accuracy: 7.541666666666667%\n",
            "loss: 0.713542  [ 4901/3554449] <-> Accuracy: 8.408163265306122%\n",
            "loss: 0.822917  [ 5001/3554449] <-> Accuracy: 8.3%\n",
            "loss: 0.916667  [ 5101/3554449] <-> Accuracy: 8.15686274509804%\n",
            "loss: 0.802083  [ 5201/3554449] <-> Accuracy: 8.557692307692308%\n",
            "loss: 0.682292  [ 5301/3554449] <-> Accuracy: 9.283018867924527%\n",
            "loss: 0.848958  [ 5401/3554449] <-> Accuracy: 9.25925925925926%\n",
            "loss: 0.781250  [ 5501/3554449] <-> Accuracy: 9.309090909090909%\n",
            "loss: 0.770833  [ 5601/3554449] <-> Accuracy: 9.25%\n",
            "loss: 0.671875  [ 5701/3554449] <-> Accuracy: 9.403508771929825%\n",
            "loss: 0.869792  [ 5801/3554449] <-> Accuracy: 9.344827586206897%\n",
            "loss: 0.848958  [ 5901/3554449] <-> Accuracy: 9.203389830508476%\n",
            "loss: 0.718750  [ 6001/3554449] <-> Accuracy: 9.316666666666666%\n",
            "loss: 0.927083  [ 6101/3554449] <-> Accuracy: 9.245901639344261%\n",
            "loss: 0.713542  [ 6201/3554449] <-> Accuracy: 9.32258064516129%\n",
            "loss: 0.812500  [ 6301/3554449] <-> Accuracy: 9.380952380952381%\n",
            "loss: 0.828125  [ 6401/3554449] <-> Accuracy: 9.328125%\n",
            "loss: 0.760417  [ 6501/3554449] <-> Accuracy: 9.307692307692307%\n",
            "loss: 0.911458  [ 6601/3554449] <-> Accuracy: 9.378787878787879%\n",
            "loss: 0.859375  [ 6701/3554449] <-> Accuracy: 9.298507462686567%\n",
            "loss: 0.906250  [ 6801/3554449] <-> Accuracy: 9.235294117647058%\n",
            "loss: 0.838542  [ 6901/3554449] <-> Accuracy: 9.130434782608695%\n",
            "loss: 0.723958  [ 7001/3554449] <-> Accuracy: 9.171428571428573%\n",
            "loss: 0.713542  [ 7101/3554449] <-> Accuracy: 9.450704225352112%\n",
            "loss: 0.776042  [ 7201/3554449] <-> Accuracy: 9.680555555555555%\n",
            "loss: 0.859375  [ 7301/3554449] <-> Accuracy: 9.63013698630137%\n",
            "loss: 0.807292  [ 7401/3554449] <-> Accuracy: 9.77027027027027%\n",
            "loss: 0.703125  [ 7501/3554449] <-> Accuracy: 9.893333333333333%\n",
            "loss: 0.791667  [ 7601/3554449] <-> Accuracy: 9.973684210526315%\n",
            "loss: 0.875000  [ 7701/3554449] <-> Accuracy: 10.03896103896104%\n",
            "loss: 0.755208  [ 7801/3554449] <-> Accuracy: 9.948717948717949%\n",
            "loss: 0.739583  [ 7901/3554449] <-> Accuracy: 9.974683544303797%\n",
            "loss: 0.848958  [ 8001/3554449] <-> Accuracy: 10.0875%\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-7fed52f6d381>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_against_anomailes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_Gear_DS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-0bb87b82ab57>\u001b[0m in \u001b[0;36mtest_against_anomailes\u001b[0;34m(dataloader, model, loss_fn)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-78c7b7445c40>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUpsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nearest'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/brevitas/nn/quant_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQuantTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQuantTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_weight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_bias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/brevitas/nn/quant_layer.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mquant_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_quant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mquant_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquant_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/brevitas/proxy/runtime_quant.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfused_activation_quant_proxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;31m# If y is an empty QuantTensor, we need to check if this is a passthrough proxy,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;31m# otherwise return a simple Tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/brevitas/proxy/runtime_quant.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_quant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/brevitas/core/quant/binary.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaling_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_clamp_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_sign_ste\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelay_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbit_width\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/brevitas/function/ops_ste.py\u001b[0m in \u001b[0;36mbinary_sign_ste\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_sign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn_prefix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd_ste_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_sign_ste_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_are_functorch_transforms_active\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_functorch/utils.py\u001b[0m in \u001b[0;36munwrap_dead_wrappers\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# NB: doesn't use tree_map_only for performance reasons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     result = tuple(\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0munwrap_if_dead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "test_against_anomailes(Train_Gear_DS, AutoEncoder, loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sCnI4dL_HvSE",
        "outputId": "953c2ee8-24de-4f9a-e858-d0b10a6e9133"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss: 0.817708  [ 1701/3697297] <-> Accuracy: 1.411764705882353%\n",
            "loss: 0.750000  [ 1801/3697297] <-> Accuracy: 1.7777777777777777%\n",
            "loss: 0.729167  [ 1901/3697297] <-> Accuracy: 2.2631578947368425%\n",
            "loss: 0.703125  [ 2001/3697297] <-> Accuracy: 3.25%\n",
            "loss: 0.760417  [ 2101/3697297] <-> Accuracy: 4.0476190476190474%\n",
            "loss: 0.838542  [ 2201/3697297] <-> Accuracy: 4.2272727272727275%\n",
            "loss: 0.802083  [ 2301/3697297] <-> Accuracy: 4.6521739130434785%\n",
            "loss: 0.864583  [ 2401/3697297] <-> Accuracy: 5.0%\n",
            "loss: 0.770833  [ 2501/3697297] <-> Accuracy: 5.0%\n",
            "loss: 0.817708  [ 2601/3697297] <-> Accuracy: 4.846153846153846%\n",
            "loss: 0.864583  [ 2701/3697297] <-> Accuracy: 4.925925925925926%\n",
            "loss: 0.765625  [ 2801/3697297] <-> Accuracy: 5.857142857142858%\n",
            "loss: 0.723958  [ 2901/3697297] <-> Accuracy: 6.275862068965517%\n",
            "loss: 0.708333  [ 3001/3697297] <-> Accuracy: 7.066666666666667%\n",
            "loss: 0.791667  [ 3101/3697297] <-> Accuracy: 7.129032258064516%\n",
            "loss: 0.739583  [ 3201/3697297] <-> Accuracy: 7.124999999999999%\n",
            "loss: 0.848958  [ 3301/3697297] <-> Accuracy: 6.9393939393939394%\n",
            "loss: 0.734375  [ 3401/3697297] <-> Accuracy: 7.205882352941176%\n",
            "loss: 0.776042  [ 3501/3697297] <-> Accuracy: 7.514285714285713%\n",
            "loss: 0.765625  [ 3601/3697297] <-> Accuracy: 8.083333333333332%\n",
            "loss: 0.723958  [ 3701/3697297] <-> Accuracy: 8.108108108108109%\n",
            "loss: 0.880208  [ 3801/3697297] <-> Accuracy: 7.947368421052632%\n",
            "loss: 0.765625  [ 3901/3697297] <-> Accuracy: 8.282051282051283%\n",
            "loss: 0.817708  [ 4001/3697297] <-> Accuracy: 8.35%\n",
            "loss: 0.911458  [ 4101/3697297] <-> Accuracy: 8.219512195121952%\n",
            "loss: 0.781250  [ 4201/3697297] <-> Accuracy: 8.142857142857144%\n",
            "loss: 0.744792  [ 4301/3697297] <-> Accuracy: 8.116279069767442%\n",
            "loss: 0.822917  [ 4401/3697297] <-> Accuracy: 8.159090909090908%\n",
            "loss: 0.791667  [ 4501/3697297] <-> Accuracy: 8.11111111111111%\n",
            "loss: 0.770833  [ 4601/3697297] <-> Accuracy: 8.021739130434783%\n",
            "loss: 0.927083  [ 4701/3697297] <-> Accuracy: 7.851063829787234%\n",
            "loss: 0.739583  [ 4801/3697297] <-> Accuracy: 7.833333333333334%\n",
            "loss: 0.807292  [ 4901/3697297] <-> Accuracy: 7.6938775510204085%\n",
            "loss: 0.786458  [ 5001/3697297] <-> Accuracy: 7.779999999999999%\n",
            "loss: 0.713542  [ 5101/3697297] <-> Accuracy: 7.92156862745098%\n",
            "loss: 0.786458  [ 5201/3697297] <-> Accuracy: 8.057692307692307%\n",
            "loss: 0.791667  [ 5301/3697297] <-> Accuracy: 8.075471698113207%\n",
            "loss: 0.776042  [ 5401/3697297] <-> Accuracy: 8.296296296296296%\n",
            "loss: 0.781250  [ 5501/3697297] <-> Accuracy: 8.272727272727273%\n",
            "loss: 0.723958  [ 5601/3697297] <-> Accuracy: 8.535714285714285%\n",
            "loss: 0.755208  [ 5701/3697297] <-> Accuracy: 9.087719298245615%\n",
            "loss: 0.911458  [ 5801/3697297] <-> Accuracy: 9.06896551724138%\n",
            "loss: 0.739583  [ 5901/3697297] <-> Accuracy: 8.983050847457626%\n",
            "loss: 0.755208  [ 6001/3697297] <-> Accuracy: 9.383333333333335%\n",
            "loss: 0.718750  [ 6101/3697297] <-> Accuracy: 9.770491803278688%\n",
            "loss: 0.864583  [ 6201/3697297] <-> Accuracy: 9.903225806451612%\n",
            "loss: 0.802083  [ 6301/3697297] <-> Accuracy: 10.0%\n",
            "loss: 0.802083  [ 6401/3697297] <-> Accuracy: 9.90625%\n",
            "loss: 0.786458  [ 6501/3697297] <-> Accuracy: 9.876923076923077%\n",
            "loss: 0.885417  [ 6601/3697297] <-> Accuracy: 9.863636363636363%\n",
            "loss: 0.666667  [ 6701/3697297] <-> Accuracy: 9.805970149253731%\n",
            "loss: 0.812500  [ 6801/3697297] <-> Accuracy: 9.735294117647058%\n",
            "loss: 0.796875  [ 6901/3697297] <-> Accuracy: 9.652173913043478%\n",
            "loss: 0.786458  [ 7001/3697297] <-> Accuracy: 9.642857142857144%\n",
            "loss: 0.859375  [ 7101/3697297] <-> Accuracy: 9.619718309859154%\n",
            "loss: 0.723958  [ 7201/3697297] <-> Accuracy: 9.583333333333334%\n",
            "loss: 0.911458  [ 7301/3697297] <-> Accuracy: 9.534246575342467%\n",
            "loss: 0.796875  [ 7401/3697297] <-> Accuracy: 9.554054054054054%\n",
            "loss: 0.692708  [ 7501/3697297] <-> Accuracy: 9.466666666666667%\n",
            "loss: 0.781250  [ 7601/3697297] <-> Accuracy: 9.355263157894738%\n",
            "loss: 0.833333  [ 7701/3697297] <-> Accuracy: 9.285714285714286%\n",
            "loss: 0.822917  [ 7801/3697297] <-> Accuracy: 9.448717948717949%\n",
            "loss: 0.750000  [ 7901/3697297] <-> Accuracy: 9.417721518987342%\n",
            "loss: 0.755208  [ 8001/3697297] <-> Accuracy: 9.325%\n",
            "loss: 0.723958  [ 8101/3697297] <-> Accuracy: 9.283950617283951%\n",
            "loss: 0.718750  [ 8201/3697297] <-> Accuracy: 9.195121951219512%\n",
            "loss: 0.885417  [ 8301/3697297] <-> Accuracy: 9.084337349397591%\n",
            "loss: 0.791667  [ 8401/3697297] <-> Accuracy: 8.976190476190476%\n",
            "loss: 0.911458  [ 8501/3697297] <-> Accuracy: 8.905882352941177%\n",
            "loss: 0.744792  [ 8601/3697297] <-> Accuracy: 8.883720930232558%\n",
            "loss: 0.796875  [ 8701/3697297] <-> Accuracy: 8.85057471264368%\n",
            "loss: 0.838542  [ 8801/3697297] <-> Accuracy: 8.75%\n",
            "loss: 0.859375  [ 8901/3697297] <-> Accuracy: 8.696629213483147%\n",
            "loss: 0.744792  [ 9001/3697297] <-> Accuracy: 8.633333333333333%\n",
            "loss: 0.677083  [ 9101/3697297] <-> Accuracy: 8.835164835164836%\n",
            "loss: 0.453125  [14601/3697297] <-> Accuracy: 5.589041095890411%\n",
            "loss: 0.781250  [14701/3697297] <-> Accuracy: 5.931972789115647%\n",
            "loss: 0.822917  [14801/3697297] <-> Accuracy: 6.047297297297297%\n",
            "loss: 0.916667  [14901/3697297] <-> Accuracy: 6.127516778523489%\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-b46aef6c2141>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_against_anomailes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_RPM_DS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-0bb87b82ab57>\u001b[0m in \u001b[0;36mtest_against_anomailes\u001b[0;34m(dataloader, model, loss_fn)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-78c7b7445c40>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# Decoder:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUpsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nearest'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUpsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nearest'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/brevitas/nn/quant_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQuantTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQuantTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_weight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_bias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/brevitas/nn/quant_layer.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mquant_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_quant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mquant_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquant_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         compute_output_quant_tensor = isinstance(quant_input, QuantTensor) and isinstance(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/brevitas/nn/mixin/parameter.py\u001b[0m in \u001b[0;36mquant_weight\u001b[0;34m(self, quant_input, subtensor_slice_list)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_quant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_to_quantize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweight_slice_tuple\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_quant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_to_quantize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweight_slice_tuple\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msubtensor_slice_list\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;31m# Restore the quantizer behaviour to full tensor quantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/brevitas/proxy/parameter_quant.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    137\u001b[0m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_quant_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_quant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mis_dynamo_compiling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/brevitas/core/quant/binary.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaling_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_clamp_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_sign_ste\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelay_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbit_width\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/brevitas/function/ops_ste.py\u001b[0m in \u001b[0;36mbinary_sign_ste\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_sign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn_prefix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd_ste_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_sign_ste_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "test_against_anomailes(Train_RPM_DS, AutoEncoder, loss_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZSs7xZ5Fu6f"
      },
      "source": [
        "# **Test Normal Test Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1JRv2PnFzuc"
      },
      "outputs": [],
      "source": [
        "def test_normal(dataloader, model, loss_fn):\n",
        "    size = len(dataloader)\n",
        "    highest = 0.0\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch, (X, labels, cls) in enumerate(dataloader):\n",
        "            if (len(X) != batch_size) and (len(X) != train_batch_size):\n",
        "                print(f\"Batch size warning! [{X.shape}]\")\n",
        "                continue\n",
        "\n",
        "            # Compute prediction and loss\n",
        "            X = torch.reshape(X, (-1, 1, 64, 12))\n",
        "            X = (X * 2) - 1.0\n",
        "            pred = model(X)\n",
        "\n",
        "            loss = loss_fn(pred, X)\n",
        "\n",
        "            if batch % 1 == 0:\n",
        "                loss = loss.item()\n",
        "                if loss > highest:\n",
        "                    highest = loss\n",
        "                    print(f\"Highest Loss: {highest}\")\n",
        "                current = batch * batch_size + len(X)\n",
        "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "TpRuLDp4GFIz",
        "outputId": "16e3480e-026a-4c26-d3dd-333d7f040129"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Highest Loss: 0.8072916865348816\n",
            "loss: 0.807292  [    1/197710]\n",
            "Highest Loss: 0.9114583730697632\n",
            "loss: 0.911458  [    2/197710]\n",
            "loss: 0.682292  [    3/197710]\n",
            "Highest Loss: 0.9270833730697632\n",
            "loss: 0.927083  [    4/197710]\n",
            "loss: 0.796875  [    5/197710]\n",
            "loss: 0.755208  [    6/197710]\n",
            "loss: 0.770833  [    7/197710]\n",
            "loss: 0.880208  [    8/197710]\n",
            "loss: 0.744792  [    9/197710]\n",
            "loss: 0.869792  [   10/197710]\n",
            "loss: 0.760417  [   11/197710]\n",
            "loss: 0.791667  [   12/197710]\n",
            "loss: 0.927083  [   13/197710]\n",
            "loss: 0.812500  [   14/197710]\n",
            "Highest Loss: 1.03125\n",
            "loss: 1.031250  [   15/197710]\n",
            "loss: 0.854167  [   16/197710]\n",
            "loss: 0.947917  [   17/197710]\n",
            "loss: 0.666667  [   18/197710]\n",
            "loss: 0.848958  [   19/197710]\n",
            "loss: 0.776042  [   20/197710]\n",
            "loss: 0.921875  [   21/197710]\n",
            "loss: 0.802083  [   22/197710]\n",
            "loss: 0.921875  [   23/197710]\n",
            "loss: 0.843750  [   24/197710]\n",
            "loss: 0.812500  [   25/197710]\n",
            "loss: 0.786458  [   26/197710]\n",
            "loss: 0.760417  [   27/197710]\n",
            "loss: 0.822917  [   28/197710]\n",
            "loss: 0.921875  [   29/197710]\n",
            "loss: 0.885417  [   30/197710]\n",
            "loss: 0.739583  [   31/197710]\n",
            "loss: 0.833333  [   32/197710]\n",
            "loss: 0.692708  [   33/197710]\n",
            "loss: 0.875000  [   34/197710]\n",
            "loss: 0.791667  [   35/197710]\n",
            "loss: 0.895833  [   36/197710]\n",
            "loss: 0.833333  [   37/197710]\n",
            "loss: 0.880208  [   38/197710]\n",
            "loss: 0.718750  [   39/197710]\n",
            "loss: 0.843750  [   40/197710]\n",
            "loss: 0.677083  [   41/197710]\n",
            "loss: 0.979167  [   42/197710]\n",
            "loss: 0.796875  [   43/197710]\n",
            "loss: 0.697917  [   44/197710]\n",
            "loss: 0.916667  [   45/197710]\n",
            "loss: 0.994792  [   46/197710]\n",
            "loss: 0.807292  [   47/197710]\n",
            "loss: 0.786458  [   48/197710]\n",
            "loss: 0.875000  [   49/197710]\n",
            "loss: 0.817708  [   50/197710]\n",
            "loss: 0.744792  [   51/197710]\n",
            "loss: 0.760417  [   52/197710]\n",
            "loss: 0.750000  [   53/197710]\n",
            "loss: 0.802083  [   54/197710]\n",
            "loss: 0.890625  [   55/197710]\n",
            "loss: 0.880208  [   56/197710]\n",
            "Highest Loss: 1.046875\n",
            "loss: 1.046875  [   57/197710]\n",
            "loss: 0.979167  [   58/197710]\n",
            "loss: 0.718750  [   59/197710]\n",
            "loss: 0.770833  [   60/197710]\n",
            "loss: 0.786458  [   61/197710]\n",
            "loss: 0.807292  [   62/197710]\n",
            "loss: 0.786458  [   63/197710]\n",
            "loss: 0.807292  [   64/197710]\n",
            "loss: 0.848958  [   65/197710]\n",
            "loss: 1.041667  [   66/197710]\n",
            "loss: 0.765625  [   67/197710]\n",
            "loss: 0.734375  [   68/197710]\n",
            "loss: 0.770833  [   69/197710]\n",
            "loss: 1.020833  [   70/197710]\n",
            "loss: 0.937500  [   71/197710]\n",
            "loss: 0.750000  [   72/197710]\n",
            "loss: 0.864583  [   73/197710]\n",
            "loss: 0.697917  [   74/197710]\n",
            "loss: 0.750000  [   75/197710]\n",
            "loss: 0.781250  [   76/197710]\n",
            "loss: 0.822917  [   77/197710]\n",
            "loss: 0.635417  [   78/197710]\n",
            "loss: 0.942708  [   79/197710]\n",
            "loss: 0.755208  [   80/197710]\n",
            "loss: 0.796875  [   81/197710]\n",
            "loss: 1.005208  [   82/197710]\n",
            "loss: 0.833333  [   83/197710]\n",
            "loss: 0.895833  [   84/197710]\n",
            "loss: 0.708333  [   85/197710]\n",
            "loss: 0.781250  [   86/197710]\n",
            "loss: 0.906250  [   87/197710]\n",
            "loss: 1.005208  [   88/197710]\n",
            "loss: 0.822917  [   89/197710]\n",
            "loss: 0.833333  [   90/197710]\n",
            "loss: 0.802083  [   91/197710]\n",
            "loss: 0.895833  [   92/197710]\n",
            "loss: 0.739583  [   93/197710]\n",
            "loss: 0.661458  [   94/197710]\n",
            "loss: 0.677083  [   95/197710]\n",
            "loss: 0.968750  [   96/197710]\n",
            "loss: 0.791667  [   97/197710]\n",
            "loss: 0.718750  [   98/197710]\n",
            "loss: 0.776042  [   99/197710]\n",
            "loss: 0.828125  [  100/197710]\n",
            "loss: 0.708333  [  101/197710]\n",
            "loss: 0.802083  [  102/197710]\n",
            "loss: 0.984375  [  103/197710]\n",
            "loss: 0.875000  [  104/197710]\n",
            "loss: 0.984375  [  105/197710]\n",
            "loss: 0.932292  [  106/197710]\n",
            "loss: 0.828125  [  107/197710]\n",
            "loss: 0.796875  [  108/197710]\n",
            "loss: 0.942708  [  109/197710]\n",
            "loss: 0.708333  [  110/197710]\n",
            "loss: 0.880208  [  111/197710]\n",
            "loss: 0.734375  [  112/197710]\n",
            "loss: 0.750000  [  113/197710]\n",
            "loss: 0.776042  [  114/197710]\n",
            "loss: 0.786458  [  115/197710]\n",
            "loss: 0.901042  [  116/197710]\n",
            "loss: 0.869792  [  117/197710]\n",
            "loss: 0.703125  [  118/197710]\n",
            "loss: 0.875000  [  119/197710]\n",
            "loss: 0.750000  [  120/197710]\n",
            "loss: 0.895833  [  121/197710]\n",
            "loss: 1.020833  [  122/197710]\n",
            "loss: 0.671875  [  123/197710]\n",
            "loss: 0.807292  [  124/197710]\n",
            "loss: 0.833333  [  125/197710]\n",
            "loss: 0.744792  [  126/197710]\n",
            "loss: 0.901042  [  127/197710]\n",
            "loss: 0.812500  [  128/197710]\n",
            "loss: 0.750000  [  129/197710]\n",
            "loss: 0.703125  [  130/197710]\n",
            "loss: 0.901042  [  131/197710]\n",
            "loss: 0.979167  [  132/197710]\n",
            "loss: 0.828125  [  133/197710]\n",
            "loss: 0.807292  [  134/197710]\n",
            "loss: 0.692708  [  135/197710]\n",
            "loss: 0.786458  [  136/197710]\n",
            "loss: 0.776042  [  137/197710]\n",
            "loss: 0.906250  [  138/197710]\n",
            "loss: 0.880208  [  139/197710]\n",
            "loss: 0.901042  [  140/197710]\n",
            "loss: 0.786458  [  141/197710]\n",
            "loss: 0.760417  [  142/197710]\n",
            "loss: 0.854167  [  143/197710]\n",
            "loss: 0.718750  [  144/197710]\n",
            "loss: 0.911458  [  145/197710]\n",
            "loss: 0.947917  [  146/197710]\n",
            "loss: 0.765625  [  147/197710]\n",
            "loss: 1.005208  [  148/197710]\n",
            "loss: 0.854167  [  149/197710]\n",
            "loss: 0.921875  [  150/197710]\n",
            "loss: 0.953125  [  151/197710]\n",
            "loss: 0.854167  [  152/197710]\n",
            "loss: 0.822917  [  153/197710]\n",
            "loss: 1.010417  [  154/197710]\n",
            "loss: 0.791667  [  155/197710]\n",
            "loss: 0.755208  [  156/197710]\n",
            "loss: 0.760417  [  157/197710]\n",
            "loss: 0.739583  [  158/197710]\n",
            "loss: 0.859375  [  159/197710]\n",
            "loss: 0.713542  [  160/197710]\n",
            "loss: 0.927083  [  161/197710]\n",
            "loss: 0.692708  [  162/197710]\n",
            "loss: 0.906250  [  163/197710]\n",
            "loss: 0.828125  [  164/197710]\n",
            "loss: 0.864583  [  165/197710]\n",
            "loss: 0.671875  [  166/197710]\n",
            "loss: 0.718750  [  167/197710]\n",
            "loss: 0.755208  [  168/197710]\n",
            "loss: 0.729167  [  169/197710]\n",
            "loss: 0.812500  [  170/197710]\n",
            "loss: 0.843750  [  171/197710]\n",
            "loss: 0.859375  [  172/197710]\n",
            "loss: 0.786458  [  173/197710]\n",
            "loss: 0.869792  [  174/197710]\n",
            "loss: 0.781250  [  175/197710]\n",
            "loss: 0.843750  [  176/197710]\n",
            "loss: 0.875000  [  177/197710]\n",
            "loss: 0.890625  [  178/197710]\n",
            "loss: 0.854167  [  179/197710]\n",
            "loss: 0.802083  [  180/197710]\n",
            "loss: 0.859375  [  181/197710]\n",
            "loss: 0.807292  [  182/197710]\n",
            "loss: 0.708333  [  183/197710]\n",
            "loss: 0.843750  [  184/197710]\n",
            "loss: 0.911458  [  185/197710]\n",
            "loss: 0.937500  [  186/197710]\n",
            "loss: 0.708333  [  187/197710]\n",
            "loss: 0.854167  [  188/197710]\n",
            "loss: 0.734375  [  189/197710]\n",
            "Highest Loss: 1.0677083730697632\n",
            "loss: 1.067708  [  190/197710]\n",
            "loss: 0.770833  [  191/197710]\n",
            "loss: 0.906250  [  192/197710]\n",
            "loss: 0.682292  [  193/197710]\n",
            "loss: 0.864583  [  194/197710]\n",
            "loss: 0.729167  [  195/197710]\n",
            "loss: 0.859375  [  196/197710]\n",
            "loss: 0.755208  [  197/197710]\n",
            "loss: 0.739583  [  198/197710]\n",
            "loss: 1.026042  [  199/197710]\n",
            "loss: 0.682292  [  200/197710]\n",
            "loss: 0.817708  [  201/197710]\n",
            "loss: 0.958333  [  202/197710]\n",
            "loss: 0.833333  [  203/197710]\n",
            "loss: 0.958333  [  204/197710]\n",
            "loss: 0.979167  [  205/197710]\n",
            "loss: 0.817708  [  206/197710]\n",
            "loss: 0.875000  [  207/197710]\n",
            "loss: 0.807292  [  208/197710]\n",
            "loss: 0.786458  [  209/197710]\n",
            "loss: 1.052083  [  210/197710]\n",
            "loss: 0.911458  [  211/197710]\n",
            "loss: 0.937500  [  212/197710]\n",
            "loss: 0.984375  [  213/197710]\n",
            "loss: 0.968750  [  214/197710]\n",
            "loss: 0.713542  [  215/197710]\n",
            "loss: 0.864583  [  216/197710]\n",
            "loss: 0.812500  [  217/197710]\n",
            "loss: 0.937500  [  218/197710]\n",
            "loss: 0.906250  [  219/197710]\n",
            "loss: 0.703125  [  220/197710]\n",
            "loss: 0.859375  [  221/197710]\n",
            "loss: 0.723958  [  222/197710]\n",
            "loss: 0.713542  [  223/197710]\n",
            "loss: 0.677083  [  224/197710]\n",
            "loss: 1.031250  [  225/197710]\n",
            "loss: 0.989583  [  226/197710]\n",
            "loss: 0.937500  [  227/197710]\n",
            "loss: 0.932292  [  228/197710]\n",
            "loss: 0.864583  [  229/197710]\n",
            "loss: 0.739583  [  230/197710]\n",
            "loss: 0.843750  [  231/197710]\n",
            "loss: 0.895833  [  232/197710]\n",
            "loss: 0.744792  [  233/197710]\n",
            "loss: 0.880208  [  234/197710]\n",
            "loss: 0.630208  [  235/197710]\n",
            "loss: 0.927083  [  236/197710]\n",
            "loss: 0.854167  [  237/197710]\n",
            "loss: 0.885417  [  238/197710]\n",
            "loss: 0.869792  [  239/197710]\n",
            "loss: 0.828125  [  240/197710]\n",
            "loss: 0.739583  [  241/197710]\n",
            "loss: 0.760417  [  242/197710]\n",
            "loss: 0.723958  [  243/197710]\n",
            "loss: 0.791667  [  244/197710]\n",
            "loss: 0.875000  [  245/197710]\n",
            "loss: 0.885417  [  246/197710]\n",
            "loss: 0.791667  [  247/197710]\n",
            "loss: 0.776042  [  248/197710]\n",
            "loss: 0.822917  [  249/197710]\n",
            "loss: 0.765625  [  250/197710]\n",
            "loss: 0.812500  [  251/197710]\n",
            "loss: 0.895833  [  252/197710]\n",
            "loss: 0.864583  [  253/197710]\n",
            "loss: 0.786458  [  254/197710]\n",
            "loss: 1.041667  [  255/197710]\n",
            "loss: 0.781250  [  256/197710]\n",
            "loss: 0.760417  [  257/197710]\n",
            "loss: 0.776042  [  258/197710]\n",
            "loss: 1.067708  [  259/197710]\n",
            "loss: 0.744792  [  260/197710]\n",
            "loss: 0.989583  [  261/197710]\n",
            "loss: 0.864583  [  262/197710]\n",
            "loss: 0.765625  [  263/197710]\n",
            "loss: 0.817708  [  264/197710]\n",
            "loss: 0.953125  [  265/197710]\n",
            "loss: 0.713542  [  266/197710]\n",
            "loss: 0.828125  [  267/197710]\n",
            "loss: 0.776042  [  268/197710]\n",
            "loss: 0.942708  [  269/197710]\n",
            "loss: 0.973958  [  270/197710]\n",
            "loss: 0.838542  [  271/197710]\n",
            "loss: 0.880208  [  272/197710]\n",
            "loss: 0.697917  [  273/197710]\n",
            "loss: 0.973958  [  274/197710]\n",
            "loss: 0.723958  [  275/197710]\n",
            "loss: 0.713542  [  276/197710]\n",
            "loss: 1.020833  [  277/197710]\n",
            "loss: 0.937500  [  278/197710]\n",
            "loss: 0.734375  [  279/197710]\n",
            "loss: 0.958333  [  280/197710]\n",
            "loss: 0.713542  [  281/197710]\n",
            "loss: 0.963542  [  282/197710]\n",
            "loss: 0.770833  [  283/197710]\n",
            "loss: 0.802083  [  284/197710]\n",
            "loss: 0.979167  [  285/197710]\n",
            "loss: 0.776042  [  286/197710]\n",
            "loss: 0.770833  [  287/197710]\n",
            "loss: 0.760417  [  288/197710]\n",
            "loss: 0.750000  [  289/197710]\n",
            "loss: 0.812500  [  290/197710]\n",
            "loss: 0.838542  [  291/197710]\n",
            "loss: 0.718750  [  292/197710]\n",
            "loss: 0.817708  [  293/197710]\n",
            "loss: 0.833333  [  294/197710]\n",
            "Highest Loss: 1.0885417461395264\n",
            "loss: 1.088542  [  295/197710]\n",
            "loss: 0.755208  [  296/197710]\n",
            "loss: 0.645833  [  297/197710]\n",
            "loss: 0.859375  [  298/197710]\n",
            "loss: 0.630208  [  299/197710]\n",
            "loss: 0.937500  [  300/197710]\n",
            "loss: 0.854167  [  301/197710]\n",
            "loss: 0.911458  [  302/197710]\n",
            "loss: 0.708333  [  303/197710]\n",
            "loss: 0.932292  [  304/197710]\n",
            "loss: 0.692708  [  305/197710]\n",
            "loss: 0.927083  [  306/197710]\n",
            "loss: 0.854167  [  307/197710]\n",
            "loss: 0.911458  [  308/197710]\n",
            "loss: 0.708333  [  309/197710]\n",
            "loss: 0.697917  [  310/197710]\n",
            "loss: 0.729167  [  311/197710]\n",
            "loss: 0.750000  [  312/197710]\n",
            "loss: 0.807292  [  313/197710]\n",
            "loss: 0.880208  [  314/197710]\n",
            "loss: 0.994792  [  315/197710]\n",
            "loss: 0.989583  [  316/197710]\n",
            "loss: 0.932292  [  317/197710]\n",
            "loss: 0.937500  [  318/197710]\n",
            "loss: 0.822917  [  319/197710]\n",
            "loss: 1.041667  [  320/197710]\n",
            "loss: 0.755208  [  321/197710]\n",
            "loss: 0.776042  [  322/197710]\n",
            "loss: 0.973958  [  323/197710]\n",
            "loss: 0.895833  [  324/197710]\n",
            "loss: 0.786458  [  325/197710]\n",
            "loss: 0.906250  [  326/197710]\n",
            "loss: 0.927083  [  327/197710]\n",
            "loss: 0.875000  [  328/197710]\n",
            "loss: 0.885417  [  329/197710]\n",
            "loss: 0.770833  [  330/197710]\n",
            "loss: 0.953125  [  331/197710]\n",
            "loss: 0.765625  [  332/197710]\n",
            "loss: 0.750000  [  333/197710]\n",
            "loss: 0.703125  [  334/197710]\n",
            "loss: 0.854167  [  335/197710]\n",
            "loss: 0.848958  [  336/197710]\n",
            "loss: 0.791667  [  337/197710]\n",
            "loss: 0.875000  [  338/197710]\n",
            "loss: 0.843750  [  339/197710]\n",
            "loss: 0.932292  [  340/197710]\n",
            "loss: 0.796875  [  341/197710]\n",
            "loss: 0.911458  [  342/197710]\n",
            "loss: 0.703125  [  343/197710]\n",
            "loss: 0.822917  [  344/197710]\n",
            "loss: 0.791667  [  345/197710]\n",
            "loss: 0.744792  [  346/197710]\n",
            "loss: 0.807292  [  347/197710]\n",
            "loss: 0.838542  [  348/197710]\n",
            "loss: 0.927083  [  349/197710]\n",
            "loss: 1.062500  [  350/197710]\n",
            "loss: 0.895833  [  351/197710]\n",
            "loss: 0.953125  [  352/197710]\n",
            "loss: 0.901042  [  353/197710]\n",
            "loss: 0.869792  [  354/197710]\n",
            "loss: 0.734375  [  355/197710]\n",
            "loss: 0.744792  [  356/197710]\n",
            "loss: 0.661458  [  357/197710]\n",
            "loss: 0.796875  [  358/197710]\n",
            "loss: 0.755208  [  359/197710]\n",
            "loss: 0.723958  [  360/197710]\n",
            "loss: 0.921875  [  361/197710]\n",
            "loss: 0.802083  [  362/197710]\n",
            "loss: 0.718750  [  363/197710]\n",
            "loss: 0.885417  [  364/197710]\n",
            "loss: 0.848958  [  365/197710]\n",
            "loss: 0.880208  [  366/197710]\n",
            "loss: 0.885417  [  367/197710]\n",
            "loss: 0.770833  [  368/197710]\n",
            "loss: 0.947917  [  369/197710]\n",
            "loss: 0.911458  [  370/197710]\n",
            "loss: 0.833333  [  371/197710]\n",
            "loss: 0.953125  [  372/197710]\n",
            "loss: 0.979167  [  373/197710]\n",
            "loss: 0.963542  [  374/197710]\n",
            "loss: 0.781250  [  375/197710]\n",
            "loss: 0.854167  [  376/197710]\n",
            "loss: 0.822917  [  377/197710]\n",
            "loss: 0.744792  [  378/197710]\n",
            "loss: 1.015625  [  379/197710]\n",
            "loss: 0.770833  [  380/197710]\n",
            "loss: 0.776042  [  381/197710]\n",
            "loss: 0.750000  [  382/197710]\n",
            "loss: 0.848958  [  383/197710]\n",
            "loss: 0.781250  [  384/197710]\n",
            "loss: 0.645833  [  385/197710]\n",
            "loss: 0.859375  [  386/197710]\n",
            "loss: 0.786458  [  387/197710]\n",
            "loss: 0.921875  [  388/197710]\n",
            "loss: 0.859375  [  389/197710]\n",
            "loss: 0.833333  [  390/197710]\n",
            "loss: 0.854167  [  391/197710]\n",
            "loss: 0.723958  [  392/197710]\n",
            "loss: 0.750000  [  393/197710]\n",
            "loss: 1.020833  [  394/197710]\n",
            "loss: 0.859375  [  395/197710]\n",
            "loss: 0.812500  [  396/197710]\n",
            "loss: 0.838542  [  397/197710]\n",
            "loss: 1.067708  [  398/197710]\n",
            "loss: 1.031250  [  399/197710]\n",
            "loss: 0.979167  [  400/197710]\n",
            "loss: 0.838542  [  401/197710]\n",
            "loss: 0.895833  [  402/197710]\n",
            "loss: 0.817708  [  403/197710]\n",
            "loss: 0.880208  [  404/197710]\n",
            "loss: 0.729167  [  405/197710]\n",
            "loss: 0.822917  [  406/197710]\n",
            "loss: 0.869792  [  407/197710]\n",
            "loss: 0.723958  [  408/197710]\n",
            "loss: 0.807292  [  409/197710]\n",
            "loss: 0.734375  [  410/197710]\n",
            "loss: 0.729167  [  411/197710]\n",
            "loss: 0.838542  [  412/197710]\n",
            "loss: 0.812500  [  413/197710]\n",
            "loss: 0.791667  [  414/197710]\n",
            "loss: 0.963542  [  415/197710]\n",
            "loss: 0.843750  [  416/197710]\n",
            "loss: 0.734375  [  417/197710]\n",
            "loss: 0.817708  [  418/197710]\n",
            "loss: 0.739583  [  419/197710]\n",
            "loss: 0.786458  [  420/197710]\n",
            "loss: 0.869792  [  421/197710]\n",
            "loss: 0.796875  [  422/197710]\n",
            "loss: 0.744792  [  423/197710]\n",
            "loss: 0.802083  [  424/197710]\n",
            "loss: 0.822917  [  425/197710]\n",
            "loss: 0.906250  [  426/197710]\n",
            "loss: 0.776042  [  427/197710]\n",
            "loss: 0.734375  [  428/197710]\n",
            "loss: 0.807292  [  429/197710]\n",
            "loss: 0.869792  [  430/197710]\n",
            "loss: 1.020833  [  431/197710]\n",
            "loss: 1.046875  [  432/197710]\n",
            "loss: 0.807292  [  433/197710]\n",
            "loss: 0.906250  [  434/197710]\n",
            "loss: 0.838542  [  435/197710]\n",
            "loss: 0.671875  [  436/197710]\n",
            "loss: 0.661458  [  437/197710]\n",
            "loss: 0.807292  [  438/197710]\n",
            "loss: 0.765625  [  439/197710]\n",
            "loss: 0.828125  [  440/197710]\n",
            "loss: 0.828125  [  441/197710]\n",
            "loss: 0.796875  [  442/197710]\n",
            "loss: 0.739583  [  443/197710]\n",
            "loss: 0.875000  [  444/197710]\n",
            "loss: 0.875000  [  445/197710]\n",
            "loss: 0.630208  [  446/197710]\n",
            "loss: 0.854167  [  447/197710]\n",
            "loss: 0.848958  [  448/197710]\n",
            "loss: 0.895833  [  449/197710]\n",
            "loss: 0.671875  [  450/197710]\n",
            "loss: 0.744792  [  451/197710]\n",
            "loss: 0.979167  [  452/197710]\n",
            "loss: 0.859375  [  453/197710]\n",
            "loss: 0.781250  [  454/197710]\n",
            "loss: 0.927083  [  455/197710]\n",
            "loss: 0.890625  [  456/197710]\n",
            "loss: 0.890625  [  457/197710]\n",
            "loss: 0.781250  [  458/197710]\n",
            "loss: 0.875000  [  459/197710]\n",
            "loss: 0.901042  [  460/197710]\n",
            "loss: 0.734375  [  461/197710]\n",
            "loss: 1.088542  [  462/197710]\n",
            "loss: 0.718750  [  463/197710]\n",
            "loss: 0.786458  [  464/197710]\n",
            "loss: 0.817708  [  465/197710]\n",
            "loss: 0.770833  [  466/197710]\n",
            "loss: 0.885417  [  467/197710]\n",
            "loss: 0.770833  [  468/197710]\n",
            "loss: 0.682292  [  469/197710]\n",
            "loss: 0.973958  [  470/197710]\n",
            "loss: 0.984375  [  471/197710]\n",
            "loss: 0.656250  [  472/197710]\n",
            "loss: 0.880208  [  473/197710]\n",
            "loss: 0.708333  [  474/197710]\n",
            "loss: 0.958333  [  475/197710]\n",
            "loss: 0.859375  [  476/197710]\n",
            "loss: 0.880208  [  477/197710]\n",
            "loss: 0.864583  [  478/197710]\n",
            "loss: 0.848958  [  479/197710]\n",
            "loss: 0.755208  [  480/197710]\n",
            "loss: 0.869792  [  481/197710]\n",
            "loss: 0.807292  [  482/197710]\n",
            "loss: 0.880208  [  483/197710]\n",
            "loss: 0.937500  [  484/197710]\n",
            "loss: 0.744792  [  485/197710]\n",
            "loss: 0.875000  [  486/197710]\n",
            "loss: 0.880208  [  487/197710]\n",
            "loss: 0.812500  [  488/197710]\n",
            "loss: 0.848958  [  489/197710]\n",
            "loss: 0.843750  [  490/197710]\n",
            "loss: 0.760417  [  491/197710]\n",
            "loss: 0.750000  [  492/197710]\n",
            "loss: 0.755208  [  493/197710]\n",
            "loss: 0.697917  [  494/197710]\n",
            "loss: 0.901042  [  495/197710]\n",
            "loss: 0.781250  [  496/197710]\n",
            "loss: 0.859375  [  497/197710]\n",
            "loss: 0.807292  [  498/197710]\n",
            "loss: 0.765625  [  499/197710]\n",
            "loss: 0.973958  [  500/197710]\n",
            "loss: 0.666667  [  501/197710]\n",
            "loss: 0.880208  [  502/197710]\n",
            "loss: 0.802083  [  503/197710]\n",
            "loss: 1.015625  [  504/197710]\n",
            "loss: 0.864583  [  505/197710]\n",
            "loss: 0.880208  [  506/197710]\n",
            "loss: 0.796875  [  507/197710]\n",
            "loss: 0.953125  [  508/197710]\n",
            "loss: 0.802083  [  509/197710]\n",
            "loss: 0.744792  [  510/197710]\n",
            "loss: 0.848958  [  511/197710]\n",
            "loss: 0.854167  [  512/197710]\n",
            "loss: 0.677083  [  513/197710]\n",
            "loss: 0.921875  [  514/197710]\n",
            "loss: 0.854167  [  515/197710]\n",
            "loss: 0.848958  [  516/197710]\n",
            "loss: 0.848958  [  517/197710]\n",
            "loss: 0.953125  [  518/197710]\n",
            "loss: 0.760417  [  519/197710]\n",
            "loss: 0.807292  [  520/197710]\n",
            "loss: 0.817708  [  521/197710]\n",
            "loss: 0.791667  [  522/197710]\n",
            "loss: 0.677083  [  523/197710]\n",
            "loss: 0.911458  [  524/197710]\n",
            "loss: 0.786458  [  525/197710]\n",
            "loss: 0.734375  [  526/197710]\n",
            "loss: 0.953125  [  527/197710]\n",
            "loss: 0.765625  [  528/197710]\n",
            "loss: 0.901042  [  529/197710]\n",
            "loss: 0.713542  [  530/197710]\n",
            "loss: 0.885417  [  531/197710]\n",
            "loss: 0.906250  [  532/197710]\n",
            "loss: 1.010417  [  533/197710]\n",
            "loss: 0.651042  [  534/197710]\n",
            "loss: 1.062500  [  535/197710]\n",
            "loss: 0.781250  [  536/197710]\n",
            "loss: 0.843750  [  537/197710]\n",
            "loss: 0.833333  [  538/197710]\n",
            "loss: 0.916667  [  539/197710]\n",
            "loss: 0.859375  [  540/197710]\n",
            "loss: 0.906250  [  541/197710]\n",
            "loss: 0.750000  [  542/197710]\n",
            "loss: 0.760417  [  543/197710]\n",
            "loss: 0.869792  [  544/197710]\n",
            "loss: 0.776042  [  545/197710]\n",
            "loss: 0.807292  [  546/197710]\n",
            "loss: 0.697917  [  547/197710]\n",
            "loss: 0.822917  [  548/197710]\n",
            "loss: 0.958333  [  549/197710]\n",
            "loss: 0.958333  [  550/197710]\n",
            "loss: 0.755208  [  551/197710]\n",
            "loss: 0.864583  [  552/197710]\n",
            "loss: 0.802083  [  553/197710]\n",
            "loss: 0.822917  [  554/197710]\n",
            "loss: 0.708333  [  555/197710]\n",
            "loss: 0.734375  [  556/197710]\n",
            "loss: 0.703125  [  557/197710]\n",
            "loss: 0.942708  [  558/197710]\n",
            "loss: 0.734375  [  559/197710]\n",
            "loss: 0.885417  [  560/197710]\n",
            "loss: 1.031250  [  561/197710]\n",
            "loss: 0.859375  [  562/197710]\n",
            "loss: 0.776042  [  563/197710]\n",
            "loss: 0.770833  [  564/197710]\n",
            "loss: 0.885417  [  565/197710]\n",
            "loss: 0.718750  [  566/197710]\n",
            "loss: 1.000000  [  567/197710]\n",
            "loss: 1.067708  [  568/197710]\n",
            "loss: 0.880208  [  569/197710]\n",
            "loss: 0.906250  [  570/197710]\n",
            "loss: 0.885417  [  571/197710]\n",
            "loss: 0.796875  [  572/197710]\n",
            "loss: 0.734375  [  573/197710]\n",
            "loss: 0.791667  [  574/197710]\n",
            "loss: 0.807292  [  575/197710]\n",
            "loss: 0.901042  [  576/197710]\n",
            "loss: 0.802083  [  577/197710]\n",
            "loss: 0.786458  [  578/197710]\n",
            "loss: 0.791667  [  579/197710]\n",
            "loss: 0.921875  [  580/197710]\n",
            "loss: 0.869792  [  581/197710]\n",
            "loss: 0.770833  [  582/197710]\n",
            "loss: 0.729167  [  583/197710]\n",
            "loss: 0.895833  [  584/197710]\n",
            "loss: 0.776042  [  585/197710]\n",
            "loss: 0.838542  [  586/197710]\n",
            "loss: 0.994792  [  587/197710]\n",
            "loss: 0.703125  [  588/197710]\n",
            "loss: 0.817708  [  589/197710]\n",
            "loss: 0.885417  [  590/197710]\n",
            "loss: 0.692708  [  591/197710]\n",
            "loss: 0.744792  [  592/197710]\n",
            "loss: 0.828125  [  593/197710]\n",
            "loss: 1.026042  [  594/197710]\n",
            "loss: 0.729167  [  595/197710]\n",
            "loss: 0.906250  [  596/197710]\n",
            "loss: 0.791667  [  597/197710]\n",
            "loss: 0.812500  [  598/197710]\n",
            "loss: 0.854167  [  599/197710]\n",
            "loss: 0.765625  [  600/197710]\n",
            "loss: 0.687500  [  601/197710]\n",
            "loss: 0.895833  [  602/197710]\n",
            "loss: 0.833333  [  603/197710]\n",
            "loss: 0.932292  [  604/197710]\n",
            "loss: 0.718750  [  605/197710]\n",
            "loss: 0.812500  [  606/197710]\n",
            "loss: 0.729167  [  607/197710]\n",
            "loss: 0.682292  [  608/197710]\n",
            "loss: 0.812500  [  609/197710]\n",
            "loss: 0.703125  [  610/197710]\n",
            "loss: 0.901042  [  611/197710]\n",
            "loss: 0.942708  [  612/197710]\n",
            "loss: 0.880208  [  613/197710]\n",
            "loss: 0.953125  [  614/197710]\n",
            "loss: 0.744792  [  615/197710]\n",
            "loss: 1.031250  [  616/197710]\n",
            "loss: 0.895833  [  617/197710]\n",
            "loss: 0.734375  [  618/197710]\n",
            "loss: 0.781250  [  619/197710]\n",
            "loss: 0.890625  [  620/197710]\n",
            "loss: 1.000000  [  621/197710]\n",
            "loss: 0.781250  [  622/197710]\n",
            "loss: 0.947917  [  623/197710]\n",
            "loss: 0.979167  [  624/197710]\n",
            "loss: 0.854167  [  625/197710]\n",
            "loss: 0.864583  [  626/197710]\n",
            "loss: 0.755208  [  627/197710]\n",
            "loss: 0.921875  [  628/197710]\n",
            "loss: 0.885417  [  629/197710]\n",
            "loss: 0.864583  [  630/197710]\n",
            "loss: 0.765625  [  631/197710]\n",
            "loss: 0.932292  [  632/197710]\n",
            "loss: 0.848958  [  633/197710]\n",
            "loss: 0.744792  [  634/197710]\n",
            "loss: 0.796875  [  635/197710]\n",
            "loss: 0.770833  [  636/197710]\n",
            "loss: 0.734375  [  637/197710]\n",
            "loss: 1.026042  [  638/197710]\n",
            "loss: 0.677083  [  639/197710]\n",
            "loss: 0.807292  [  640/197710]\n",
            "loss: 0.838542  [  641/197710]\n",
            "loss: 0.651042  [  642/197710]\n",
            "loss: 0.729167  [  643/197710]\n",
            "loss: 0.869792  [  644/197710]\n",
            "loss: 0.916667  [  645/197710]\n",
            "loss: 0.927083  [  646/197710]\n",
            "loss: 0.901042  [  647/197710]\n",
            "loss: 0.713542  [  648/197710]\n",
            "loss: 0.796875  [  649/197710]\n",
            "loss: 0.718750  [  650/197710]\n",
            "loss: 0.859375  [  651/197710]\n",
            "loss: 0.880208  [  652/197710]\n",
            "loss: 0.859375  [  653/197710]\n",
            "loss: 0.968750  [  654/197710]\n",
            "loss: 0.791667  [  655/197710]\n",
            "loss: 0.729167  [  656/197710]\n",
            "loss: 0.776042  [  657/197710]\n",
            "loss: 0.791667  [  658/197710]\n",
            "loss: 0.781250  [  659/197710]\n",
            "loss: 0.776042  [  660/197710]\n",
            "loss: 0.807292  [  661/197710]\n",
            "loss: 0.750000  [  662/197710]\n",
            "loss: 0.859375  [  663/197710]\n",
            "loss: 0.843750  [  664/197710]\n",
            "loss: 0.682292  [  665/197710]\n",
            "loss: 0.911458  [  666/197710]\n",
            "loss: 0.895833  [  667/197710]\n",
            "loss: 0.619792  [  668/197710]\n",
            "loss: 1.005208  [  669/197710]\n",
            "loss: 0.859375  [  670/197710]\n",
            "loss: 0.932292  [  671/197710]\n",
            "loss: 0.932292  [  672/197710]\n",
            "loss: 0.812500  [  673/197710]\n",
            "loss: 0.927083  [  674/197710]\n",
            "loss: 0.692708  [  675/197710]\n",
            "loss: 0.895833  [  676/197710]\n",
            "loss: 0.812500  [  677/197710]\n",
            "loss: 0.625000  [  678/197710]\n",
            "loss: 0.739583  [  679/197710]\n",
            "loss: 0.848958  [  680/197710]\n",
            "loss: 0.838542  [  681/197710]\n",
            "loss: 0.947917  [  682/197710]\n",
            "loss: 0.760417  [  683/197710]\n",
            "loss: 0.916667  [  684/197710]\n",
            "loss: 0.932292  [  685/197710]\n",
            "loss: 0.859375  [  686/197710]\n",
            "loss: 0.921875  [  687/197710]\n",
            "loss: 0.989583  [  688/197710]\n",
            "loss: 0.760417  [  689/197710]\n",
            "loss: 0.854167  [  690/197710]\n",
            "loss: 0.817708  [  691/197710]\n",
            "loss: 0.968750  [  692/197710]\n",
            "loss: 0.854167  [  693/197710]\n",
            "loss: 0.755208  [  694/197710]\n",
            "loss: 0.776042  [  695/197710]\n",
            "loss: 0.895833  [  696/197710]\n",
            "loss: 0.791667  [  697/197710]\n",
            "loss: 0.718750  [  698/197710]\n",
            "loss: 0.843750  [  699/197710]\n",
            "loss: 0.937500  [  700/197710]\n",
            "loss: 0.770833  [  701/197710]\n",
            "loss: 0.776042  [  702/197710]\n",
            "loss: 0.828125  [  703/197710]\n",
            "loss: 0.869792  [  704/197710]\n",
            "loss: 0.890625  [  705/197710]\n",
            "loss: 0.864583  [  706/197710]\n",
            "loss: 0.953125  [  707/197710]\n",
            "loss: 0.968750  [  708/197710]\n",
            "loss: 1.015625  [  709/197710]\n",
            "loss: 1.005208  [  710/197710]\n",
            "loss: 0.833333  [  711/197710]\n",
            "loss: 0.802083  [  712/197710]\n",
            "loss: 0.932292  [  713/197710]\n",
            "loss: 1.041667  [  714/197710]\n",
            "loss: 0.723958  [  715/197710]\n",
            "loss: 0.828125  [  716/197710]\n",
            "loss: 0.958333  [  717/197710]\n",
            "loss: 0.791667  [  718/197710]\n",
            "loss: 0.843750  [  719/197710]\n",
            "loss: 0.906250  [  720/197710]\n",
            "loss: 0.718750  [  721/197710]\n",
            "loss: 0.953125  [  722/197710]\n",
            "loss: 0.734375  [  723/197710]\n",
            "loss: 0.729167  [  724/197710]\n",
            "loss: 0.822917  [  725/197710]\n",
            "loss: 0.828125  [  726/197710]\n",
            "loss: 0.895833  [  727/197710]\n",
            "loss: 1.067708  [  728/197710]\n",
            "loss: 0.718750  [  729/197710]\n",
            "loss: 0.817708  [  730/197710]\n",
            "loss: 0.864583  [  731/197710]\n",
            "loss: 1.052083  [  732/197710]\n",
            "loss: 0.989583  [  733/197710]\n",
            "loss: 0.687500  [  734/197710]\n",
            "loss: 0.677083  [  735/197710]\n",
            "loss: 0.661458  [  736/197710]\n",
            "loss: 0.796875  [  737/197710]\n",
            "loss: 0.677083  [  738/197710]\n",
            "loss: 0.802083  [  739/197710]\n",
            "loss: 0.864583  [  740/197710]\n",
            "loss: 0.807292  [  741/197710]\n",
            "loss: 0.744792  [  742/197710]\n",
            "loss: 0.687500  [  743/197710]\n",
            "loss: 0.755208  [  744/197710]\n",
            "loss: 0.760417  [  745/197710]\n",
            "loss: 0.802083  [  746/197710]\n",
            "loss: 0.911458  [  747/197710]\n",
            "loss: 0.859375  [  748/197710]\n",
            "loss: 0.921875  [  749/197710]\n",
            "loss: 0.838542  [  750/197710]\n",
            "loss: 0.895833  [  751/197710]\n",
            "loss: 0.817708  [  752/197710]\n",
            "loss: 0.713542  [  753/197710]\n",
            "loss: 0.682292  [  754/197710]\n",
            "loss: 0.838542  [  755/197710]\n",
            "loss: 0.994792  [  756/197710]\n",
            "loss: 0.666667  [  757/197710]\n",
            "loss: 0.937500  [  758/197710]\n",
            "loss: 0.895833  [  759/197710]\n",
            "loss: 0.963542  [  760/197710]\n",
            "loss: 0.817708  [  761/197710]\n",
            "loss: 0.802083  [  762/197710]\n",
            "loss: 0.901042  [  763/197710]\n",
            "loss: 0.750000  [  764/197710]\n",
            "loss: 0.947917  [  765/197710]\n",
            "loss: 0.817708  [  766/197710]\n",
            "loss: 0.843750  [  767/197710]\n",
            "loss: 1.000000  [  768/197710]\n",
            "loss: 0.708333  [  769/197710]\n",
            "loss: 0.807292  [  770/197710]\n",
            "loss: 0.765625  [  771/197710]\n",
            "loss: 0.963542  [  772/197710]\n",
            "loss: 0.921875  [  773/197710]\n",
            "loss: 0.645833  [  774/197710]\n",
            "loss: 1.036458  [  775/197710]\n",
            "loss: 0.848958  [  776/197710]\n",
            "loss: 0.697917  [  777/197710]\n",
            "loss: 0.833333  [  778/197710]\n",
            "loss: 0.942708  [  779/197710]\n",
            "loss: 0.880208  [  780/197710]\n",
            "loss: 0.697917  [  781/197710]\n",
            "loss: 0.916667  [  782/197710]\n",
            "loss: 0.812500  [  783/197710]\n",
            "loss: 0.781250  [  784/197710]\n",
            "loss: 0.697917  [  785/197710]\n",
            "loss: 0.770833  [  786/197710]\n",
            "loss: 0.843750  [  787/197710]\n",
            "loss: 0.776042  [  788/197710]\n",
            "loss: 0.906250  [  789/197710]\n",
            "loss: 0.901042  [  790/197710]\n",
            "loss: 0.614583  [  791/197710]\n",
            "loss: 0.666667  [  792/197710]\n",
            "loss: 0.781250  [  793/197710]\n",
            "loss: 1.078125  [  794/197710]\n",
            "loss: 0.760417  [  795/197710]\n",
            "loss: 0.843750  [  796/197710]\n",
            "loss: 0.880208  [  797/197710]\n",
            "loss: 0.791667  [  798/197710]\n",
            "loss: 0.817708  [  799/197710]\n",
            "loss: 0.864583  [  800/197710]\n",
            "loss: 0.869792  [  801/197710]\n",
            "loss: 1.005208  [  802/197710]\n",
            "loss: 0.828125  [  803/197710]\n",
            "loss: 0.770833  [  804/197710]\n",
            "loss: 0.932292  [  805/197710]\n",
            "loss: 0.822917  [  806/197710]\n",
            "loss: 0.734375  [  807/197710]\n",
            "loss: 0.848958  [  808/197710]\n",
            "loss: 1.052083  [  809/197710]\n",
            "loss: 1.015625  [  810/197710]\n",
            "loss: 0.734375  [  811/197710]\n",
            "Highest Loss: 1.1197917461395264\n",
            "loss: 1.119792  [  812/197710]\n",
            "loss: 0.885417  [  813/197710]\n",
            "loss: 0.817708  [  814/197710]\n",
            "loss: 0.843750  [  815/197710]\n",
            "loss: 0.656250  [  816/197710]\n",
            "loss: 0.880208  [  817/197710]\n",
            "loss: 0.885417  [  818/197710]\n",
            "loss: 0.901042  [  819/197710]\n",
            "loss: 0.864583  [  820/197710]\n",
            "loss: 0.953125  [  821/197710]\n",
            "loss: 0.734375  [  822/197710]\n",
            "loss: 0.848958  [  823/197710]\n",
            "loss: 0.671875  [  824/197710]\n",
            "loss: 0.802083  [  825/197710]\n",
            "loss: 0.968750  [  826/197710]\n",
            "loss: 0.697917  [  827/197710]\n",
            "loss: 0.843750  [  828/197710]\n",
            "loss: 0.958333  [  829/197710]\n",
            "loss: 0.875000  [  830/197710]\n",
            "loss: 0.718750  [  831/197710]\n",
            "loss: 0.885417  [  832/197710]\n",
            "loss: 0.697917  [  833/197710]\n",
            "loss: 0.786458  [  834/197710]\n",
            "loss: 0.880208  [  835/197710]\n",
            "loss: 0.744792  [  836/197710]\n",
            "loss: 0.661458  [  837/197710]\n",
            "loss: 0.906250  [  838/197710]\n",
            "loss: 0.739583  [  839/197710]\n",
            "loss: 0.812500  [  840/197710]\n",
            "loss: 0.927083  [  841/197710]\n",
            "loss: 0.776042  [  842/197710]\n",
            "loss: 0.973958  [  843/197710]\n",
            "loss: 0.703125  [  844/197710]\n",
            "loss: 0.932292  [  845/197710]\n",
            "loss: 0.963542  [  846/197710]\n",
            "loss: 0.828125  [  847/197710]\n",
            "loss: 0.786458  [  848/197710]\n",
            "loss: 0.848958  [  849/197710]\n",
            "loss: 0.864583  [  850/197710]\n",
            "loss: 0.703125  [  851/197710]\n",
            "loss: 0.796875  [  852/197710]\n",
            "loss: 0.833333  [  853/197710]\n",
            "loss: 1.015625  [  854/197710]\n",
            "loss: 1.005208  [  855/197710]\n",
            "loss: 0.750000  [  856/197710]\n",
            "loss: 1.046875  [  857/197710]\n",
            "loss: 0.697917  [  858/197710]\n",
            "loss: 0.937500  [  859/197710]\n",
            "loss: 0.864583  [  860/197710]\n",
            "loss: 0.817708  [  861/197710]\n",
            "loss: 1.052083  [  862/197710]\n",
            "loss: 0.895833  [  863/197710]\n",
            "loss: 0.682292  [  864/197710]\n",
            "loss: 0.734375  [  865/197710]\n",
            "loss: 0.885417  [  866/197710]\n",
            "loss: 0.817708  [  867/197710]\n",
            "loss: 0.734375  [  868/197710]\n",
            "loss: 0.760417  [  869/197710]\n",
            "loss: 0.760417  [  870/197710]\n",
            "loss: 0.984375  [  871/197710]\n",
            "loss: 0.781250  [  872/197710]\n",
            "loss: 0.703125  [  873/197710]\n",
            "loss: 0.750000  [  874/197710]\n",
            "loss: 0.718750  [  875/197710]\n",
            "loss: 0.692708  [  876/197710]\n",
            "loss: 0.947917  [  877/197710]\n",
            "loss: 0.796875  [  878/197710]\n",
            "loss: 0.979167  [  879/197710]\n",
            "loss: 0.875000  [  880/197710]\n",
            "loss: 0.744792  [  881/197710]\n",
            "loss: 1.052083  [  882/197710]\n",
            "loss: 0.791667  [  883/197710]\n",
            "loss: 0.968750  [  884/197710]\n",
            "loss: 0.864583  [  885/197710]\n",
            "loss: 0.968750  [  886/197710]\n",
            "loss: 0.833333  [  887/197710]\n",
            "loss: 0.791667  [  888/197710]\n",
            "loss: 0.958333  [  889/197710]\n",
            "loss: 0.765625  [  890/197710]\n",
            "loss: 0.885417  [  891/197710]\n",
            "loss: 0.786458  [  892/197710]\n",
            "loss: 0.677083  [  893/197710]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-c4bfc014afbb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTest_Normal_DL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-57-21f94a10984f>\u001b[0m in \u001b[0;36mtest_normal\u001b[0;34m(dataloader, model, loss_fn)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-1006728f65a0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# Encoder:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/brevitas/nn/quant_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQuantTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQuantTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_weight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_bias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/brevitas/nn/quant_layer.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquant_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mquant_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_quant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquant_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/brevitas/proxy/runtime_quant.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfused_activation_quant_proxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;31m# If y is an empty QuantTensor, we need to check if this is a passthrough proxy,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;31m# otherwise return a simple Tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/brevitas/proxy/runtime_quant.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_quant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/brevitas/core/quant/binary.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_sign_ste\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelay_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbit_width\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/brevitas/core/utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mbrevitas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscript_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     def _load_from_state_dict(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "test_normal(Test_Normal_DL, AutoEncoder, loss_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLVHBC7VCEWX"
      },
      "source": [
        "# **Export to ONNX**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkRWhEYuCStg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "7Ector0qOoer",
        "qnx3IVEXHHra",
        "ajZYWALM_RaB",
        "py-nYpLpND0Z",
        "KlVxgRDsWuhL",
        "SMS9F4eA5eej",
        "rZSs7xZ5Fu6f"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}